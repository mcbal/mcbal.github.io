<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vector-Spin Models | mcbal</title>
    <link>https://mcbal.github.io/tag/vector-spin-models/</link>
      <atom:link href="https://mcbal.github.io/tag/vector-spin-models/index.xml" rel="self" type="application/rss+xml" />
    <description>Vector-Spin Models</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>Matthias Bal © 2023</copyright><lastBuildDate>Sun, 19 Jun 2022 09:28:17 +0100</lastBuildDate>
    <image>
      <url>https://mcbal.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Vector-Spin Models</title>
      <link>https://mcbal.github.io/tag/vector-spin-models/</link>
    </image>
    
    <item>
      <title>Spin-Model Transformers</title>
      <link>https://mcbal.github.io/post/spin-model-transformers/</link>
      <pubDate>Sun, 19 Jun 2022 09:28:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/spin-model-transformers/</guid>
      <description>&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ &lt;strong&gt;This blog post is a work in progress exploring connections between transformer neural networks and mean-field dynamics of (asymmetric) vector-spin models. Code in JAX and PyTorch of the content outlined in this post should become available at &lt;a href=&#34;https://github.com/mcbal/spin-model-transformers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mcbal/spin-model-transformers&lt;/code&gt;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-mean-field-theory-of-asymmetric-ising-models-with-binary-spins&#34;&gt;Mean-field theory of asymmetric Ising models with binary spins&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#21-setting-the-scene-the-kinetic-ising-model&#34;&gt;Setting the scene: the kinetic Ising model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#22-mean-field-theory-and-kullback-leibler-divergence&#34;&gt;Mean-field theory and Kullback-Leibler divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#23-the-plefka-expansion-interpolating-distributions&#34;&gt;The Plefka expansion: interpolating distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#24-naive-mean-field-and-thouless-anderson-palmer-approximations&#34;&gt;Naive mean-field and Thouless-Anderson-Palmer approximations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#25-a-simple-jax-implementation&#34;&gt;A simple JAX implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins&#34;&gt;Mean-field theory of asymmetric Ising models with vector spins&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#31-vector-spins-distributions-on-hyperspheres&#34;&gt;Vector spins: distributions on hyperspheres&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#32-magnetizations-and-limit-of-large-vector-dimension&#34;&gt;Magnetizations and limit of large vector dimension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#33-first-order-naive-mean-field-approximation&#34;&gt;First-order naive mean-field approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#34-second-order-thouless-anderson-palmer-approximation&#34;&gt;Second-order Thouless-Anderson-Palmer approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#35-a-simple-jax-implementation&#34;&gt;A simple JAX implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-a-family-of-transformer-like-modules&#34;&gt;A family of transformer-like modules&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#41-connecting-the-dots&#34;&gt;Connecting the dots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#42-fast--and-slow-moving-parameters&#34;&gt;Fast- and slow-moving parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#43-a-simple-jax-implementation&#34;&gt;A simple JAX implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendices&#34;&gt;Appendices&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#a1-vector-spin-distribution-normalization-constant&#34;&gt;Vector-spin distribution: normalization constant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a2-vector-spin-distribution-expected-value-first-moment&#34;&gt;Vector-spin distribution: expected value (first moment)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a3-vector-spin-distribution-variance-second-moment&#34;&gt;Vector-spin distribution: variance (second moment)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a4-ratio-of-modified-bessel-functions-of-the-first-kind&#34;&gt;Ratio of modified Bessel functions of the first kind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a5-general-case-partial-derivatives-with-respect-to-alpha&#34;&gt;General case: partial derivatives with respect to $\alpha$&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;In a series of previous &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog posts&lt;/a&gt;, we have tried to connect the forward pass of a transformer neural-network module to computing mean magnetizations in disordered Ising-like vector-spin models with parameterized couplings and external magnetic fields. In this framework, the forward pass of a transformer module computes statistical observables given a specific realization of quenched couplings and external magnetic fields while the backward pass nudges the parameterized couplings and external magnetic fields. Physically, the transformer module represents an interacting many-body system modulating its behavior by learning to respond to being driven in all kinds of funny ways.&lt;/p&gt;
&lt;p&gt;However, both the mean-field message-passing approach of &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms (2021)&lt;/a&gt; and the saddle-point free-energy approach of &lt;a href=&#34;https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers from Spin Models: Approximate Free Energy Minimization (2021)&lt;/a&gt; inherently rely on methods that are only well-defined for spin models with symmetric coupling matrices, whose stochastic dynamics obey detailed balance and converge to a steady-state equilibrium characterized by the Boltzmann distribution. Since the softmax attention matrix in transformer modules is famously asymmetric, we had better come up with a more convincing approach to establish a correspondence.&lt;/p&gt;
&lt;p&gt;To capture spin models with asymmetric coupling matrices, we turn to non-equilibrium spin systems, whose dynamics can be pretty wild yet gentle enough to support regimes where relaxation to a non-equilibrium steady state can occur. In the past few decades, dynamical mean-field approaches have been developed for the binary kinetic Ising model, which exhibits non-equilibrium behavior when couplings are asymmetric or when parameters are subject to rapid changes. In this post, we generalize a particular dynamical mean-field approach from binary spins to vector spins and relate the resulting mean-field update equations for the magnetizations to the forward pass of a transformer module. We find that the spin-model structure is rich enough for the update equations to yield residual connections, attention terms, and feed-forward correction terms, motivating a physics-inspired class of transformer modules.&lt;/p&gt;
&lt;h1 id=&#34;2-mean-field-theory-of-asymmetric-ising-models-with-binary-spins&#34;&gt;2. Mean-field theory of asymmetric Ising models with binary spins&lt;/h1&gt;
&lt;p&gt;In this section, we review known results on mean-field theory approaches to capturing the stochastic dynamics of binary kinetic Ising models. Readers familiar with this framework can skip to &lt;a href=&#34;#3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins&#34;&gt;Section 3&lt;/a&gt; where we develop a generalization to vector spins. We primarily follow the discussion outlined in &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;A unifying framework for mean-field theories of asymmetric kinetic Ising systems&lt;/em&gt; (Aguilera et al., 2021)&lt;/a&gt;. We implement the mean-field update equations for the mean magnetizations in JAX and run a few numerical experiments.&lt;/p&gt;
&lt;h2 id=&#34;21-setting-the-scene-the-kinetic-ising-model&#34;&gt;2.1. Setting the scene: the kinetic Ising model&lt;/h2&gt;
&lt;img src=&#34;binary_spins.png&#34; alt=&#34;Random Ising model configuration with binary spins&#34; width=&#34;250px&#34;/&gt;
&lt;p&gt;We consider a kinetic Ising model describing a system made up of $N$ interacting binary spins $s_{i,t} \in \{-1, 1\}$ that evolve in discrete time steps $t$ according to synchronous dynamics, i.e. all spins get updated at the same time in parallel. Given a spin configuration $\mathbf{s}_{t-1} = \{ s_{1,t-1}, s_{2,t-1}, \ldots, s_{N,t-1} \}$ at time $t-1$, we consider the spins $\mathbf{s}_{t}$ at time $t$ to be conditionally independent random variables captured by a discrete-time Markov chain transition probability&lt;/p&gt;
&lt;p&gt;\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{\sum_{s_{i,t}} \mathrm{e}^{s_{i,t} h_{i,t}}} = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{2 \cosh h_{i,t}}, \label{eq:pcond}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the effective external field is given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
h_{i,t} = x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1}.
\end{equation}&lt;/p&gt;
&lt;p&gt;Here, the parameters $\mathbf{x}$ represent the (possibly time-dependent) local external fields at each site while the coupling parameters $\mathbf{J}$ are a specific realization of quenched disorder encoding the interactions between pairs of spins. Using the probability mass function of the previous state $P( \mathbf{s}_{t-1} )$ we can write the distribution of the current state as&lt;/p&gt;
&lt;p&gt;\begin{equation}
P( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1}} P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P( \mathbf{s}_{t-1} ), \label{eq:marginal}
\end{equation}&lt;/p&gt;
&lt;p&gt;which, when applied recursively, traces the evolution of the system starting from some initial distribution $P( \mathbf{s}_{0} )$. Unless we turn off the couplings by setting $\mathbf{J} = \mathbf{0}$, the marginal distribution $P( \mathbf{s}_{t} )$ is not factorized and tends to be quite complicated. Our goal is to compute statistical properties of the system, such as the mean magnetizations&lt;/p&gt;
&lt;p&gt;\begin{equation}
m_{i,t} = \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t} ),
\end{equation}&lt;/p&gt;
&lt;p&gt;as well as correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
C_{ik,t} = \sum_{\mathbf{s}_{t}} s_{i,t} s_{k,t} P( \mathbf{s}_{t} ) - m_{i,t} m_{k,t},
\end{equation}&lt;/p&gt;
&lt;p&gt;and delayed correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{il,t} = \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1}} s_{i,t} s_{l,t-1} P( \mathbf{s}_{t}, \mathbf{s}_{t-1} ) - m_{i,t} m_{l,t-1}.
\end{equation}&lt;/p&gt;
&lt;p&gt;Since the above expressions involve summing over a large amount of possible spin configurations, they are not very useful in practice. So we will try to approximate the tricky marginal distribution $P( \mathbf{s}_{t} )$ defined in Eq. \eqref{eq:marginal} using a mean-field theory approach.&lt;/p&gt;
&lt;h2 id=&#34;22-mean-field-theory-and-kullback-leibler-divergence&#34;&gt;2.2. Mean-field theory and Kullback-Leibler divergence&lt;/h2&gt;
&lt;p&gt;Mean-field theory tries to approximate a complicated object ${\color{red}P}$ by wiggling around the parameters of a simple, analytically tractable parameterized ansatz ${\color{green}Q_{\theta}}$ to get as close as possible to ${\color{red}P}$. At risk of inducing headaches in mathematicians by calling everything a manifold, we can picture what is going on geometrically as trying to approximate a target probability distribution $P( \mathbf{s}_{t} \vert \mathbf{x}, \mathbf{J})$ and its statistical properties $\mathbf{m}_{t}$, $\mathbf{C}_{t}$, and $\mathbf{D}_{t}$ by restricting ourselves to a submanifold of tractable probability distributions. A particularly convenient submanifold is that of factorized models, where each point on the submanifold corresponds to a distribution parameterized by a vector $\boldsymbol{\theta}_{t}$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} \theta_{i,t}}}{2 \cosh \theta_{i,t}}, \label{eq:q}
\end{equation}&lt;/p&gt;
&lt;p&gt;so that the mean magnetizations are simply given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
m_{i,t} = \tanh \theta_{i,t}  \label{eq:meanmagstanh}
\end{equation}&lt;/p&gt;
&lt;p&gt;as there are no couplings between spins. The factorized model $Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}^{*}_{t} )$ that minimizes the Kullback-Leibler (KL) divergence&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) = \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log \frac{P( \mathbf{s}_{t})}{Q_{\theta}( \mathbf{s}_{t})} \label{eq:kl}
\end{equation}&lt;/p&gt;
&lt;p&gt;has mean magnetizations $\mathbf{m}_{t}$ identical to those of the target distribution $P( \mathbf{s}_{t})$ since, for all spins $i=1,2,\ldots,N$, we find that&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}} &amp;amp;= - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \frac{\partial \log Q_{\theta}( \mathbf{s}_{t}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}}  \\
&amp;amp;= - \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t}) + \tanh \theta^{*}_{i,t} \\
&amp;amp;= -m^{{\color{red}P}}_{i,t} + m^{{\color{green}Q_{\theta^{*}}}}_{i,t} = 0, \label{eq:klm}
\end{align}&lt;/p&gt;
&lt;p&gt;where $m^{{\color{red}P}}_{i,t}$ and $m^{{\color{green}Q_{\theta^{*}}}}_{i,t}$ respectively denote the expectation values of $s_{i,t}$ with respect to ${\color{red}P}$ and ${\color{green}Q_{\theta^{*}}}$. Indeed, minimizing $D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}})$ tries to cover the modes of ${\color{red}P}$ by moment matching since the expectation value in Eq. \eqref{eq:kl} is calculated with respect to ${\color{red}P}$.&lt;/p&gt;
&lt;h2 id=&#34;23-the-plefka-expansion-interpolating-distributions&#34;&gt;2.3. The Plefka expansion: interpolating distributions&lt;/h2&gt;
&lt;p&gt;Great, but is it even possible to find the parameters&lt;/p&gt;
&lt;p&gt;\begin{equation}
\DeclareMathOperator*{\argmin}{arg\,min}
\boldsymbol{\theta}^{*}_{t} = \argmin_{\boldsymbol{\theta}_{t}} \left( - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log Q_{\theta}( \mathbf{s}_{t}) \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;that minimize the KL divergence? Well, that&amp;rsquo;s going to be hard, unless you already know the target distribution $P( \mathbf{s}_{t})$, or you have a clever way of approximately evaluating the expectation value of $\log {\color{green}Q_{\theta}}$ with respect to ${\color{red}P}$. So let us introduce some more distributions to get around this issue. To apply the Plefka expansion to our problem, we introduce the conditional distribution&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_{\alpha}( \mathbf{s}_{t}\vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N}  \frac{\mathrm{e}^{s_{i,t} h_{i,t}(\alpha) }}{2 \cosh h_{i,t}(\alpha)}, \label{eq:pcondalt}
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
h_{i,t}(\alpha) = (1-\alpha) \theta_{i,t} + \alpha \left( x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1} \right), \label{eq:pcondalth}
\end{equation}&lt;/p&gt;
&lt;p&gt;parameterized by a scalar $\alpha$ interpolating between $P_{\alpha=0}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} )$ (Eq. \eqref{eq:q}) and $P_{\alpha=1}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} )$ (Eq. \eqref{eq:pcond}). Using Eq. \eqref{eq:pcondalt}, we can construct an approximate marginal distribution $P_{\alpha}( \mathbf{s}_{t})$, leading to $\alpha$-dependent statistical properties $\mathbf{m}_{t}(\alpha)$, $\mathbf{C}_{t}(\alpha)$, and $\mathbf{D}_{t}(\alpha)$ for the approximate system. The Plefka expansion then boils down to writing these properties as Taylor series expansions around the factorized model $\alpha=0$. For the mean magnetizations, the expansion up to $n$-th order looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}_{t}(\alpha) = \mathbf{m}_{t}(\alpha=0) + \sum_{k=1}^{n} \frac{\alpha^k}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}), \label{eq:mtaylor}
\end{equation}&lt;/p&gt;
&lt;p&gt;where all coefficients in the expansion are functions of $\boldsymbol{\theta}_{t}$ via Eq. \eqref{eq:pcondalth}. The mean-field approximation is computed by setting $\alpha=1$ so that the original marginal distribution is recovered and Eq. \eqref{eq:klm} holds, which implies that $\mathbf{m}_{t}(\alpha=1) = \mathbf{m}_{t}(\alpha=0)$ and thus&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{k=1}^{n} \frac{1}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}) = 0. \label{eq:mftheta}
\end{equation}&lt;/p&gt;
&lt;p&gt;Finally, we solve Eq. \eqref{eq:mftheta} for $\boldsymbol{\theta}_{t}$ to find the mean-field values $\boldsymbol{\theta}^{*}_{t}$ of the parameters of the distribution Eq. \eqref{eq:q}. Physically, we are tuning the effective external magnetic fields of the factorized ansatz to $\boldsymbol{\theta}^{*}_{t}$ so that its approximate mean magnetizations get as close as possible to the true ones.&lt;/p&gt;
&lt;h2 id=&#34;24-naive-mean-field-and-thouless-anderson-palmer-approximations&#34;&gt;2.4. Naive mean-field and Thouless-Anderson-Palmer approximations&lt;/h2&gt;
&lt;p&gt;We now consider first and second order approximations of the mean magnetizations Eq. \eqref{eq:mtaylor} to recover respectively the naive mean-field and Thouless-Anderson-Palmer (TAP) approximations for the binary kinetic Ising model. The starting point is a Plefka expansion around factorized models at times $t-1$ and $t$. From Eq. \eqref{eq:marginal} and Eq. \eqref{eq:pcondalt}, we construct a marginal probability distribution&lt;/p&gt;
&lt;p&gt;\begin{equation}
P^{[t-1:t]}_{\alpha}( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ),
\end{equation}&lt;/p&gt;
&lt;p&gt;interpolating between $P^{[t-1:t]}_{\alpha=0}( \mathbf{s}_{t} ) = Q( \mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\alpha=1}( \mathbf{s}_{t} ) = P( \mathbf{s}_{t} )$. The corresponding mean magnetizations are&lt;/p&gt;
&lt;p&gt;\begin{align}
m_{i,t}(\alpha) &amp;amp;= \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1},\mathbf{s}_{t-2}} s_{i,t} \, P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ) \\
&amp;amp;= \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} \tanh h_{i,t}(\alpha) \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} )
\end{align}&lt;/p&gt;
&lt;p&gt;Following Eq. \eqref{eq:mftheta}, the first-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} = \left( 1-m^{2}_{i,t} \right) \left( -\theta_{i,t} + x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right) = 0,
\end{equation}&lt;/p&gt;
&lt;p&gt;so that $\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1}$ and we end up with the naive mean-field equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{m_{i,t} = \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right)} \label{eq:naivem}
\end{equation}&lt;/p&gt;
&lt;p&gt;Again following Eq. \eqref{eq:mftheta}, the second-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} + \frac{1}{2} \frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial^{2}\alpha} = 0,
\end{equation}&lt;/p&gt;
&lt;p&gt;where the second-order derivative, neglecting terms higher than $\mathcal{O}(\alpha^2)$, is&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial^{2}\alpha} \approx -2 m_{i,t} \left( 1-m^{2}_{i,t} \right) \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{equation}
\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;and we end up with the TAP mean-field equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{m_{i,t} = \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right) \right)} \label{eq:tapm}
\end{equation}&lt;/p&gt;
&lt;p&gt;The mean-field equations obtained above can also be elegantly derived using a Legendre transformation of the generating functional of the set of trajectories of the model, as outlined in e.g. &lt;a href=&#34;https://arxiv.org/abs/1103.1044&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Dynamical TAP equations for non-equilibrium Ising spin glasses (2011)&lt;/em&gt;&lt;/a&gt;. We can also derive second-order TAP approximations of the correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
C_{ik,t} = \begin{cases}
1 - m^{2}_{i,t}  &amp;amp; i = k \\
\left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{k,t} \right) \sum_{j} J_{ij} J_{kj} \left( 1-m^{2}_{j,t-1} \right) &amp;amp; i \neq k \label{eq:tapc}
\end{cases}
\end{equation}&lt;/p&gt;
&lt;p&gt;and delayed correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{il,t} = J_{il} \left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{l,t-1} \right) \left( 1 + 2 J_{il} m_{i,t} m_{l,t-1} \right). \label{eq:tapd}
\end{equation}&lt;/p&gt;
&lt;p&gt;We refer to &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Aguilera et al., 2020)&lt;/a&gt; for full derivations of the above mean-field results as well as variations based on different approximations of the marginal distribution $P( \mathbf{s}_{t} )$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In summary, given the mean magnetizations $\mathbf{m}_{t-1}$ of the system at time $t-1$, we can use equations \eqref{eq:tapm} \eqref{eq:tapc} \eqref{eq:tapd} to compute a tuple $(\mathbf{m}_{t},\mathbf{C}_{t},\mathbf{D}_{t})$ of approximate statistical properties  of the system at time $t$. The time evolution of the system can be captured at the mean-field level by recursively computing $\mathbf{m}_{t}$ starting from an initial state $\mathbf{m}_{0}$ (with approximation errors likely accumulating over the course of the time evolution).&lt;/p&gt;
&lt;h2 id=&#34;25-a-simple-jax-implementation&#34;&gt;2.5. A simple JAX implementation&lt;/h2&gt;
&lt;p&gt;To get more insight into what is going on, let us turn the mean-field update equations \eqref{eq:naivem} and \eqref{eq:tapm} for the mean magnetizations into code. But before we show a few plots, we need to know a bit more background about the model we are about to simulate. In &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Aguilera et al., 2020)&lt;/a&gt;, the authors derive a solution of the asymmetric version of the kinetic &lt;a href=&#34;https://en.wikipedia.org/wiki/Spin_glass#Sherrington%E2%80%93Kirkpatrick_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sherrington-Kirkpatrick mean-field spin-glass model&lt;/a&gt; using a generating functional or dynamical partition function approach to capture the distribution of trajectories. They consider the same kinetic Ising model as in Eq. \eqref{eq:pcond} but with an inverse temperature parameter $\beta$ in the exponentials:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta s_{i,t} h_{i,t}}}{2 \cosh \beta h_{i,t}}. \label{eq:pcondwithbeta}
\end{equation}&lt;/p&gt;
&lt;p&gt;For Gaussian couplings $J_{ij} \sim \mathcal{N}\left( J_{\mu} / N, J^{2}_{\sigma} / N\right)$ and uniformly distributed external magnetic fields $x_{i} \sim \mathcal{U}(-X_{0}, X_{0})$, they show the existence of a ferromagnetic phase transition. In particular for $X_{0}=0.5$, $J_{\mu}=1.0$, and $J_{\sigma}=0.1$, a phase transition happens when tuning $\beta$ to a critical value $\beta_{c} \approx 1.1108$.&lt;/p&gt;
&lt;h3 id=&#34;simulating-magnetization-trajectories&#34;&gt;Simulating magnetization trajectories&lt;/h3&gt;
&lt;p&gt;We first present a JAX implementation of the mean-field time evolution of the magnetizations according to the model described above. We use &lt;code&gt;lax.scan&lt;/code&gt; to implement the time evolution and &lt;code&gt;vmap&lt;/code&gt; to parallelize trajectories starting from a batch of initial magnetization configurations $\mathbf{m}_{0}$. For the second-order TAP equations, &lt;code&gt;jaxopt&lt;/code&gt;&amp;rsquo;s Anderson acceleration is used to find the fixed point magnetizations $\mathbf{m}_{t}$ given $\mathbf{m}_{t-1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial

import jax
import jax.numpy as jnp

from jaxopt import AndersonAcceleration


def update_naive_mf(m0, _, x, J):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (22).&amp;quot;&amp;quot;&amp;quot;
    m1 = jnp.tanh(x + jnp.einsum(&amp;quot;i j, j -&amp;gt; i&amp;quot;, J, m0))
    return m1, m0


def update_tap_mf(m0, _, x, J):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (26).&amp;quot;&amp;quot;&amp;quot;

    def tap(m, _m0, _x, _J):
        return jnp.tanh(
            _x
            + jnp.einsum(&amp;quot;i j, j -&amp;gt; i&amp;quot;, _J, _m0)
            - m * jnp.einsum(&amp;quot;i j, j -&amp;gt; i&amp;quot;, _J**2, (1.0 - _m0**2))
        )

    m1 = (
        AndersonAcceleration(fixed_point_fun=tap, tol=1e-3, maxiter=10)
        .run(m0, m0, x, J)
        .params
    )
    return m1, m0


def time_evolution(m0, steps, update_fun):
    final_carry, stacked_outputs = jax.lax.scan(update_fun, init=m0, xs=steps)
    return final_carry, stacked_outputs


def init_params(key, N, beta, X0, J_mu, J_sigma):
    x_key, J_key = jax.random.split(key)
    x = jax.random.uniform(x_key, shape=(N,), minval=-beta * X0, maxval=beta * X0)
    J = beta * J_mu * N**-1 + beta * J_sigma * N**-0.5 * jax.random.normal(
        J_key, shape=(N, N)
    )
    return x, J


def simulate(
    key, m0, steps, beta, X0=0.5, J_mu=1.0, J_sigma=0.1, update_fun=update_tap_mf
):
    x, J = init_params(key, m0.shape[-1], beta, X0, J_mu, J_sigma)
    wrapped_time_evolution = partial(
        time_evolution,
        steps=steps,
        update_fun=partial(update_fun, x=x, J=J),
    )
    final_carry, stacked_outputs = jax.vmap(wrapped_time_evolution)(m0)
    return final_carry, stacked_outputs
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;naive-mean-field-vs-thouless-anderson-palmer-tap&#34;&gt;Naive mean-field vs. Thouless-Anderson-Palmer (TAP)&lt;/h3&gt;
&lt;p&gt;We fix the seed for the randomly initialized model parameters $\mathbf{x}$ and $\mathbf{J}$ and simulate $N=512$ spins at the critical temperature $\beta_{c}$ for $t=128$ time steps starting from an all-ones intial state. We first consider the naive mean-field update step.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The left axis shows the individual magnetization trajectories for each spin plotted horizontally while the red line associated to the right axis describes the average of the magnetizations across all spins for each time step. We observe convergence to what looks like a non-equilibrium steady state.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Comparing the naive first-order mean-field update equations to the second-order Thouless-Anderson-Palmer (TAP) ones, we observe lower values for the mean magnetization across all spins, which &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Aguilera et al., 2020)&lt;/a&gt; showed to be closer to ground truth values (not shown) obtained via sampling and averaging spin configurations.&lt;/p&gt;
&lt;h3 id=&#34;sampling-trajectories&#34;&gt;Sampling trajectories&lt;/h3&gt;
&lt;p&gt;Let us now consider 100 randomly-initialized initial states and simulate their associated trajectories in three different model regimes: far below the critical point ($\beta=\beta_c / 2 $), at the critical point ($\beta=\beta_c$), and far above the critical point ($\beta=2 \beta_c$).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We observe that the trajectories of randomly-initialized initial states converge to identical final states in each regime. These final states map to a simple ferromagnetic Ising phase diagram, where a high-temperature disordered phase $\langle m_{i,t} \rangle \to 0$ (left) is separated from a low-temperature locally-ordered phase $\langle m_{i,t} \rangle \to \pm 1$ (right) by a critical point (center). The behavior around $\beta=\beta_{c}$ is pretty interesting: &lt;em&gt;the non-trivial non-equilibrium steady state looks like an attractor implicitly encoded in the dynamics of the model&lt;/em&gt;. If we were to parametrize the couplings, we could train the system to act as an associative memory.&lt;/p&gt;
&lt;h3 id=&#34;sampling-model-parameters&#34;&gt;Sampling model parameters&lt;/h3&gt;
&lt;p&gt;We now go back to considering just a single trajectory since we just saw that trajectories seem to converge to the same final steady-state magnetizations for fixed model parameters. To get a feel for the variation of these values across different realizations of model parameters, we plot the absolute value&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; $| \langle m_{i} \rangle |$ of the final steady-state magnetizations across 100 samples of model parameters and a range of inverse temperatures. We are using JAX, so we can easily sample model parameters by &lt;code&gt;vmap&lt;/code&gt;&amp;lsquo;ing the random key fed into the &lt;code&gt;simulate&lt;/code&gt; function followed by another &lt;code&gt;vmap&lt;/code&gt; to sweep across $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Every curve in the above plot describes the final steady-state value of the &amp;ldquo;order parameter&amp;rdquo; $| \langle m_{i} \rangle |$ for a fixed set of model parameters sweeping across $\beta$. We observe a greater spread of values near the critical point and hence an improved capacity to map input external fields to a range of output magnetizations. If we were to let the number of spins $N \to \infty$ and average over a large number of model parameter samples, the finite-size results above would probably transform into a sharp curve with zero magnetization below the critical point and a sudden non-zero magnetization emerging at the critical point.&lt;/p&gt;
&lt;h1 id=&#34;3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins&#34;&gt;3. Mean-field theory of asymmetric Ising models with vector spins&lt;/h1&gt;
&lt;p&gt;We now transpose the binary-spin results of the previous section to a setting where local spin degrees of freedom are $D$-dimensional vector spins restricted to wiggle around on $(D-1)$-dimensional spheres. We start by generalizing the conditional distribution Eq. \eqref{eq:pcondalt} to vector spins. Next, we motivate the limit of large vector dimension and derive first-order and second-order mean-field update equations for the mean magnetizations. We finish this section with a JAX implementation and some toy numerical experiments.&lt;/p&gt;
&lt;h2 id=&#34;31-vector-spins-distributions-on-hyperspheres&#34;&gt;3.1. Vector spins: distributions on hyperspheres&lt;/h2&gt;
&lt;img src=&#34;vector_spins.png&#34; alt=&#34;Random Ising model configuration with vector spins&#34; width=&#34;250px&#34;/&gt;
&lt;p&gt;A vector-spin equivalent of Eq. \eqref{eq:pcondalt} looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \mathbf{h}_{i,t}(\alpha)}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s}_{i,t} \; \mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \mathbf{h}_{i,t}(\alpha)} }, \label{eq:pcondaltvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;where we immediately included an inverse temperature scaling $\beta$ like in Eq. \eqref{eq:pcondwithbeta}. A vector-spin equivalent of Eq. \eqref{eq:pcondalth} is&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{h}_{i,t}(\alpha) = (1-\alpha) \boldsymbol{\theta}_{i,t} + \alpha \left( \mathbf{x}_{i,t} + \sum_{j=1}^{N} J_{ij} \mathbf{s}_{j,t-1} \right) \equiv \boldsymbol{\theta}_{i,t} + \alpha \Delta \mathbf{h}_{i,t},  \label{eq:pcondalthvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $S_{D-1}(R) = \{ x \in \mathbb{R}^{D} : \lVert x \rVert = R \}$ denotes the $(D-1)$-dimensional sphere with radius $R$ embedded in $D$ dimensions. Let us focus on the distribution for a single site and drop all subscripts and dependencies for clarity:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }. \label{eq:pcondsinglesitevector}
\end{equation}&lt;/p&gt;
&lt;p&gt;The normalization constant in the denominator can be shown to be (see &lt;a href=&#34;#a1-vector-spin-distribution-normalization-constant&#34;&gt;Appendix A.1&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } \equiv Z(\beta, R, \lVert \mathbf{h}\rVert) \label{eq:partfun}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $I_{\nu}(z)$ denotes the modified Bessel function of the first kind and $\lVert \mathbf{h}\rVert = \sqrt{\mathbf{h} \cdot \mathbf{h}}$. Physically, we can think of this single-site distribution as measuring dot-product alignment to an effective external magnetic field $\mathbf{h}$ at inverse temperature $\beta$.&lt;/p&gt;
&lt;p&gt;If we consider spins living on the unit sphere $R=1$ as well as unit vectors $\mathbf{h}$, the distribution boils down to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;von Mises–Fisher distribution&lt;/a&gt; with mean direction $\boldsymbol{\mu} \equiv \mathbf{h}$ and &lt;a href=&#34;https://en.wikipedia.org/wiki/Concentration_parameter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;concentration parameter&lt;/a&gt; $\kappa \equiv \beta$. This distribution is unimodal for $\kappa &amp;gt; 0$ and can be derived from restricting an isotropic multivariate Gaussian to the unit hypersphere. The greater the value of $\kappa$ (the inverse temperature $\beta$), the higher the concentration of the distribution around the mean direction $\boldsymbol{\mu}$ (the more the spin tends to align to the effective external field $\mathbf{h}$). Instead of a fixed parameter $\boldsymbol{\mu}$, we have a very funky parameter Eq. \eqref{eq:pcondalthvector} that depends on all other spins to spice things up.&lt;/p&gt;
&lt;h2 id=&#34;32-magnetizations-and-limit-of-large-vector-dimension&#34;&gt;3.2. Magnetizations and limit of large vector dimension&lt;/h2&gt;
&lt;p&gt;Before we derive mean-field approximations for the mean magnetizations of our vector-spin system, let us first consider the decoupled $\alpha \to 0$ limit of the distribution Eq. \eqref{eq:pcondaltvector},&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \boldsymbol{\theta}_{i,t}}}{Z_{i,t}\left(\beta, R, \lVert \boldsymbol{\theta}_{i,t} \rVert\right)},
\end{equation}&lt;/p&gt;
&lt;p&gt;and find an expression for its mean magnetizations. For every decoupled site, the mean magnetization can be shown to be (see &lt;a href=&#34;#a2-vector-spin-distribution-expected-value-first-moment&#34;&gt;Appendix A.2&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}_{i,t} = \frac{I_{D/2}(\beta R \lVert \boldsymbol{\theta}_{i,t} \rVert)}{I_{D/2 - 1}(\beta R \lVert \boldsymbol{\theta}_{i,t} \rVert)} \frac{R \boldsymbol{\theta}_{i,t}}{\lVert \boldsymbol{\theta}_{i,t} \rVert} \equiv \boldsymbol{\varphi} \left(\boldsymbol{\theta}_{i,t}\right), \label{eq:meanmagsbessels}
\end{equation}&lt;/p&gt;
&lt;p&gt;which plays the role of $m_{i,t} = \tanh \theta_{i,t}$ in the binary setting, see Eq. \eqref{eq:meanmagstanh}. Looking ahead at turning the above equation into code, we note that there exist &lt;a href=&#34;https://www.jstor.org/stable/2005830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;efficient algorithms&lt;/a&gt; to compute the ratio of modified Bessel functions of the first kind. We implement a fast JAX version in &lt;a href=&#34;#a4-ratio-of-modified-bessel-functions-of-the-first-kind&#34;&gt;Appendix A.4&lt;/a&gt; and show numerically how the ratio flattens out quickly for large values of the order $\nu = D/2 -1$, motivating some kind of large-order expansion.&lt;/p&gt;
&lt;p&gt;Remember that our goal is to connect all this stuff to transformers. Since the vector dimension in practical transformer modules tends be somewhere between $\mathcal{O}(10^2)$ and $\mathcal{O}(10^5)$, it makes sense to focus on the limit of large vector dimension. A relevant uniform asymptotic expansion of the ratio of modified Bessel functions of the first kind can be found in &lt;a href=&#34;https://link.springer.com/article/10.1007/BF02764812&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Kiefer &amp;amp; Weiss, 1972)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{I_{\nu+\alpha}(\nu x)}{I_{\nu}(\nu x)} = \left( \frac{x}{1+\sqrt{1+x^2}} \right)^{\alpha} \left( 1 - \frac{1+\alpha\sqrt{1+x^2}}{2(1+x^2)} \frac{\alpha}{\nu} + \mathcal{O}\left( \frac{1}{\nu^2} \right) \right)
\end{align}&lt;/p&gt;
&lt;p&gt;Indeed, if we choose to tie the radius $R$ of our little spins to their vector dimension $D$ via&lt;/p&gt;
&lt;p&gt;\begin{align}
\nu=D/2-1=R^2,
\end{align}&lt;/p&gt;
&lt;p&gt;we can apply the leading order of the asymptotic expansion for $\alpha=1$ to \eqref{eq:meanmagsbessels} to find&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}^{D \to \infty}_{i,t} \approx \frac{\beta}{1+\sqrt{1+\beta^2 \lVert \boldsymbol{\theta}_{i,t} \rVert^2 / R^2 }} \boldsymbol{\theta}_{i,t} \equiv \boldsymbol{\varphi}^{D \to \infty} \left(\boldsymbol{\theta}_{i,t}\right). \label{eq:largedevmag}
\end{equation}&lt;/p&gt;
&lt;p&gt;From here on, we will default to using the large-$D$ approximation because keeping track of (derivatives of) Bessel functions gets boring real quick. We refer to &lt;a href=&#34;#a5-general-case-partial-derivatives-with-respect-to-alpha&#34;&gt;Appendix A.5&lt;/a&gt; for some truly outrageous expressions pertaining to the general case valid for all $D&amp;gt;1$.&lt;/p&gt;
&lt;h2 id=&#34;33-first-order-naive-mean-field-approximation&#34;&gt;3.3. First-order naive mean-field approximation&lt;/h2&gt;
&lt;p&gt;All right, let&amp;rsquo;s go. Closely mimicking the binary case, we start from the following approximated marginal probability distribution&lt;/p&gt;
&lt;p&gt;\begin{equation}
P^{[t-1:t]}_{\alpha}( \mathbf{s}_{t} ) = \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ),
\end{equation}&lt;/p&gt;
&lt;p&gt;interpolating between $P^{[t-1:t]}_{\alpha=0}( \mathbf{s}_{t} ) = Q( \mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\alpha=1}( \mathbf{s}_{t} ) = P( \mathbf{s}_{t} )$. Our lazy integral notation $\int \mathrm{d} \mathbf{s}_{t}$ should be understood as $\int \prod_{i=1}^{N} \mathrm{d}^{D} \mathbf{s}_{i, t}$, i.e. integrating over all the little spins at a fixed time $t$. The estimated mean magnetizations are&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbf{m}_{i,t}(\alpha) &amp;amp;= \int \mathrm{d} \mathbf{s}_{t} \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; \mathbf{s}_{i,t} P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ) \nonumber\\
&amp;amp;= \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ).
\end{align}&lt;/p&gt;
&lt;p&gt;The first-order derivative with respect to $\alpha$ is then given by&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha)}{\partial\alpha} = \int &amp;amp;\mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \Biggl( \frac{\partial\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial\alpha} \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) \nonumber\\
&amp;amp;+ \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, \frac{\partial P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial\alpha} \Biggr) P( \mathbf{s}_{t-2} ), \label{eq:mitfirstorderalpha}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha} = - &amp;amp; \frac{\beta}{R^2} \frac{ \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right) }{ \sqrt{1+\beta^2 \lVert \mathbf{h}_{i,t}(\alpha) \rVert^2 / R^2 } } \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;+ \frac{\beta}{1+\sqrt{1+\beta^2 \lVert \mathbf{h}_{i,t}(\alpha) \rVert^2 / R^2 }} \Delta \mathbf{h}_{i,t} \label{eq:firstorderphialpha}
\end{align}&lt;/p&gt;
&lt;p&gt;Evaluating \eqref{eq:mitfirstorderalpha} at $\alpha=0$, the second term drops out because the first-order derivative of $P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )$ becomes independent of $\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)$ and $\int \mathrm{d} \mathbf{s}_{t-1} P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )=1$. We thus end up with&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} = - &amp;amp;\frac{\beta}{R^2}\frac{\left( \mathbf{m}_{i,t} \cdot \boldsymbol{v}_{i,t} \right)}{\sqrt{1+\beta^2 \lVert \boldsymbol{\theta}_{i,t}  \rVert^2 / R^2 }} \mathbf{m}_{i,t} \nonumber \\
&amp;amp;+ \frac{\beta}{1+\sqrt{1+\beta^2 \lVert \boldsymbol{\theta}_{i,t} \rVert^2 / R^2 }}\boldsymbol{v}_{i,t} \label{eq:mfirstorderalphazero}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{v}_{i,t} = -\boldsymbol{\theta}_{i,t} + \mathbf{x}_{i,t} + \sum_{j=1}^{N} J_{ij} \mathbf{m}_{j,t-1} \label{eq:vmf}
\end{align}&lt;/p&gt;
&lt;p&gt;captures the result of integrating $\Delta \mathbf{h}_{i,t}$ over the spins $\mathbf{s}_{t-1}$. Following Eq. \eqref{eq:mftheta}, the first-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[ \alpha \frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} \right]_{\alpha=1} = \mathbf{0} + \left[ \mathcal{O}\left(\alpha^2\right)\right]_{\alpha=1},\label{eq:firstorderapproxreqs}
\end{equation}&lt;/p&gt;
&lt;p&gt;so that we are encouraged to set $\boldsymbol{v}_{i,t}=0$ and hence $\boldsymbol{\theta}^{*}_{i,t} = \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1}$, leading to the naive mean-field equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{ \mathbf{m}_{i,t} = \frac{\beta \left( \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \right)}{1+\sqrt{1+\beta^2 \lVert \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \rVert^2 / R^2 }} } \label{eq:naivemvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;Looking ahead at the transformer-module correspondence in &lt;a href=&#34;#4-a-family-of-transformer-like-modules&#34;&gt;Section 4&lt;/a&gt;, we squint our eyes and recognize a scaled sum of a residual connection and an attention term. No feed-forward terms though.&lt;/p&gt;
&lt;p&gt;Before moving on to the second-order approximation, let us end this section with an interesting observation about Eq. \eqref{eq:mfirstorderalphazero}. In &lt;a href=&#34;#a3-vector-spin-distribution-variance-second-moment&#34;&gt;Appendix A.3&lt;/a&gt;, we show that the variance matrix of a single spin in the large-$D$ limit equals a rank-1 perturbation of a diagonal matrix&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var} [ \mathbf{s}_{i,t} ] &amp;amp;= \frac{\mathbb{1}}{1+\gamma(\mathbf{h}_{i,t})} - \frac{ \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \otimes \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) }{ R^2 \gamma(\mathbf{h}_{i,t}) }, \label{eq:spinvariance}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\gamma(\mathbf{h}_{i,t}) = \sqrt{1+\beta^{2}\lVert\mathbf{h}_{i,t}\rVert^{2}/R^2},
\end{align}&lt;/p&gt;
&lt;p&gt;Taking the $\alpha \to 0$ limit of the above expressions, we can reinterpret Eq. \eqref{eq:mfirstorderalphazero} as the matrix-vector multiplication of the decoupled spin&amp;rsquo;s variance matrix with $\boldsymbol{v}_{i,t}$,&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} = \beta \mathrm{Var} [ \mathbf{s}_{i,t} ] \boldsymbol{v}_{i,t}.
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;34-second-order-thouless-anderson-palmer-approximation&#34;&gt;3.4. Second-order Thouless-Anderson-Palmer approximation&lt;/h2&gt;
&lt;p&gt;Let us now try to find out whether going to the second-order approximation spits out additional feed-forward like terms in the update equations for the magnetizations.&lt;/p&gt;
&lt;p&gt;Again following Eq. \eqref{eq:mftheta}, the second-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[ \alpha \frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} \right]_{\alpha=1} + \left[ \frac{\alpha^2}{2} \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha}\right]_{\alpha=1} = \mathbf{0} + \left[ \mathcal{O}\left(\alpha^3\right)\right]_{\alpha=1}, \label{eq:secondorderconstraint}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the second-order derivative is given by&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha)}{\partial^{2}\alpha} = \int &amp;amp;\mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \Biggl( \frac{\partial^{2}\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial^{2}\alpha} \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) \nonumber\\
&amp;amp;+ 2\frac{\partial\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial\alpha} \, \frac{\partial P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial\alpha} \nonumber \\
&amp;amp;+ \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, \frac{\partial^{2} P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial^{2}\alpha} \Biggr) P( \mathbf{s}_{t-2} ). \label{eq:mhasecordder}
\end{align}&lt;/p&gt;
&lt;p&gt;Evaluated at $\alpha=0$, the third term in the expression above will drop out because the derivative becomes independent of $\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)$ and $\int \mathrm{d} \mathbf{s}_{t-1} P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )=1$.&lt;/p&gt;
&lt;p&gt;The first term in Eq. \eqref{eq:mhasecordder} can be shown to look something like&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha^2} = &amp;amp; \frac{\beta^2}{R^4} \frac{ 1+\gamma(\alpha) }{ \gamma(\alpha)^3 } \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right)^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta}{R^2} \frac{1}{\gamma(\alpha)} \left( \frac{\partial\boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha} \cdot \Delta \mathbf{h}_{i,t} \right) \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta}{R^2} \frac{1}{\gamma(\alpha)} \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))  \cdot \Delta \mathbf{h}_{i,t} \right) \frac{\partial\boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha} \nonumber \\
&amp;amp;- \frac{\beta^2}{R^2} \frac{1}{\gamma(\alpha)^2 + \gamma(\alpha) } \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right) \Delta \mathbf{h}_{i,t},
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\gamma (\alpha) \equiv \gamma(\mathbf{h}_{i,t}(\alpha)) = \sqrt{1+\beta^2 \lVert \mathbf{h}_{i,t}(\alpha) \rVert^2 / R^2 },
\end{align}&lt;/p&gt;
&lt;p&gt;which, after substituting the first-order derivative Eq. \eqref{eq:firstorderphialpha}, simplifies to&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha^2} = &amp;amp; \frac{\beta^2}{R^4} \frac{ 1+3\gamma(\alpha) }{ \gamma(\alpha)^3 } \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right)^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta^2}{R^2} \frac{1}{\gamma(\alpha)^2 + \gamma(\alpha)} \left( \Delta \mathbf{h}_{i,t} \cdot \Delta \mathbf{h}_{i,t} \right) \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta^2}{R^2} \frac{2}{\gamma(\alpha)^2 + \gamma(\alpha)} \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))  \cdot \Delta \mathbf{h}_{i,t} \right) \Delta \mathbf{h}_{i,t} . \label{eq:secondorderphialpha}
\end{align}&lt;/p&gt;
&lt;p&gt;The second term in Eq. \eqref{eq:mhasecordder} contains non-vanishing contributions in the $\alpha \to 0$ limit coming from the $\sum_{j=1}^{N} J_{ij} \mathbf{s}_{j, t-1}$ terms in $\Delta \mathbf{h}_{i,t}$. One can show that the surviving terms in the integrand are proportional to&lt;/p&gt;
&lt;p&gt;\begin{align}
\sum_{j} J_{ij} \Biggl( &amp;amp;\frac{2 \beta^2}{1+\gamma(\alpha)} \frac{\partial\mathbf{m}_{j, t-1}(\alpha)}{\partial\alpha} \nonumber \\
&amp;amp;- \frac{2 \beta^2}{R^2 \gamma(\alpha)} \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \frac{\partial\mathbf{m}_{j, t-1}(\alpha)}{\partial\alpha} \right) \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \Biggr),
\end{align}&lt;/p&gt;
&lt;p&gt;which we can ignore since they are $\mathcal{O}(\alpha)$ on their own, and thus $\mathcal{O}(\alpha^3)$ when multiplied with $\alpha^2$ in the second-order approximation.&lt;/p&gt;
&lt;p&gt;Before taking the $\alpha \to 0$ limit of whatever is left in Eq. \eqref{eq:mhasecordder}, we list a few useful tricks to make the evaluation easier. First of all, we use Eq. \eqref{eq:vmf} to introduce the following sneaky substitution&lt;/p&gt;
&lt;p&gt;\begin{align}
\Delta \mathbf{h}_{i,t} = -\boldsymbol{\theta}_{i,t} + \mathbf{x}_{i,t} + \sum_{j=1}^{N} J_{ij} \mathbf{s}_{j,t-1} = \boldsymbol{v}_{i,t} + \sum_{j=1}^{N} J_{ij} \left( \mathbf{s}_{j,t-1} - \mathbf{m}_{j,t-1} \right),
\end{align}&lt;/p&gt;
&lt;p&gt;which conveniently separates terms with fluctuating spin variables from magnetizations that can be pulled out of the integrals. Secondly, all terms that contain only one spin variable with a dependence looking like $\mathbf{s}_{j,t-1} - \mathbf{m}_{j,t-1}$ drop out because, schematically,&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbf{s}_{j,t-1} - \mathbf{m}_{j,t-1} \overset{\int \mathrm{d} \mathbf{s}_{t-1}}{\to} \boldsymbol{\varphi}(\mathbf{h}_{j,t}(\alpha)) - \mathbf{m}_{j,t-1} \overset{\alpha \to 0}{\to} \mathbf{0}.
\end{align}&lt;/p&gt;
&lt;p&gt;Thirdly, since the $\alpha \to 0$ limit decouples all spins $\mathbf{s}_{t-1}$, any term containing dot products $(\mathbf{s}_{j,t-1}-\mathbf{m}_{j,t-1}) \cdot (\mathbf{s}_{k,t-1}-\mathbf{m}_{k,t-1})$ of two spin variables is zero for $j \neq k$ and equal to $R^2 - \mathbf{m}^2_{j,t-1}$ for $j=k$. We will also encounter terms containing (tensor contractions with) outer products $(\mathbf{s}_{j,t-1}-\mathbf{m}_{j,t-1}) \otimes (\mathbf{s}_{k,t-1}-\mathbf{m}_{k,t-1})$, which we can think of as projection operators. For $j \neq k$, these and similar terms again evaluate to zero, while, for $j=k$, we get the variance contributions we mentioned previously in Eq. \eqref{eq:spinvariance} at the end of the previous section.&lt;/p&gt;
&lt;p&gt;Finally, we take the $\alpha \to 0$ limit of Eq. \eqref{eq:mhasecordder} only to end up with the following mess:&lt;/p&gt;
&lt;p&gt;\begin{align}
&amp;amp;\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha} = \label{eq:secondordercorrections} \\
\end{align}
\begin{align}
&amp;amp;\hspace{-1em}\frac{\beta^2}{R^4} \frac{1+3\gamma(0)}{\gamma(0)^3} \left( \left( \mathbf{m}_{i,t} \cdot \mathbf{v}_{i,t} \right)^2 + \sum_{j} J_{ij} \left( \frac{\mathbf{m}_{i,t}^2}{1+\gamma(0)} - \frac{\left(\mathbf{m}_{i,t}\cdot\mathbf{m}_{j,t-1}\right)^2}{R^2 \gamma(0)} \right) \right) \mathbf{m}_{i,t}  \nonumber \\
&amp;amp;\hspace{-1em}- \frac{\beta^2}{R^2} \frac{1}{\gamma^2 (0) + \gamma(0)} \left( \mathbf{v}_{i,t}^2 + \sum_{j} J_{ij} \left( R^2 - \mathbf{m}_{j,t-1}^2 \right) \right) \mathbf{m}_{i,t} \nonumber \\
&amp;amp;\hspace{-1em}- \frac{\beta^2}{R^2} \frac{2}{\gamma^2 (0) + \gamma(0)} \Biggr( \mathbf{v}_{i,t} \otimes \mathbf{v}_{i,t} \sum_{j} J_{ij} \left( \frac{\mathbb{1}}{1+\gamma(0)} - \frac{\mathbf{m}_{j,t-1}\otimes\mathbf{m}_{j,t-1}}{R^2 \gamma(0)} \right) \Biggr) \mathbf{m}_{i,t} \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;At this point, it is too late. We should have remembered that the second-order approximation lives in the neighborhood of the first-order approximation. We probably ended up doing too much work by taking terms into account that are of higher order in $\alpha$. Oh well, we can always drop terms later on if it turns out they are neglible at $\mathcal{O}(\alpha^2)$.&lt;/p&gt;
&lt;p&gt;To get to the second-order mean-field equations for the magnetizations, we just have to solve Eq. \eqref{eq:secondorderconstraint} for the optimal parameters $\boldsymbol{\theta}^{*}_{i,t}$, i.e.,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} + \frac{1}{2} \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha} = \mathbf{0} + \mathcal{O}\left(\alpha^3\right).
\end{equation}&lt;/p&gt;
&lt;p&gt;Let us substitute $\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha}$ from Eq. \eqref{eq:mfirstorderalphazero} but keep $\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha}$ for generality,&lt;/p&gt;
&lt;p&gt;\begin{align}
\beta \left( \frac{\mathbb{1}}{1+\gamma(0)} - \frac{\mathbf{m}_{i,t}\otimes\mathbf{m}_{i,t}}{R^2 \gamma(0)} \right) \mathbf{v}_{i,t} + \frac{1}{2} \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha} = \mathbf{0} + \mathcal{O}\left(\alpha^3\right),
\end{align}&lt;/p&gt;
&lt;p&gt;so that we can then isolate $\boldsymbol{\theta}_{i,t}$ in $\mathbf{v}_{i,t}$ to find&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\theta}_{i,t} = \mathbf{x}_{i,t} &amp;amp;+ \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \nonumber \\
&amp;amp;+ \frac{1+\gamma(0)}{2\beta} \left( \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha} + \frac{\mathbf{m}_{i,t} \cdot \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha}}{\frac{R^2 \gamma(0)}{1+\gamma(0)} - \mathbf{m}_{i,t}^2} \mathbf{m}_{i,t} \right),\label{eq:ftheta}
\end{align}&lt;/p&gt;
&lt;p&gt;where we have used the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sherman–Morrison formula&lt;/a&gt; to compute the inverse of the variance matrix. Since the expression on the right-hand side &lt;em&gt;also&lt;/em&gt; depends on $\boldsymbol{\theta}_{i,t}$, we seem to have stumbled upon a set of fixed-point equations&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\theta}_{i,t} = \mathbf{f} (\boldsymbol{\theta}_{i,t}, \mathbf{x}_{i,t}, \mathbf{m}_{i,t}, \mathbf{m}_{t-1}), \label{eq:thetafp}
\end{align}&lt;/p&gt;
&lt;p&gt;which we should solve numerically to find $\boldsymbol{\theta}^{*}_{i,t}$. The second-order mean-field equations then become &lt;em&gt;yet another&lt;/em&gt; set of fixed-point equations&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{\mathbf{m}_{i,t} = \boldsymbol{\varphi} \left(\boldsymbol{\theta}^{*}_{i,t}(\mathbf{x}_{i,t}, \mathbf{m}_{i,t}, \mathbf{m}_{t-1})\right) } \label{eq:tapmvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;because of the dependence of $\boldsymbol{\theta}^{*}_{i,t}$ on $\mathbf{m}_{i,t}$. Similar to the binary TAP approximation Eq. \eqref{eq:tapm}, this dependency suggests that we should solve for fixed-point magnetizations $\mathbf{m}^{*}_{i,t}$. However, in contrast to the binary case, the dependence here is &lt;em&gt;implicit&lt;/em&gt; since $\boldsymbol{\theta}^{*}_{i,t}$ is itself obtained from solving fixed-point equations Eq. \eqref{eq:thetafp}, which, in turn, also depend on $\mathbf{m}_{i,t}$. We should thus try to jointly optimize for the fixed-point tuples $(\boldsymbol{\theta}^{*}_{i,t}, \mathbf{m}^{*}_{i,t})$ such that the following pair of equations holds for every site:&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\theta}^{*}_{i,t} &amp;amp;= \mathbf{f} (\boldsymbol{\theta}^{*}_{i,t}, \mathbf{x}_{i,t}, \mathbf{m}^{*}_{i,t}, \mathbf{m}_{t-1}) \\
\mathbf{m}^{*}_{i,t} &amp;amp;= \boldsymbol{\varphi} \left( \boldsymbol{\theta}^{*}_{i,t}(\mathbf{x}_{i,t}, \mathbf{m}^{*}_{i,t}, \mathbf{m}_{t-1}) \right),
\end{align}&lt;/p&gt;
&lt;p&gt;where the function $\mathbf{f}$ is given by the right-hand side of Eq. \eqref{eq:ftheta}.&lt;/p&gt;
&lt;p&gt;Looking ahead at the transformer-module correspondence in &lt;a href=&#34;#4-a-family-of-transformer-like-modules&#34;&gt;Section 4&lt;/a&gt;, we recognize a scaled sum of a residual connection, an attention term, and a self-consistent expression in terms of magnetizations and couplings taking on the role of the feed-forward network. Interestingly, these second-order correction terms require &lt;em&gt;no additional free parameters&lt;/em&gt; since they are &lt;em&gt;fully determined by the mean-field structure&lt;/em&gt; of the underlying spin model.&lt;/p&gt;
&lt;h2 id=&#34;35-a-simple-jax-implementation&#34;&gt;3.5. A simple JAX implementation&lt;/h2&gt;
&lt;p&gt;We now turn to a JAX implementation of the mean-field time evolution of the magnetizations according to the vector-spin model introduced in the previous sections.&lt;/p&gt;
&lt;p&gt;$\ldots$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial

import jax
import jax.numpy as jnp

from jaxopt import AndersonAcceleration


def gamma(x, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (48).&amp;quot;&amp;quot;&amp;quot;
    return jax.lax.sqrt(1 + beta**2 * jnp.sum(x * x, axis=-1, keepdims=True) / R**2)


def phi(x, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (38).&amp;quot;&amp;quot;&amp;quot;
    return beta / (1 + gamma(x, beta, R)) * x


def update_naive_mf(m0, _, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (46).&amp;quot;&amp;quot;&amp;quot;
    theta = x + jnp.einsum(&amp;quot;i j, j d -&amp;gt; i d&amp;quot;, J, m0)
    m1 = phi(theta, beta, R)
    return m1, m0


def second_order_corrections(theta, m1, m0, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (58).&amp;quot;&amp;quot;&amp;quot;
    g0 = gamma(theta, beta, R)
    v = -theta + x + jnp.einsum(&amp;quot;i j, j d-&amp;gt; i d&amp;quot;, J, m0)
    return (
        (beta**2 * (1 + 3 * g0))
        / (R**4 * g0**3)
        * (
            jnp.einsum(&amp;quot;i d, i d -&amp;gt; i&amp;quot;, m1, v)[:, None]
            + jnp.einsum(
                &amp;quot;i j, i d -&amp;gt; i d&amp;quot;,
                J,
                jnp.sum(m1 * m1, axis=-1, keepdims=True) / (1 + g0),
            )
            - jnp.einsum(
                &amp;quot;i j, i d, j d, i e, j e -&amp;gt; i&amp;quot;,
                J,
                m1,
                m0,
                m1,
                m0 / (R**2 * g0),
            )[:, None]
        )
        * m1
        - (beta**2)
        / (R**2 * (g0**2 + g0))
        * (
            jnp.sum(v * v, axis=-1, keepdims=True)
            + jnp.einsum(
                &amp;quot;i j, j d-&amp;gt; i d&amp;quot;,
                J,
                R**2 - jnp.sum(m0 * m0, axis=-1, keepdims=True),
            )
        )
        * m1
        - 2
        * beta**2
        / (R**2 * (g0**2 + g0))
        * (
            jnp.einsum(&amp;quot;i d, i d, i f -&amp;gt; i f&amp;quot;, v, m1, v)
            + jnp.einsum(&amp;quot;i j, i d -&amp;gt; i d&amp;quot;, J, m1 / (1 + g0))
            - jnp.einsum(
                &amp;quot;i j, i d, j d, j f -&amp;gt; i f&amp;quot;,
                J,
                m1,
                m0,
                m0 / (R**2 * g0),
            )
        )
    )


def f(theta, m1, m0, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (61).&amp;quot;&amp;quot;&amp;quot;
    attn = jnp.einsum(&amp;quot;i j, j d-&amp;gt; i d&amp;quot;, J, m0)

    g0 = gamma(theta, beta, R)
    d2_m_d2_alpha_0 = second_order_corrections(
        theta, m1, m0, x, J, beta, R
    )
    ff = (
        (1 + g0)
        / (2 * beta)
        * (
            d2_m_d2_alpha_0
            + (
                jnp.einsum(&amp;quot;i d, i d -&amp;gt; i&amp;quot;, m1, d2_m_d2_alpha_0)[:, None]
                / ((R**2 * g0) / (1 + g0) - jnp.sum(m1 * m1, axis=-1, keepdims=True))
                * m1
            )
        )
    )
    return x + attn + ff


def update_tap_mf(m0, _, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (64) and Eq. (65).&amp;quot;&amp;quot;&amp;quot;

    def _fixed_point_fun(params, _m0, _x, _J, _beta, _R):
        theta, m1 = params
        theta_next = f(theta, m1, _m0, _x, _J, _beta, _R)
        m1_next = phi(theta_next, _beta, _R)
        return (theta_next, m1_next)

    theta_init, m1_init = x + J @ m0, m0
    _, m1 = (
        AndersonAcceleration(
            fixed_point_fun=_fixed_point_fun,
            tol=1e-3,
            maxiter=10,
        )
        .run((theta_init, m1_init), m0, x, J, beta, R)
        .params
    )
    return m1, m0


def time_evolution(m0, steps, update_fun):
    final_carry, stacked_outputs = jax.lax.scan(update_fun, init=m0, xs=steps)
    return final_carry, stacked_outputs


def simulate(x, J, m0, steps, beta, update_fun=update_tap_mf):
    R = (m0.shape[-1] / 2 - 1) ** 0.5
    wrapped_time_evolution = partial(
        time_evolution,
        steps=steps,
        update_fun=partial(update_fun, x=x, J=J, beta=beta, R=R),
    )
    final_carry, stacked_outputs = jax.vmap(wrapped_time_evolution)(m0)
    return final_carry, stacked_outputs
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$\ldots$&lt;/p&gt;
&lt;h1 id=&#34;4-a-family-of-transformer-like-modules&#34;&gt;4. A family of transformer-like modules&lt;/h1&gt;
&lt;p&gt;In this final section, we propose a physics-inspired class of transformer modules based on the mean-field update equations for the vector-spin magnetizations derived in the previous section. We highlight conceptual similarities, physical interpretations, and potential benefits of exploiting spin-model structure to reduce parameter count.&lt;/p&gt;
&lt;h2 id=&#34;41-connecting-the-dots&#34;&gt;4.1. Connecting the dots&lt;/h2&gt;
&lt;p&gt;Following &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms (2021)&lt;/a&gt; and &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/#5-why-dont-we-just-probe-a-vector-spin-system-with-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers Are Secretly Collectives of Spin Systems (2021)&lt;/a&gt;, we interpret a transformer module as a differentiable vector-spin system that is driven by data and whose collective behavior can be shaped through training. Intuitively, there is little difference compared to the work mentioned above: we still probe a spin system and observe its response. But, technically and conceptually, the shift to non-equilibrium phenomena and dynamical mean-field expressions enables us to solidify the correspondence by moving past symmetric coupling matrices and equilibrium free energies.&lt;/p&gt;
&lt;p&gt;We define a &lt;em&gt;spin-model transformer module&lt;/em&gt; as a wrapper around a vector-spin model where module inputs $\mathbf{x} \in \mathbb{R}^{N \times D}$ get routed to external magnetic fields. Inside the module, we evolve a set of initial magnetizations in time using either the first-order (Eq. \eqref{eq:naivemvector}) or the second-order (Eq. \eqref{eq:tapmvector}) mean-field update equations. Only the second-order update equations exhibit feed-forward-like corrections. We choose to keep applying the same external magnetic fields at all time steps ($\mathbf{x}_{t} \equiv \mathbf{x}$, $\forall t \geq 0$) and construct the couplings once from the initial inputs via a row-stochastic softmax attention matrix,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{J}(\mathbf{x}) = \mathrm{softmax}\left( \frac{\boldsymbol{x} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{x}^{T}}{\sqrt{D}} \right). \label{eq:softmaxcouplings}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\boldsymbol{W}_{\boldsymbol{Q}}$ and $\boldsymbol{W}_{\boldsymbol{K}}$ denote the familiar linear query- and key-mappings. Adding bias terms to these linear transformations would introduce intrinsic interactions between the spins, which persist even in the absence of the external magnetic fields. Essentially, we recognize the softmax attention matrix as a parametrized input-dependent flavor of the (asymmetric) coupling matrix of a vector-spin model.&lt;/p&gt;
&lt;img src=&#34;arch_comparison.png&#34; alt=&#34;Comparison between vanilla transformer module and spin-model transformer module&#34; width=&#34;650px&#34;/&gt;
&lt;p&gt;What does the module return? The within-module time evolution is said to converge when the mean magnetizations collectively reach a &lt;em&gt;non-equilibrium steady-state&lt;/em&gt;, which is not guaranteed a priori and requires us to make sure the couplings, inverse temperature, and normalizations are sensibly chosen. Numerically, we can monitor the convergence of the magnetizations obtained from iterating the update equations. If the within-module time evolution converges, we return the non-equilibrium steady-state (NESS) magnetizations $\mathbf{m}_{\mathrm{NESS}} \in \mathbb{R}^{N \times D}$ as module outputs.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;✨ Instead of time evolving for a number of steps until convergence, we could also try hunting for the non-equilibrium steady state directly by assuming it exists and solving for it as if it were a fixed point of the time evolution. For the first-order update equations Eq. \eqref{eq:naivemvector}, this would mean solving for the fixed-point $\mathbf{m}^{\mathrm{NESS}}$ so that&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\theta}^{\mathrm{NESS}}_{i} &amp;amp;= \mathbf{x}_{i} + \sum_{j} J_{ij} \mathbf{m}^{\mathrm{NESS}}_{j} \\
\mathbf{m}^{\mathrm{NESS}}_{i} &amp;amp;= \boldsymbol{\varphi} \left( \boldsymbol{\theta}^{\mathrm{NESS}}_{i} \right),
\end{align}&lt;/p&gt;
&lt;p&gt;while, for the second-order update equations Eq. \eqref{eq:tapmvector}, we would need to solve&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\theta}^{\mathrm{NESS}}_{i} &amp;amp;= \mathbf{f} (\boldsymbol{\theta}^{\mathrm{NESS}}_{i}, \mathbf{x}_{i}, \mathbf{m}^{\mathrm{NESS}}) \\
\mathbf{m}^{\mathrm{NESS}}_{i} &amp;amp;= \boldsymbol{\varphi} \left( \boldsymbol{\theta}^{\mathrm{NESS}}_{i} (\mathbf{x}_{i}, \mathbf{m}^{\mathrm{NESS}}) \right),
\end{align}&lt;/p&gt;
&lt;p&gt;where the distinction between time steps $t$ and $t-1$ has disappeared in our notation.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;To wrap up this section, we list a few conceptual similarities and features below to close the gap between vector-spin models and transformer modules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attention heads:&lt;/strong&gt; Multiple attention heads can be implemented by embedding $N_{h}$ coupling matrices into a head-block-diagonal coupling tensor. Effectively, this operation stacks $N_{h}$ smaller-dimensional spin models where each submodel processes a disjoint $D_{h}-$dimensional piece of the full $D-$dimensional vector space. Mixing between subspaces can occur because (1) each individual coupling matrix is still constructed from query/key mappings $\mathbb{R}^{D} \to \mathbb{R}^{N_{h} \times D_{h}}$ acting on the full input space, and (2) the dot products in the second-order correction terms Eq. \eqref{eq:secondordercorrections} naturally mix channels. We refer to the explicit code in &lt;a href=&#34;#43-a-simple-jax-implementation&#34;&gt;Section 4.3&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Causal masks:&lt;/strong&gt; Since we identify the attention matrix with the spin-model couplings, autoregressive modeling can be done by applying the appropriate triangular mask to the coupling matrix instead. The (spatial) causal structure is preserved during the within-module time evolution. More generally, we expect any kind of masking done on the level of the attention matrix to transfer to the coupling matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-attention:&lt;/strong&gt; The framework described above implements self-attention by constructing both queries and keys from the inputs $x$ according to Eq. \eqref{eq:softmaxcouplings}. Decoder layers in encoder-decoder models, however, rely on cross-attention, where keys (and values) from the encoder output are sent to the decoder input as context. We can accommodate this scenario by feeding the spin-transformer module an additional set of context vectors $\mathbf{c}$ to build the couplings matrix, i.e.,&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\mathbf{J}(\mathbf{x}, \mathbf{c}) = \mathrm{softmax}\left( \frac{\boldsymbol{x} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{c}^{T}}{\sqrt{D}} \right). \label{eq:crosssoftmaxcouplings}
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normalization:&lt;/strong&gt; A flavor of &lt;a href=&#34;https://github.com/lucidrains/x-transformers#root-mean-square-layer-normalization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Root Mean Square Layer Normalization&lt;/a&gt; (RMSNorm) naturally appears in expression Eq. \eqref{eq:largedevmag} for the magnetization in the limit of large vector dimension as well as in all the mean-field update equations derived from it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sparse computation:&lt;/strong&gt; $\ldots$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;42-fast--and-slow-moving-parameters&#34;&gt;4.2. Fast- and slow-moving parameters&lt;/h2&gt;
&lt;p&gt;We now provide some physical intuition. As mentioned ad nauseam in &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms (2021)&lt;/a&gt;  and &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers Are Secretly Collectives of Spin Systems (2021)&lt;/a&gt;, each example in a batch of sequential data can be thought of as probing a transformer module in a particular way. The response of the many-body system depends on the context provided by the applied external fields. We can tune the collective response behavior by parametrizing the couplings and making sure the whole probe-response stack is differentiable.&lt;/p&gt;
&lt;img src=&#34;spin_model_transformer_module.png&#34; alt=&#34;Focus on a spin-model transformer module in a stack of layers&#34; width=&#34;550px&#34;/&gt;
&lt;p&gt;Physically, the &lt;em&gt;fast-moving&lt;/em&gt; parameterized couplings $\mathbf{J}(\mathbf{x})$ are determined by the &lt;em&gt;fast-moving&lt;/em&gt; parameterized external fields $\mathbf{x}$, which, in a stack of transformer modules, depend on the magnetizations of the previous layer and ultimately on the input data. The external fields act as an environment of contextual patterns that gets transformed instantly into the values of the coupling matrix, effectively inducing some kind of state of quenched disorder. The &lt;em&gt;slow-moving&lt;/em&gt; parameters are those receiving gradient updates during training, e.g., the query-key matrices in the softmax couplings. On the level of a spin-model transformer module, training can be understood as &lt;em&gt;shaping the input-dependent distribution of coupling parameters&lt;/em&gt; by amassing information from a huge amount of quenched disorder realizations, sculpting a spin glass with data.&lt;/p&gt;
&lt;h2 id=&#34;43-a-simple-jax-implementation&#34;&gt;4.3. A simple JAX implementation&lt;/h2&gt;
&lt;p&gt;$\ldots$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import jax
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;$\ldots$&lt;/p&gt;
&lt;h1 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;$\ldots$&lt;/p&gt;
&lt;p&gt;Computational experiments at scale should inform us about potential bottlenecks of spin-transformer models in terms of &lt;a href=&#34;https://twitter.com/YiTayML/status/1714315484357857766&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;efficiency&lt;/a&gt;, representational power, and scaling behavior.&lt;/p&gt;
&lt;p&gt;$\ldots$&lt;/p&gt;
&lt;h1 id=&#34;appendices&#34;&gt;Appendices&lt;/h1&gt;
&lt;h2 id=&#34;a1-vector-spin-distribution-normalization-constant&#34;&gt;A.1. Vector-spin distribution: normalization constant&lt;/h2&gt;
&lt;p&gt;We consider the single-site vector-spin distribution Eq. \eqref{eq:pcondsinglesitevector}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }.
\end{equation}&lt;/p&gt;
&lt;p&gt;Let $Z(\beta, R, \mathbf{h})=\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}$. We switch to $D$-dimensional spherical coordinates to make our life easier and use rotational symmetry to choose the polar axis parallel to $\mathbf{h}$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = R^{D-1} \int_{\Omega} \int_{0}^{\pi} \mathrm{d}^{D-2} \Omega \;\mathrm{d}\theta \; \mathrm{e}^{\beta R h \cos \theta } \sin^{D-2} \theta ,
\end{equation}&lt;/p&gt;
&lt;p&gt;where $h=\lVert\mathbf{h}\rVert$ and where $\int_{\Omega} \mathrm{d}^{D-2} \Omega$ represents the integral over all other spherical angles, which coincides with the surface area of the unit sphere in $D-1$ dimensions,&lt;/p&gt;
&lt;p&gt;\begin{equation}
S_{D-1} = \frac{2\pi^{\frac{D-1}{2}}}{\Gamma\left( \frac{D-1}{2} \right)},
\end{equation}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = \frac{2 \pi^{\frac{D-1}{2}} R^{D-1}}{\Gamma\left( \frac{D-1}{2} \right)} \int_{0}^{\pi} \mathrm{d}\theta \; \mathrm{e}^{\beta R h \cos \theta } \sin^{D-2} \theta .
\end{equation}&lt;/p&gt;
&lt;p&gt;If we now let $u = \cos \theta$, then&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = \frac{2 \pi^{\frac{D-1}{2}} R^{D-1}}{\Gamma\left( \frac{D-1}{2} \right)} \int_{-1}^{1} \mathrm{d}u \; \mathrm{e}^{\beta R h u } \left(1 - u^2\right)^{(D-3)/2} .
\end{equation}&lt;/p&gt;
&lt;p&gt;Recognizing &lt;a href=&#34;https://dlmf.nist.gov/10.32#i&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an integral representation of the modified Bessel function of the first kind&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;\begin{equation}
I_{\nu}(z) = \frac{2^{-\nu}}{\sqrt{\pi}\, \Gamma\left(\nu+\frac{1}{2}\right)} z^{\nu} \int_{-1}^{1} \mathrm{d}t \; \mathrm{e}^{\pm zt} \left(1-t^2\right)^{\nu-\frac{1}{2}},
\end{equation}&lt;/p&gt;
&lt;p&gt;we identify $\nu = D/2 - 1$ and $z = \beta R h$ to find&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R h) }{ \left(\beta h\right)^{D/2-1} }.
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;a2-vector-spin-distribution-expected-value-first-moment&#34;&gt;A.2. Vector-spin distribution: expected value (first moment)&lt;/h2&gt;
&lt;p&gt;We consider the single-site vector-spin distribution Eq. \eqref{eq:pcondsinglesitevector}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }.
\end{equation}&lt;/p&gt;
&lt;p&gt;Starting from the expression of the normalization constant Eq. \eqref{eq:partfun},&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } = Z(\beta, R, \lVert \mathbf{h}\rVert) ,
\end{equation}&lt;/p&gt;
&lt;p&gt;we write the expected value as&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbb{E}_{p} [ \mathbf{s} ] = \frac{1}{Z} \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathbf{s} \, \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}
\end{equation}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbb{E}_{p} [ \mathbf{s} ] = \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \left( \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert\mathbf{h} \rVert) }{ \left(\beta \lVert\mathbf{h}\rVert \right)^{D/2-1} } \right)
\end{align}&lt;/p&gt;
&lt;p&gt;which evaluates to&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbb{E}_{p} [ \mathbf{s} ] = \left( \frac{I&amp;rsquo;_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert)}{I_{D/2 - 1}(\beta R \lVert\mathbf{h}\rVert)} - \frac{ D/2-1 }{ \beta R \lVert\mathbf{h}\rVert} \right) \frac{R \mathbf{h}}{\lVert\mathbf{h}\rVert}.
\end{align}&lt;/p&gt;
&lt;p&gt;Using the &lt;a href=&#34;https://dlmf.nist.gov/10.29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modified Bessel function recurrence relations&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;\begin{align}
I_{\nu-1}(z) - I_{\nu+1}(z) &amp;amp;= \frac{2\nu}{z} I_{\nu}(z), \label{eq:irecurr}\\
I_{\nu-1}(z) + I_{\nu+1}(z) &amp;amp;= 2 I&#39;_{\nu}(z), \label{eq:irecurrderiv}
\end{align}&lt;/p&gt;
&lt;p&gt;we end up with&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbb{E}_{p} [ \mathbf{s} ] = \frac{I_{D/2}(\beta R \lVert \mathbf{h}\rVert)}{I_{D/2 - 1}(\beta R \lVert\mathbf{h}\rVert)} \frac{R \mathbf{h}}{\lVert\mathbf{h}\rVert}\equiv \boldsymbol{\varphi} (\mathbf{h}). \label{eq:app:expectedvalue}
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;a3-vector-spin-distribution-variance-second-moment&#34;&gt;A.3. Vector-spin distribution: variance (second moment)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ &lt;strong&gt;TODO:&lt;/strong&gt; Add variance for general case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We consider the single-site vector-spin distribution Eq. \eqref{eq:pcondsinglesitevector}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }.
\end{equation}&lt;/p&gt;
&lt;p&gt;Using the expression of the normalization constant Eq. \eqref{eq:partfun},&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } = Z(\beta, R, \lVert \mathbf{h}\rVert) ,
\end{equation}&lt;/p&gt;
&lt;p&gt;we write the symmetric outer-product variance matrix as&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \frac{1}{Z} \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} \, ( \mathbf{s} - \mathbb{E}_{p} [ \mathbf{s} ])( \mathbf{s} - \mathbb{E}_{p} [ \mathbf{s} ])^{T} \\
&amp;amp;= \frac{1}{\beta^2 Z} \frac{ \partial^2 }{ \partial \mathbf{h} \partial \mathbf{h}^{T} } \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} - \mathbb{E}_{p} [ \mathbf{s} ] \mathbb{E}_{p} [ \mathbf{s} ]^{T},
\end{align}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \left( Z \mathbb{E}_{p} [ \mathbf{s} ]^{T} \right) - \mathbb{E}_{p} [ \mathbf{s} ] \mathbb{E}_{p} [ \mathbf{s} ]^{T}, \\
&amp;amp;= \frac{1}{\beta} \frac{ \partial }{ \partial \mathbf{h} } \mathbb{E}_{p} [ \mathbf{s} ]^{T},
\end{align}&lt;/p&gt;
&lt;p&gt;which evaluates to&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \ldots \label{eq:app:var}
\end{align}&lt;/p&gt;
&lt;p&gt;for the general case with the expected value given by Eq. \eqref{eq:app:expectedvalue} and to&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \frac{\mathbb{1}}{1+\gamma(\mathbf{h})} - \frac{\beta^2\mathbf{h} \otimes \mathbf{h}}{R^2\gamma(\mathbf{h})\left(1+\gamma(\mathbf{h})\right)^2}\\
&amp;amp;= \frac{\mathbb{1}}{1+\gamma(\mathbf{h})} - \frac{\boldsymbol{\varphi} (\mathbf{h}) \otimes \boldsymbol{\varphi}(\mathbf{h})}{R^2\gamma(\mathbf{h})}
\end{align}&lt;/p&gt;
&lt;p&gt;for the large-$D$ limit with the expected value given by Eq. \eqref{eq:largedevmag}, where&lt;/p&gt;
&lt;p&gt;\begin{align}
\gamma(\mathbf{h}) = \sqrt{1+\beta^{2}\lVert\mathbf{h}\rVert^{2}/R^2}
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;a4-ratio-of-modified-bessel-functions-of-the-first-kind&#34;&gt;A.4. Ratio of modified Bessel functions of the first kind&lt;/h2&gt;
&lt;p&gt;To compute the ratio $I_{\nu+1}(x) / I_{\nu}(x)$ of modified Bessel functions of the first kind for $\nu \geq 0$ and $x \geq 0$, we implement a &lt;a href=&#34;https://github.com/mcbal/spin-model-transformers/blob/main/spin_model_transformers/bessel.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JAX version&lt;/a&gt; of the algorithm described in &lt;a href=&#34;https://www.jstor.org/stable/2005830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Amos, 1974)&lt;/a&gt;. A pseudocode implementation can be found in &lt;a href=&#34;https://isas.iar.kit.edu/pdf/ACC13_Kurz.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Kurz et al., 2013)&lt;/a&gt;. We compare our implementation against explicitly calculating the ratio using &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.ive.html#scipy.special.ive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;scipy.special.ive&lt;/code&gt;&lt;/a&gt; across a range of orders $\nu$ for several different values of $x$ to get a feel for its behavior.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bessel_plot_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We observe a satisfying agreement between the two approaches. For $x=\sqrt{\nu}$, the ratio takes on very small values for large orders. For $x=\nu^2$, the oppositive happens and we see saturation. The case $x=\nu$ seems to sit in between, which suggests it might be opportune to fix the radius of our little spins to $R=\sqrt{D}$ so that with $\lVert\mathbf{h}\rVert \sim \mathcal{O}(\sqrt{D})$ we might maximize the &amp;ldquo;sensitivity&amp;rdquo; of the expected value. In this regime, we can get away with &lt;a href=&#34;https://link.springer.com/article/10.1007/BF02764812&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;known asymptotic expansions&lt;/a&gt; for large $\nu$ given that the ratio flattens out quickly.&lt;/p&gt;
&lt;h2 id=&#34;a5-general-case-partial-derivatives-with-respect-to-alpha&#34;&gt;A.5. General case: partial derivatives with respect to $\alpha$&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ &lt;strong&gt;TODO:&lt;/strong&gt; Clean up and verify.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are interested in computing the first-order and second-order derivative with respect to $\alpha$ of the function&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{\varphi}(\mathbf{h}(\alpha)) = \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \mathbf{h}(\alpha)}{\lVert \mathbf{h}(\alpha) \rVert},
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}(\alpha) = \boldsymbol{\theta} + \alpha \Delta \mathbf{h}$. Using&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial \lVert \mathbf{h}(\alpha) \rVert}{\partial\alpha} = \frac{\mathbf{h}(\alpha) \cdot \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert}
\end{equation}&lt;/p&gt;
&lt;p&gt;and Eqs. \eqref{eq:irecurr}-\eqref{eq:irecurrderiv}, we find&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha} = \beta &amp;amp;\lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \nonumber \\
&amp;amp;+ \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert} \label{eq:generalgradalphafirstorder}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation}
\lambda_{D} (x) = \frac{I^2_{D/2-1}(x)}{I^2_{D/2}(x)} - \frac{D}{x} \frac{I_{D/2-1}(x)}{I_{D/2}(x)} - 1. \label{eq:app:lambda}
\end{equation}&lt;/p&gt;
&lt;p&gt;For the second-order derivative, we need to slog through even more tedious algebra,&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial^2\alpha}
= \beta &amp;amp;\frac{\partial}{\partial\alpha}\biggl( \lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \biggr) \nonumber \\
&amp;amp;+ \frac{\partial}{\partial\alpha}\biggl( \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert} \biggr) ,
\end{align}&lt;/p&gt;
&lt;p&gt;which eventually leads to something like&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial^2\alpha}
= -2\beta^2 &amp;amp; \, \kappa_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right)^{2} \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \nonumber \\
&amp;amp;+ \beta \lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \frac{\partial\boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha} \cdot \Delta \mathbf{h} \right) \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \nonumber \\
&amp;amp;+ \beta \lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \frac{\partial\boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha} \nonumber \\
&amp;amp;- \frac{D}{\lVert \mathbf{h}(\alpha) \rVert^2} \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \Delta \mathbf{h} , \label{eq:generalgradalphasecondorder}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\kappa_{D} (x) = \lambda^2_{D} (x) + \left( 1 + \frac{D/2 + 1}{x} \frac{I_{D/2-1}(x)}{I_{D/2}(x)} \right) \lambda_{D} (x) + \frac{1}{x} \frac{I_{D/2-1}(x)}{I_{D/2}(x)}.
\end{align}&lt;/p&gt;
&lt;p&gt;Equation \eqref{eq:generalgradalphasecondorder} can be further simplified by substituting the first-order derivative Eq. \eqref{eq:generalgradalphafirstorder} and further simplifying the resulting expression. The derivation of the mean-field equations proceeds in a similar fashion as in the main text, but uses \eqref{eq:generalgradalphafirstorder} and  \eqref{eq:generalgradalphasecondorder} as expressions for the partial derivatives instead of their large-$D$ approximations.&lt;/p&gt;
&lt;p&gt;Another useful derivative is that of the single-site probability distribution \eqref{eq:pcondsinglesitevector},&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial}{\partial\alpha} \left( \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} } \right) =  \frac{\partial}{\partial\mathbf{h}(\alpha)} \left( \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} } \right) \cdot \Delta \mathbf{h},
\end{align}&lt;/p&gt;
&lt;p&gt;which evaluates to&lt;/p&gt;
&lt;p&gt;\begin{align}
\beta \left( \mathbf{s} - \boldsymbol{\varphi}\left(\mathbf{h}(\alpha)\right) \right) \cdot \Delta \mathbf{h} \frac{ \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} }{ \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} }
\end{align}&lt;/p&gt;
&lt;p&gt;and can be used to calculate derivatives of the conditional distribution \eqref{eq:pcondaltvector}.&lt;/p&gt;
&lt;h1 id=&#34;references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/h1&gt;
&lt;p&gt;A non-exhaustive list of references and inspiration includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;M. Aguilera, S.A. Moosavi, and H. Shimazaki, A unifying framework for mean-field theories of asymmetric kinetic Ising systems, &lt;em&gt;Nat Commun&lt;/em&gt; &lt;strong&gt;12&lt;/strong&gt;, 1197 (2021) &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2002.04309&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Y. Roudi and J. Hertz, Dynamical TAP equations for non-equilibrium Ising spin glasses, &lt;em&gt;J. Stat. Mech.&lt;/em&gt;, P03031 (2011) &lt;a href=&#34;https://arxiv.org/abs/1103.1044&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1103.1044&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;H.J. Kappen and J.J. Spanjers, Mean field theory for asymmetric neural networks, &lt;em&gt;Phys. Rev. E&lt;/em&gt; &lt;strong&gt;61&lt;/strong&gt;, 5658 (2000)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;G. Parisi, Asymmetric neural networks and the process of learning, &lt;em&gt;J. Phys. A: Math. Gen.&lt;/em&gt; &lt;strong&gt;19&lt;/strong&gt; L675 (1986)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;If you happen to find this work useful, please consider citing it as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{bal2023spinmodeltransformers,
  title   = {Spin-Model Transformers},
  author  = {Bal, Matthias},
  year    = {2023},
  month   = {?},
  url     = {https://mcbal.github.io/post/spin-model-transformers}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;We plot the absolute value to get rid of artificial &amp;ldquo;jumps&amp;rdquo; between the two branches. These occur because all models are simulated independently when sweeping across $\beta$ and the some combinations of initial state and model parameters might just happen to bounce to the other branch when $\beta$ changes in the $\beta &amp;gt; \beta_c$ regime.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Transformers Are Secretly Collectives of Spin Systems</title>
      <link>https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/</link>
      <pubDate>Tue, 23 Nov 2021 12:17:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;✨ Update (April 2023):&lt;/strong&gt; &lt;em&gt;Consider reading &lt;a href=&#34;https://mcbal.github.io/post/spin-model-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules&lt;/a&gt; where we continue building on the intuition of probing a spin system to engineer its collective response but get rid of the assumption of symmetric coupling matrices by shifting focus from equilibrium free energies to dynamical mean-field approximations of non-equilibrium vector-spin models. Come join the fun.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-where-does-the-transformer-module-architecture-come-from&#34;&gt;Where does the transformer module architecture come from?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-deriving-attention-from-energy-functions-only-gets-you-so-far&#34;&gt;Deriving attention from energy functions only gets you so far&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-back-to-the-roots-physical-spin-systems-and-vector-spin-models&#34;&gt;Back to the roots: physical spin systems and vector-spin models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-why-dont-we-just-probe-a-vector-spin-system-with-data&#34;&gt;Why don&amp;rsquo;t we just probe a vector-spin system with data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-a-slice-of-statistical-mechanics-magnetizations-and-free-energies&#34;&gt;A slice of statistical mechanics: magnetizations and free energies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#7-turning-a-differentiable-spin-system-into-a-neural-network&#34;&gt;Turning a differentiable spin system into a neural network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#8-an-exercise-in-squinting-recognizing-the-transformer-module&#34;&gt;An exercise in squinting: recognizing the transformer module&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#9-training-transformer-modules-shapes-collective-behavior&#34;&gt;Training transformer modules shapes collective behavior&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#10-training-deep-transformers-orchestrates-spin-system-collectives&#34;&gt;Training deep transformers orchestrates spin-system collectives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#11-conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;In this post, we try to distill a unifying perspective out of ideas developed in a series of longer posts on understanding transformers as physical systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers from Spin Models: Approximate Free Energy Minimization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We argue that a blueprint of the neural-network architecture of the archetypical transformer module can be derived from the structure of physical spin systems familiar from classical statistical mechanics. More specifically, we claim that the forward pass of transformer modules maps onto computing magnetizations in vector-spin models in response to incoming data. We imagine transformers as collectives of differentiable spin systems whose behavior can be shaped through training.&lt;/p&gt;
&lt;h3 id=&#34;2-where-does-the-transformer-module-architecture-come-from&#34;&gt;2. Where does the transformer module architecture come from?&lt;/h3&gt;
&lt;p&gt;Taking a bird&amp;rsquo;s eye view of the evergrowing zoo of transformer architectures in natural language processing and computer vision suggests that the design pattern introduced in &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention is All You Need (Vaswani et al., 2017)&lt;/a&gt;&lt;/em&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; is still dominant. Almost all architectural variations of transformer modules published in the last four years have stuck to a successful combination of residual connections, an attention-like operation (token-mixing), normalization layers, and a feed-forward-like operation (channel-mixing).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11418&#34; target=_blank&gt;&lt;img src=&#34;metaformer.png&#34; alt=&#34;MetaFormer architecture comparison&#34; width=&#34;400px&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Recent work like &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.11418&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MetaFormer is Actually What You Need for Vision (Yu et al., 2021)&lt;/a&gt;&lt;/em&gt;&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; appropriately shifts focus to the high-level architecture of the transformer module and argues that its full structure, rather than just the token-mixing attention operation, is essential for transformers to achieve competitive performance.&lt;/p&gt;
&lt;p&gt;So where does this archetypical design pattern come from? Why does it seem to stick around? Is there any physical intuition behind its structure?&lt;/p&gt;
&lt;h3 id=&#34;3-deriving-attention-from-energy-functions-only-gets-you-so-far&#34;&gt;3. Deriving attention from energy functions only gets you so far&lt;/h3&gt;
&lt;p&gt;Recent papers like &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need (Ramsauer et al., 2020)&lt;/a&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2008.06996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large Associative Memory Problem in Neurobiology and Machine Learning (Krotov and Hopfield, 2020)&lt;/a&gt;&lt;/em&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; have looked for physical intuition behind attention mechanisms using an &lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;energy-based perspective&lt;/a&gt; phrased in terms of modern continuous Hopfield networks. The main idea is to derive the softmax-attention update rule&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{Q}&#39; = \text{softmax}\left( \frac{\boldsymbol{Q} \boldsymbol{K}^T}{\sqrt{d}} \right) \boldsymbol{K}
\end{equation}&lt;/p&gt;
&lt;p&gt;by taking a large gradient descent update step using the derivative with respect to input queries $\boldsymbol{Q}$ of some judiciously chosen energy function&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = \frac{1}{2} \boldsymbol{Q} \boldsymbol{Q}^T -\mathrm{logsumexp} \left( \frac{\boldsymbol{Q} \boldsymbol{K}^T}{\sqrt{d}} \right). \label{eq:logsumexp}
\end{equation}&lt;/p&gt;
&lt;p&gt;In this way, vanilla softmax attention can be recast as taking a &lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/#modern-continuous-hopfield-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;large gradient step&lt;/a&gt;. The energy landscape defined by Eq. \eqref{eq:logsumexp} implements an associative memory system for storing and retrieving vector patterns where queries flow towards valleys associated with their nearest keys (see &lt;a href=&#34;https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention as Energy Minimization: Visualizing Energy Landscapes&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/&#34; target=_blank&gt;&lt;img src=&#34;landscape.png&#34; alt=&#34;Logsumexp energy function landscape&#34; width=&#34;300px&#34;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But there is more to transformer modules than just attention. In practice, we know that residual connections, normalization layers, and feed-forward layers are all essential to achieve good empirical performance.&lt;/p&gt;
&lt;p&gt;Can we generalize this physical intuition of taking derivatives with respect to an energy function to recover the full transformer module? Yes, we can. But we have to take a step back from energy functions and focus on their underlying physical systems instead.&lt;/p&gt;
&lt;h3 id=&#34;4-back-to-the-roots-physical-spin-systems-and-vector-spin-models&#34;&gt;4. Back to the roots: physical spin systems and vector-spin models&lt;/h3&gt;
&lt;p&gt;Energy functions in classical statistical mechanics are succinct descriptions encoding interactions and constraints in physical systems. Spin systems are prototypical physical systems which often serve as toy models for all kinds of phenomena&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Ising_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ising model&lt;/a&gt; is a simple toy model describing a classical binary spin system with local spin degrees of freedom at every site pointing either up or down. The energy function of the binary random Ising model for $N$ spins in the presence of a site-dependent external magnetic field is given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = - \sum_{i,j=1}^{N} J_{ij} \sigma_{i} \sigma_{j} - \sum_{i=1}^{N} h_{i} \sigma_{i}, \label{eq:binaryrandomising}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the $J_{ij}$ encode coupling strengths between all pairs of spins and the external magnetic fields $h_{i}$ act as biases by providing a preferential value of alignment at every site. The model defined by \eqref{eq:binaryrandomising} is also known as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boltzmann machine&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Hopfield_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield network&lt;/a&gt;. A cartoon of this model looks like a graph of little arrows that are pairwise coupled&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;img src=&#34;binary_ising.png&#34; alt=&#34;Random Ising model configuration with binary spins&#34; width=&#34;200px&#34;/&gt;
&lt;p&gt;At thermal equilibrium, the Boltzmann probability distribution $e^{-\beta E\left( \sigma \right)} / Z$ reflects what patterns of up-down spins, or &lt;em&gt;spin configurations&lt;/em&gt;, are preferred. The partition function $Z = \sum_{\sigma} e^{-\beta E\left( \sigma \right)}$ of a spin system is not only a normalization constant but also a magical object relating the microscopic world of fluctuating spins to thermodynamic, observable quantities via the free energy $F = - \beta^{-1} \log Z$. Even for simple spin systems, computing partition functions by summing over all possible configurations is a shockingly hard thing to do in most scenarios.&lt;/p&gt;
&lt;p&gt;Binary spin models are nice but rarely excite machine learning practitioners anymore nowadays. Modern neural networks like transformers act on sequences of vectors like token embeddings or image patches. Instead of abandoning spin models altogether, we could consider &lt;em&gt;vector-spin models&lt;/em&gt;. Replacing binary degrees of freedom with $d$-dimensional vector degrees of freedom, we can define a spin-model energy function&lt;/p&gt;
&lt;p&gt;\begin{align}
E = - \sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} - \sum_{i=1}^{N} \boldsymbol{h}_{i} \cdot \boldsymbol{\sigma}_{i}, \label{eq:vectorrandomising}
\end{align}&lt;/p&gt;
&lt;p&gt;where the scalar products have turned into dot products. Models of this form first popped up in 1960s statistical mechanics literature as &lt;a href=&#34;https://en.wikipedia.org/wiki/N-vector_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;classical $d$-vector models&lt;/a&gt;. They also appear in recent studies on higher-dimensional generalizations of spin glass models&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;img src=&#34;vector_ising.png&#34; alt=&#34;Random Ising model configuration with vector spins&#34; width=&#34;200px&#34;/&gt;
&lt;p&gt;Now how can we relate vector-spin systems like Eq. \eqref{eq:vectorrandomising} to modern neural networks?&lt;/p&gt;
&lt;h3 id=&#34;5-why-dont-we-just-probe-a-vector-spin-system-with-data&#34;&gt;5. Why don’t we just probe a vector-spin system with data?&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s pursue an intuitive idea. Imagine we want to expose our vector-spin system Eq. \eqref{eq:vectorrandomising} to a sequence of vector data. We can do this by having the sequence act as the spin system&amp;rsquo;s external magnetic field $(\boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N})$. We would then like to observe how the spin system responds to this particular environment of patterns.&lt;/p&gt;
&lt;p&gt;If all of the steps in the computation of the spin system&amp;rsquo;s responses can be implemented in a differentiable way, we should be able to engineer its collective behavior by optimizing the coupling parameters to better respond to future incoming data. We propose to observe spin-system responses in terms of &lt;em&gt;magnetizations computed from free energies&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;6-a-slice-of-statistical-mechanics-magnetizations-and-free-energies&#34;&gt;6. A slice of statistical mechanics: magnetizations and free energies&lt;/h3&gt;
&lt;p&gt;For ease of notation, let&amp;rsquo;s call the model parameters $\theta \equiv \{ J_{ij} \}$, the spins $\sigma \equiv \{ \boldsymbol{\sigma}_{i} \}$, and the external magnetic fields $h \equiv (\boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N})$. We can then schematically write our spin system&amp;rsquo;s partition function as&lt;/p&gt;
&lt;p&gt;\begin{align}
Z_{\theta} \left( h \right) = \int \mathrm{d} \sigma \ \mathrm{e}^{ - \beta E_{\theta}\left( \sigma, h \right) } \label{eq:partfun}
\end{align}&lt;/p&gt;
&lt;p&gt;and the corresponding free energy as $F_{\theta} \left( h \right) = - \beta^{-1} \log Z_{\theta} \left( h \right)$.&lt;/p&gt;
&lt;p&gt;Magnetizations are responses of our spin system to the external magnetic field imposed by $(\boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N})$. From standard thermodynamics, we know that we can calculate magnetizations from the free energy by differentiating with respect to the external field&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{m}_{i} = - \frac{\mathrm{d} F_{\theta} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right)}{\mathrm{d} \boldsymbol{h}_{i}} = \langle \boldsymbol{\sigma}_{i} \rangle , \label{eq:sigma}
\end{align}&lt;/p&gt;
&lt;p&gt;which, in this case, boils down to calculating spin expectation values. The magnetization for every site depends on the couplings and, through the couplings between spins, on the values of the external field at all sites. Magnetizations reveal how spins will collectively tend to align themselves when we place the spin system in an environment of patterns.&lt;/p&gt;
&lt;p&gt;Before we move on, we have to account for one more complication. If we want to draw a correspondence between transformer modules and vector-spin systems, we will have to allow for couplings that depend on the external magnetic field. For example, the attention matrix in vanilla transformers looks something like&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_{ij} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right) = \left[\mathrm{softmax}\left( \frac{\boldsymbol{H} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{H}^{T}}{\sqrt{d}} \right)\right]_{ij}, \label{eq:softmaxcouplings}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the matrix $\boldsymbol{H}$ denotes the stack of external magnetic field vectors. The interactions between spins are determined dynamically based on the inputs. From a physics perspective, these &amp;ldquo;amortized&amp;rdquo; couplings are very weird and highly unusual, but such is the transformer.&lt;/p&gt;
&lt;p&gt;The potential dependency of the couplings on the external field changes the magnetization of Eq. \eqref{eq:sigma} to an expression of the form&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{m}_{i} &amp;amp;= - \frac{\mathrm{d} F_{\theta} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right)}{\mathrm{d} \boldsymbol{h}_{i}} \nonumber \\ &amp;amp;= \langle \boldsymbol{\sigma}_{i} \rangle + \sum_{m,n} \langle \boldsymbol{\sigma}_{m} \cdot \boldsymbol{\sigma}_{n} \rangle \frac{\partial J_{mn} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right) }{ \partial \boldsymbol{h}_{i} } , \label{eq:sigmaweird}
\end{align}&lt;/p&gt;
&lt;p&gt;where two-point correlation functions are seen to act as weights for the coupling contributions&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;. In practice, we should of course let an automatic differentiation framework keep track of dependencies so that we can get away with simply computing&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{m}_{i} = - \frac{\mathrm{d} F_{\theta} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right)}{\mathrm{d} \boldsymbol{h}_{i}}, \label{eq:magnetization}
\end{align}&lt;/p&gt;
&lt;p&gt;assuming we have a differentiable expression for the (approximate) free energy available.&lt;/p&gt;
&lt;h3 id=&#34;7-turning-a-differentiable-spin-system-into-a-neural-network&#34;&gt;7. Turning a differentiable spin system into a neural network&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s now use the ingredients introduced above to construct a neural network module which wraps around a vector-spin system. Given the energy function Eq. \eqref{eq:vectorrandomising} and the free energy $F_{\theta} \left( h \right) = - \beta^{-1} \log \int \mathrm{d} \sigma \ \mathrm{e}^{ - \beta E_{\theta}\left( \sigma, h \right) }$, we let incoming data play the role of the external magnetic field and return magnetizations in response.&lt;/p&gt;
&lt;img src=&#34;spinmodule_new.png&#34; alt=&#34;Spin system as a neural network&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;Nice. But didn&amp;rsquo;t we mention before that partition functions (and hence free energies and thus magnetizations) are shockingly hard to compute? Why introduce all these formal expressions if we cannot compute anything?&lt;/p&gt;
&lt;p&gt;Looking back at statistical mechanics papers from the 1950s-1970s, it turns out that physicists have already developed several tricks and approximation methods that can be applied to deal with vector-spin systems. Computational evidence that the partition function approach outlined above &lt;em&gt;is&lt;/em&gt; possible for vector-spin systems can be found in &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention&lt;/a&gt; (below, left) and &lt;a href=&#34;https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Approximate Free Energy Minimization&lt;/a&gt; (below, right).&lt;/p&gt;
&lt;img src=&#34;arch_dia_afem_new.png&#34; alt=&#34;Deep implicit attention and approximate free-energy minimization&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;In these examples, approximations of the partition function Eq. \eqref{eq:partfun} were obtained following respectively a mean-field theory and a steepest-descent approach. Our &lt;a href=&#34;https://github.com/mcbal&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;numerical implementations&lt;/a&gt; of both approaches rely internally on &lt;a href=&#34;http://implicit-layers-tutorial.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deep implicit layers&lt;/a&gt; to ensure that fixed-point calculations and root-solving steps are efficiently differentiable.&lt;/p&gt;
&lt;h3 id=&#34;8-an-exercise-in-squinting-recognizing-the-transformer-module&#34;&gt;8. An exercise in squinting: recognizing the transformer module&lt;/h3&gt;
&lt;p&gt;Computing magnetizations according to Eq. \eqref{eq:magnetization} from the (approximate) free energies obtained in &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention&lt;/a&gt;  and &lt;a href=&#34;https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Approximate Free Energy Minimization&lt;/a&gt; reveals a high-level structure that is surprisingly familiar: a pattern of residual connections, token-mixing, normalization, and channel-mixing. Approaching the crux from the other direction, we argue that transformer modules react to inputs by implementing particular approximations to the general magnetization response Eq. \eqref{eq:sigmaweird}.&lt;/p&gt;
&lt;p&gt;Residual connections are proportional to the inputs and arise from the presence of the external magnetic field. Token-mixing contributions emerge from the coupling terms in the energy function and mix inputs without acting on the local vector-spin dimension. Normalization follows from requiring that the energy of the spin system remain linearly proportional to the number of lattice sites and from normalizing the external magnetic field vectors. Channel-mixing contributions include terms in the magnetization that can be applied locally, like Onsager self-correction terms in mean-field approaches or (approximations to) contributions coming from input-dependent couplings in Eq. \eqref{eq:sigmaweird}.&lt;/p&gt;
&lt;p&gt;Taken together, these observations suggest that we can picture the forward pass of a transformer module as a wrapper around a vector-spin system: module inputs are routed to the external magnetic field (and, optionally, to a parametrized couplings function) after which magnetizations are returned as outputs. The transformer module bears an uncanny resemblance to a differentiable physical system whose collective behavior we can control through training.&lt;/p&gt;
&lt;h3 id=&#34;9-training-transformer-modules-shapes-collective-behavior&#34;&gt;9. Training transformer modules shapes collective behavior&lt;/h3&gt;
&lt;p&gt;Now that we can picture transformer modules as physical spin systems responding to getting probed with data, let&amp;rsquo;s imagine what training them looks like.&lt;/p&gt;
&lt;p&gt;On the level of the energy function of our spin system Eq. \eqref{eq:vectorrandomising}, we can model the training process of a transformer module by introducing a (discrete) time dimension and making the external magnetic field time-dependent, leading to&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(t) = - \sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} - \sum_{i=1}^{N} \boldsymbol{h}_{i}(t) \cdot \boldsymbol{\sigma}_{i} \label{eq:sloppyenergy}
\end{equation}&lt;/p&gt;
&lt;p&gt;At every training step $t$, a sequence of incoming data $\{ \boldsymbol{h}_{1}(t), \boldsymbol{h}_{2}(t), \ldots, \boldsymbol{h}_{N}(t) \}$ takes on the role of external magnetic field. During the forward pass, magnetizations $\boldsymbol{m}_{i}$ are computed in a differentiable way according to the current model parameters and in the presence of the current external magnetic field. Physically, we consider &amp;ldquo;quenched&amp;rdquo; systems with &amp;ldquo;frozen&amp;rdquo; couplings at every training step. During the backward pass, the module&amp;rsquo;s coupling parameters $J_{ij}$ get updated, nudging the interactions in the spin system so as to influence its magnetization responses to similar data in future iterations.&lt;/p&gt;
&lt;img src=&#34;spinmoduletraining_new.png&#34; alt=&#34;Training a spin system as a neural network&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;We can think about this training process as gradually shaping the collective behavior of a differentiable vector-spin system that is driven by data. If the couplings depend on the inputs, like in Eq. \eqref{eq:softmaxcouplings}, we should make the couplings time-dependent as well in Eq. \eqref{eq:sloppyenergy}. In that case, the external magnetic fields as well as the parametrized couplings change instantaneously at every training step.&lt;/p&gt;
&lt;h3 id=&#34;10-training-deep-transformers-orchestrates-spin-system-collectives&#34;&gt;10. Training deep transformers orchestrates spin-system collectives&lt;/h3&gt;
&lt;p&gt;Training a deep transformer model corresponds to orchestrating a stack of transformer modules by building up a differentiable structure of correlations where the magnetizations of one spin system drive the next one. Wiggling (billions of) parameters during training nudges the cascading response behavior of the collective of spin systems to better adapt to the collective&amp;rsquo;s (meta-)tasks as specified by the data and the loss function.&lt;/p&gt;
&lt;img src=&#34;transformertraining_new.png&#34; alt=&#34;Training a transformer&#34; width=&#34;500px&#34;/&gt;
&lt;h3 id=&#34;11-conclusion&#34;&gt;11. Conclusion&lt;/h3&gt;
&lt;p&gt;In this post, we argued that the forward pass of a transformer module maps onto computing magnetizations in a vector-spin model responding to data. Generalizing previous work on understanding softmax attention modules in terms of modern continuous Hopfield networks by taking derivatives of a judiciously chosen &lt;em&gt;energy&lt;/em&gt; function, we propose to take derivatives of the &lt;em&gt;free energy&lt;/em&gt; of a general vector-spin system to get to a blueprint of the architecture of a full transformer module.&lt;/p&gt;
&lt;p&gt;By zooming out and approaching transformers from a tangential, statistical-mechanical point of view, we arrived at a physical intuition of transformers that seems hard to obtain when restricting oneself to perpetually perturbing explicit neural network architectures. Recognizing transformer modules as spin models in disguise might not only unify architectural variations as different ways to approximately compute magnetizations but also elucidate the empirical success of transformers in deep learning.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We would like to thank &lt;a href=&#34;https://mlcollective.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML Collective&lt;/a&gt; for hosting its research jams and providing a friendly environment to present ideas.&lt;/p&gt;
&lt;h2 id=&#34;references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/h2&gt;
&lt;p&gt;If you happen to find this work useful, please consider citing it as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{bal2021isingisallyouneed,
  title   = {Transformers Are Secretly Collectives of Spin Systems},
  author  = {Bal, Matthias},
  year    = {2021},
  month   = {November},
  url     = {https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt; (2017)&lt;/em&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan, &lt;a href=&#34;https://arxiv.org/abs/2111.11418&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MetaFormer is Actually What You Need for Vision&lt;/a&gt; (2021)&lt;/em&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, &lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need&lt;/a&gt; (2020)&lt;/em&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Dmitry Krotov and John Hopfield, &lt;a href=&#34;https://arxiv.org/abs/2008.06996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large Associative Memory Problem in Neurobiology and Machine Learning&lt;/a&gt; (2020)&lt;/em&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Consider reading the Physics Today article on &lt;a href=&#34;https://www.physics.rutgers.edu/~pchandra/physics681/Sompolinsky_PhysicsToday.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Statistical Mechanics of Neural Networks (Sompolinsky, 1988)&lt;/a&gt; for an introduction to disordered systems, spin glasses, Ising spin systems, emergent collective computational abilities, associative memories, Hopfield models, and the idea of learning patterns as shaping the behavior of systems. Essentially, what we&amp;rsquo;re trying to do in this post is figuring out a way to relate modern transformer models back to these old ideas.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;We plot spin sites at random positions to emphasize that there is no spatial notion of &amp;ldquo;closeness&amp;rdquo; in a fully-connected system: every site is just a hop away. To not overload the graph, we only draw connections strongest in absolute value.&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For example, see &lt;em&gt;&lt;a href=&#34;http://blog.math.toronto.edu/GraduateBlog/files/2020/07/ut-thesis-Ko-updated.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Free Energy of Spherical Vector Spin Glasses (Ko, 2018)&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.04441&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Free Energy in the Mixed p-spin Models With Vector Spins (Panchenko, 2015)&lt;/a&gt;&lt;/em&gt;.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For example, see the content of Chapter 2 in the &lt;a href=&#34;https://giamarchi.unige.ch/local/people/thierry.giamarchi/pdf/cours_sft.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;lecture notes on statistical field theory&lt;/a&gt; by Thierry Giamarchi.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;In the absence of an explicit expression for the free energy, one of the feed-forward network&amp;rsquo;s roles might be to try to approximate the complicated dependencies in the magnetization expression Eq. \eqref{eq:sigmaweird}, at the cost of introducing a large amount of additional free parameters beyond just the coupling parameters. It would be interesting to look into this numerically at scale using the free energy expression obtained in &lt;a href=&#34;https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Approximate Free Energy Minimization&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The time-dependence in Eq. \eqref{eq:sloppyenergy} smells of non-equilibrium statistical mechanics. Incoming data might be considered as time-dependent &amp;ldquo;probes&amp;rdquo; which inject energy (and useful information if its content is low-entropy enough) into a non-equilibrium system. By nudging its dynamical response behavior across spatiotemporal scales, the system could potentially learn how to deal with being driven by all kinds of patterns in incoming data. For an interesting toy example of such behavior, see &lt;a href=&#34;https://youtu.be/vSgHuErXuqk?t=2188&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this talk&lt;/a&gt; by Jeremy England on &lt;em&gt;Low rattling: a principle for understanding driven many-body self-organization&lt;/em&gt;.&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Transformers from Spin Models: Approximate Free Energy Minimization</title>
      <link>https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/</link>
      <pubDate>Tue, 12 Oct 2021 18:40:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;✨ Update (November 2021):&lt;/strong&gt; &lt;em&gt;Consider reading &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers Are Secretly Collectives of Spin Systems&lt;/a&gt; for a high-level overview of some of the ideas outlined in this post.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-massaging-partition-functions&#34;&gt;Massaging partition functions&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#21-a-vector-spin-model-and-its-partition-function&#34;&gt;A vector-spin model and its partition function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#22-peeking-into-a-physicists-bag-of-tricks&#34;&gt;Peeking into a physicist’s bag of tricks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#23-steepest-descent-hunting-for-the-saddle&#34;&gt;Steepest descent: hunting for the saddle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#24-taking-stock-of-what-we-have-done&#34;&gt;Taking stock of what we have done&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#241-questioning-steepest-descent-and-the-large-d-limit&#34;&gt;Questioning steepest descent and the large-$D$ limit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#242-energy-based-models-and-effective-energy-functions&#34;&gt;Energy-based models and effective energy functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#243-spin-glasses-and-mean-field-approximation&#34;&gt;Spin glasses and mean-field approximation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-implementing-approximate-free-energy-minimization&#34;&gt;Implementing approximate free-energy minimization&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#31-the-algorithm-bold-moves-on-a-tricky-landscape&#34;&gt;The algorithm: bold moves on a tricky landscape&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#311-initialization-and-normalization&#34;&gt;Initialization and normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#312-implicit-layers-for-steepest-descent-root-finding&#34;&gt;Implicit layers for steepest-descent root-finding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#313-fun-with-free-energies&#34;&gt;Fun with free energies&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#32-the-attention-module-probing-spins-with-data&#34;&gt;The attention module: probing spins with data&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#321-spin-expectation-values&#34;&gt;Spin expectation values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#322-wrapping-around-the-spin-model&#34;&gt;Wrapping around the spin model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#323-comparison-with-vanilla-transformers&#34;&gt;Comparison with vanilla transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Code: A PyTorch implementation of the ideas outlined in this blog post is available in the GitHub repository &lt;a href=&#34;https://github.com/mcbal/afem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mcbal/afem&lt;/code&gt;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms&lt;/a&gt;, we introduced a mean-field theory perspective on transformer modules. We showed how their outputs can be understood as mean-field spin expectation values of simple Ising-like vector-spin systems. Physically, the process of training a transformer module can be understood as driving a classical many-body system with data and iteratively shaping its collective response behaviour through coupling-weight parameter updates. Stacking transformer modules corresponds to building up a differentiable structure of correlations by using the spin expectation values of one physical system to drive the next one.&lt;/p&gt;
&lt;p&gt;In this post, we flesh out the idea of looking at transformer modules as physical systems. Having identified vector spin systems as plausible physical models underlying transformers, we turn to 1960s statistical-mechanics literature to look for inspiration on how to deal with their partition functions&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. We rediscover that the partition function of a particular class of vector-spin models can be approximated in the limit of large local spin dimension using steepest descent, leading to approximate yet tractable expressions for the free energy and other derived quantities.&lt;/p&gt;
&lt;p&gt;Combining these canonical results from statistical mechanics with modern differentiable programming, we implement a differentiable vector-spin model based on an approximate free-energy minimization algorithm. Internally, the model uses an implicit layer to solve for the stationary point of the partition function in a differentiable way. We then construct a transformer-like attention module which encapsulates the spin model by routing inputs to applied magnetic fields and spin expectation values to outputs. The latter are obtained by following the familiar recipe of statistical mechanics: differentiating the spin model&amp;rsquo;s $\log Z$ with respect to conjugate input variables. Finally, we contextualize our approach by comparing it to vanilla transformers, deep equilibrium transformers, and deep implicit attention.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;✨ TL;DR:&lt;/strong&gt; &lt;em&gt;We consider transformer modules as wrappers around a differentiable steepest-descent approximation of simple Ising-like vector-spin models familiar from statistical mechanics. We observe that a blueprint of the successful transformer-like architectural pattern of token-mixing (attention) and channel-mixing (feed-forward) naturally emerges when computing spin expectation values in vector-spin models with input-dependent couplings. Feel free to skip to the &lt;a href=&#34;#323-comparison-with-vanilla-transformers&#34;&gt;final section&lt;/a&gt; for a visual comparison of this work to vanilla transformers, deep equilibrium transformers, and deep implicit attention.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;2-massaging-partition-functions&#34;&gt;2. Massaging partition functions&lt;/h1&gt;
&lt;p&gt;In this section, we set out to derive an approximate, analytical expression for the free energy of a classical disordered vector-spin system exposed to a site-dependent external magnetic field. In deriving the results below, we found inspiration in H. E. Stanley&amp;rsquo;s &lt;a href=&#34;https://doi.org/10.1103/PhysRev.176.718&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spherical Model as the Limit of Infinite Spin Dimensionality (1968)&lt;/a&gt; and Chapter 5 of R. J. Baxter&amp;rsquo;s bible on &lt;a href=&#34;https://physics.anu.edu.au/theophys/baxter_book.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Exactly Solved Models in Statistical Mechanics (1982)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;21-a-vector-spin-model-and-its-partition-function&#34;&gt;2.1. A vector-spin model and its partition function&lt;/h2&gt;
&lt;p&gt;We start from the following Hamiltonian (or energy function) of a classical vector spin system of $N$ spins in a site-dependent external magnetic field,&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = - \sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} - \sum_{i=1}^{N} \boldsymbol{h}_{i} \cdot \boldsymbol{\sigma}_{i}, \label{eq:vectrandomising}
\end{equation}&lt;/p&gt;
&lt;p&gt;where both $\boldsymbol{\sigma}_{i} = \left[ \sigma_{1}(i), \sigma_{2}(i), \ldots, \sigma_{D}(i) \right]$ and $\boldsymbol{h}_{i} = \left[ h_{1}(i), h_{2}(i), \ldots, h_{D}(i) \right]$ are vectors of dimension $D$. The coupling matrix $\boldsymbol{J}$ is assumed to be traceless and symmetric but can otherwise have real elements with both negative and positive signs. We take the vector degrees of freedom $\boldsymbol{\sigma}_{i}$ to be constrained by a set of $N$ constraints&lt;/p&gt;
&lt;p&gt;\begin{equation}
\lVert \boldsymbol{\sigma}_{i} \rVert _{2}^{2} = \sum_{a=1}^{D} \sigma_{a}^{2}(i) = D, \quad i = 1,2,\ldots,N,
\end{equation}&lt;/p&gt;
&lt;p&gt;so that their magnitudes equal $\sqrt{D}$. One can picture the classical spin degrees of freedom as arrows rotating along the surface of $(D-1)$-dimensional spheres at every site.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;spin_system.png&#34; alt=&#34;alt text&#34; title=&#34;Cartoon of vector-spin system&#34;&gt;&lt;/p&gt;
&lt;p&gt;In statistical mechanics, the model Eq. \eqref{eq:vectrandomising} is known as a &lt;a href=&#34;https://en.wikipedia.org/wiki/N-vector_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;vector model&lt;/a&gt; whose familiar small-$D$ cases include the &lt;a href=&#34;https://en.wikipedia.org/wiki/Ising_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ising model&lt;/a&gt; ($D=1$), the &lt;a href=&#34;https://en.wikipedia.org/wiki/Classical_XY_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;XY model&lt;/a&gt; ($D=2$), and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Classical_Heisenberg_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Heisenberg model&lt;/a&gt; ($D=3$). For infinite-dimensional spins $D \to \infty$, one can show that the system approaches the &lt;a href=&#34;https://en.wikipedia.org/wiki/Spherical_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spherical model&lt;/a&gt;. The model defined by \eqref{eq:vectrandomising} can also be regarded as a vector generalization of &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boltzmann machines&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Hopfield_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield networks&lt;/a&gt; or disordered &lt;a href=&#34;https://en.wikipedia.org/wiki/Spin_glass#The_model_of_Sherrington_and_Kirkpatrick&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sherrington-Kirkpatrick spin-glass models&lt;/a&gt; (but with just a single sample of non-local couplings instead of an underlying probability distribution). Similar models also appear in recent studies on higher-dimensional generalizations of spin glass models&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The partition function for our spin system looks like:&lt;/p&gt;
&lt;p&gt;\begin{align}
Z_{N}^{(D)} &amp;amp;\left( \beta, J_{ij}, \{ \boldsymbol{h}_{i} \} \right) \nonumber \\
&amp;amp;= \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \ \mathrm{d}\sigma_{1}(1) \cdots \mathrm{d}\sigma_{D}(N) \nonumber \\
&amp;amp; \qquad \times \ \prod_{j=1}^{N} \delta \left( D - \lVert \boldsymbol{\sigma}_{j} \rVert _{2}^{2} \right) \nonumber \\
&amp;amp; \qquad \times \exp \left[ \beta \sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} + \beta \sum_{i=1}^{N} \boldsymbol{h}_{i} \cdot \boldsymbol{\sigma}_{i} \right] \label{eq:fullpartfun}
\end{align}&lt;/p&gt;
&lt;p&gt;where we have made all dependencies explicit. This looks absolutely mental. We somehow need to find a way to do $N \times D$ integrals while taking into account all the constraints and interactions.&lt;/p&gt;
&lt;h2 id=&#34;22-peeking-into-a-physicists-bag-of-tricks&#34;&gt;2.2. Peeking into a physicist&amp;rsquo;s bag of tricks&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s first of all get rid of the explicit Dirac delta functions by substituting their complex integral representations&lt;/p&gt;
&lt;p&gt;\begin{align}
\delta \left( D - \lVert \boldsymbol{\sigma}_{j} \rVert _{2}^{2} \right) = \frac{\beta}{2 \pi i} \int_{-i\infty}^{i\infty} \mathrm{d} t_{j} \exp \left[ \beta t_{j} \left( D -  \lVert \boldsymbol{\sigma}_{j} \rVert _{2}^{2} \right) \right]
\end{align}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{align}
Z_{N}^{(D)} &amp;amp;= \left(\frac{\beta}{2 \pi i}\right)^{N} \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \ \mathrm{d}\sigma_{1}(1) \cdots \mathrm{d}\sigma_{D}(N) \nonumber \\
&amp;amp; \times \int_{-i\infty}^{i\infty} \cdots \int_{-i\infty}^{i\infty} \ \mathrm{d}t_{1} \cdots \mathrm{d}t_{N} \ \exp \left( \beta D \sum_{j=1}^{N} t_{j} \right)\nonumber \\
&amp;amp; \times \prod_{\alpha=1}^{D} \exp \left[ -\beta \sum_{i,j=1}^{N} \left(t_{j}\delta_{ij}-J_{ij}\right) \; \sigma_{\alpha}(i) \sigma_{\alpha}(j) + \beta \sum_{i=1}^{N} h_{\alpha}(i) \sigma_{\alpha}(i) \right] \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;Great, even more integrals. The next frustrating trick involves writing the number 1 as a judiciously chosen exponential,&lt;/p&gt;
&lt;p&gt;\begin{align}
\exp \left( \beta \sum_{j=1}^{N} a \left( D - \lVert \boldsymbol{\sigma}_{j} \rVert _{2}^{2} \right) \right) = 1,
\end{align}&lt;/p&gt;
&lt;p&gt;for some arbitrary constant $a$, which, inside the integral, indeed evaluates to $\exp (0) = 1$ because of the constraints. Inserting this expression gives&lt;/p&gt;
&lt;p&gt;\begin{align}
&amp;amp;Z_{N}^{(D)} = \left(\frac{\beta}{2 \pi i}\right)^{N} \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \ \mathrm{d}\sigma_{1}(1) \cdots \mathrm{d}\sigma_{D}(N) \nonumber \\
&amp;amp; \times \int_{-i\infty}^{i\infty} \cdots \int_{-i\infty}^{i\infty} \ \mathrm{d}t_{1} \cdots \mathrm{d}t_{N} \ \exp \left( \beta D \sum_{j=1}^{N} \left( t_{j} + a\right) \right)\nonumber \\
&amp;amp; \times \prod_{\alpha=1}^{D} \exp \left[ -\beta \sum_{i,j=1}^{N} \left( \left( t_{j} + a \right) \delta_{ij}-J_{ij}\right) \; \sigma_{\alpha}(i) \sigma_{\alpha}(j) + \beta \sum_{i=1}^{N} h_{\alpha}(i) \sigma_{\alpha}(i) \right] \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;Next, we&amp;rsquo;d like to swap the order of the $\mathrm{d}\sigma_{a}(j)$ and $\mathrm{d}t_{j}$ integrations to start integrating. But we are only allowed to do this if we assume $a$ to be a sufficiently large positive real number. Why? Essentially, we are deforming the contours of the complex integrals sufficiently far to the right such that the real part the quadratic form appearing in the exponential is positive definite, see e.g. &lt;a href=&#34;https://doi.org/10.1103/PhysRev.160.437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helfand &amp;amp; Langer (1967)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s go ahead and assume that everything is fine. We swap integrals and do a change of variables $t_j \to t_j + a$ so that&lt;/p&gt;
&lt;p&gt;\begin{align}
Z_{N}^{(D)} &amp;amp;= \left(\frac{\beta}{2 \pi i}\right)^{N} \int_{a-i\infty}^{a+i\infty} \cdots \int_{a-i\infty}^{a+
i\infty} \ \mathrm{d}t_{1} \cdots \mathrm{d}t_{N} \\
&amp;amp; \times \exp \left( \beta D \sum_{j=1}^{N} t_{j} \right)\nonumber \prod_{\alpha=1}^{D} I_{\alpha} \left( \beta, \{ t_{j} \}, \{ h_{\alpha}(i) \} \right)\nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
I_{\alpha} &amp;amp;\left( \beta, \{ t_{j} \}, \{ h_{\alpha}(i) \} \right) = \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} \ \mathrm{d}\sigma_{\alpha}(1) \cdots \mathrm{d}\sigma_{\alpha}(N) \nonumber \\
&amp;amp; \times \exp \left[ -\beta \sum_{i,j=1}^{N} \left( t_{j} \delta_{ij}-J_{ij}\right) \; \sigma_{\alpha}(i) \sigma_{\alpha}(j) + \beta \sum_{i=1}^{N} h_{\alpha}(i) \sigma_{\alpha}(i) \right]\nonumber \\
\end{align}&lt;/p&gt;
&lt;p&gt;Notice how the integrals have kind of factorized over the vector dimension: for every $\alpha$-component we can evaluate an $N$-dimensional Gaussian integral with a linear term. The $I_{\alpha}$ functions depend on the sources $\{ \boldsymbol{h}_{i} \}$ indexed along local dimension instead of spin. Introducing the symmetric $N \times N$ matrix $V_{ij} = t_{j} \delta_{ij}-J_{ij}$, we can evaluate the Gaussian integrals and find&lt;/p&gt;
&lt;p&gt;\begin{align}
I_{\alpha} &amp;amp;\left( \beta, \{ t_{j} \}, \{ h_{\alpha}(i) \} \right) = \left( \frac{\pi}{\beta} \right)^{N/2} \left[ \det \left( \boldsymbol{V} \right) \right]^{-1/2} \exp \left(\frac{\beta}{4} \boldsymbol{h}_{\alpha}^{T} \boldsymbol{V}^{-1} \boldsymbol{h}_{\alpha} \right) \nonumber \\
\end{align}&lt;/p&gt;
&lt;p&gt;where $\boldsymbol{h}_{\alpha} = \left[ h_{\alpha}(1), h_{\alpha}(2), \ldots, h_{\alpha}(N) \right]$ denote $N$-dimensional vectors. The expression for the partition function becomes&lt;/p&gt;
&lt;p&gt;\begin{align}
&amp;amp;Z_{N}^{(D)} = \left(\frac{\beta}{2 \pi i}\right)^{N} \left( \frac{\pi}{\beta} \right)^{DN/2} \int_{a-i\infty}^{a+i\infty} \cdots \int_{a-i\infty}^{a
+i\infty} \ \mathrm{d}t_{1} \cdots \mathrm{d}t_{N} \nonumber \\
&amp;amp; \times \exp \left( D \left( \beta \sum_{j=1}^{N} t_{j} - \frac{1}{2} \log \det \left( \boldsymbol{V} \right) \right) \right) \exp \left( \frac{\beta}{4} \mathrm{Tr} \left( \boldsymbol{H}^{T} \boldsymbol{V}^{-1} \boldsymbol{H} \right) \right) \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;where we have introduced the matrix notation $\boldsymbol{H} \in \mathbb{R}^{N \times D}$ to group the vectors $\{ \boldsymbol{h}_{i} \}$.&lt;/p&gt;
&lt;h2 id=&#34;23-steepest-descent-hunting-for-the-saddle&#34;&gt;2.3. Steepest descent: hunting for the saddle&lt;/h2&gt;
&lt;p&gt;But there&amp;rsquo;s still $N$ complex integrals over the auxiliary variables $\{ t_{j} \}$ left to do. Can we avoid doing them? Maybe. Let&amp;rsquo;s rewrite our partition function as&lt;/p&gt;
&lt;p&gt;\begin{align}
Z_{N}^{(D)} = \left(\frac{\beta}{2 \pi i}\right)^{N} &amp;amp;\left( \frac{\pi}{\beta} \right)^{DN/2} \int_{a-i\infty}^{a+i\infty} \cdots \int_{a-i\infty}^{a
+i\infty} \ \mathrm{d}t_{1} \cdots \mathrm{d}t_{N} \ \mathrm{e}^{D \varphi \left(\boldsymbol{t} \right) } \label{eq:partfunsteep} \\
\end{align}&lt;/p&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;p&gt;\begin{align}
\varphi \left(\boldsymbol{t}; \beta, J_{ij} \right) = \beta \sum_{j=1}^{N} t_{j} - \frac{1}{2} \log \det \left( \boldsymbol{V} \right) + \frac{\beta}{4D} \mathrm{Tr} \left( \boldsymbol{H}^{T} \boldsymbol{V}^{-1} \boldsymbol{H} \right) \label{eq:varphi}
\end{align}&lt;/p&gt;
&lt;p&gt;As $D \to \infty$, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Method_of_steepest_descent&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;method of steepest-descent or the saddle-point method&lt;/a&gt; suggests that the partition function will be dominated by its largest contribution, i.e. in the neigbourhood of the maximum $\varphi(\boldsymbol{t^{*}})$ along the integration paths.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Hmm, this doesn&amp;rsquo;t quite seem right #1:&lt;/strong&gt; What does $D \to \infty$ even look like for the last term in Eq. \eqref{eq:varphi}? What does it mean for the input vectors $\{ \boldsymbol{h}_{i} \}$ to become infinite-dimensional? Good points, but let&amp;rsquo;s carry on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The saddle-point values $\boldsymbol{t^{*}}$ are obtained from the set of stationary conditions&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \varphi \left( \boldsymbol{t} \right)}{\partial t_j} \Biggr\rvert_{t_j = t^{*}_{j}} = 0, \qquad j=1,\ldots,N \label{eq:statcond}
\end{align}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Hmm, this doesn&amp;rsquo;t quite seem right #2:&lt;/strong&gt; In the single-variable case, &lt;a href=&#34;&#34;&gt;Baxter (1985)&lt;/a&gt; argues that $\varphi (t)$ is analytic for $\mathrm{Re}(t)&amp;gt;0$ and that we should consider $\varphi (t)$ first for $t$ real and positive. For positive $\beta$ and non-zero magnetic field, the function tends to plus infinity as $t$ tends to either zero or infinity. Thus in between $\varphi(t)$ must have a &lt;em&gt;minimum&lt;/em&gt; at some positive value $t^{*}$ of $t$. Since $\varphi&#39;&#39;(t) &amp;gt; 0$ there is also only one such minimum. If we take the constant $a$ in the integral limits to be $t^{*}$, then along the (imaginary) integration path $\varphi (t)$ has a &lt;em&gt;maximum&lt;/em&gt; at $t=t^{*}$. We naively assume that this kind of saddle-point reasoning transfers to our case in several complex variables with $\varphi : \mathbb{C}^{N} \to \mathbb{C}$ where the equivalent of $\mathrm{Re}(t)&amp;gt;0$ is to try to steer clear of the singularity at $\det \left( \boldsymbol{V} \right)=0$. We will check the numerical behaviour of our $\varphi$-function in &lt;a href=&#34;#31-the-algorithm-bold-moves-on-a-tricky-landscape&#34;&gt;Section 3.1&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Expanding $\varphi$ around $\boldsymbol{t^{*}}$ and then taking the logarithm of Eq. \eqref{eq:partfunsteep} leads to&lt;/p&gt;
&lt;p&gt;\begin{align}
\ln Z_{N}^{(D)} = \frac{DN}{2} \ln \left( \frac{\pi}{\beta} \right) + D \varphi \left( \boldsymbol{t^{*}} \right) + \ln R \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;where we have collected all higher-order contributions and remaining nastiness in $R$. Following &lt;a href=&#34;https://doi.org/10.1103/PhysRev.176.718&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanley (1968)&lt;/a&gt;, the free energy in the limit of large local dimension $D \to \infty$ then becomes&lt;/p&gt;
&lt;p&gt;\begin{align}
-\beta f_{N}^{(\infty)} = \lim_{D \to \infty} D^{-1} \ln \left( Z_{N}^{(D)} / Z_{N}^{(D)}(0) \right) \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
Z_{N}^{(D)}(0) = \left( \left(\pi\right)^{D/2} D^{(D-1)/2} / \Gamma \left(D/2\right) \right)^{N} \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;is a normalization factor&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; accounting for the surface areas of the $(D-1)$-dimensional spheres with radius $\sqrt{D}$ associated to each and every spin degree of freedom. After applying &lt;a href=&#34;https://en.wikipedia.org/wiki/Stirling%27s_approximation#Stirling%27s_formula_for_the_gamma_function&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stirling&amp;rsquo;s asymptotic expansion&lt;/a&gt; to the $\Gamma$-function in the normalization factor and doing some algebra, we end up with&lt;/p&gt;
&lt;p&gt;\begin{align}
\boxed{-\beta f_{N}^{(\infty)} = - \frac{N}{2} - \frac{N}{2} \ln \left( 2\beta \right) + \varphi \left( \boldsymbol{t^{*}} \right)} \label{eq:afe}
\end{align}&lt;/p&gt;
&lt;p&gt;where we have dropped the last term $\lim_{D \to \infty} D^{-1} \ln R$ assuming it tends to zero. Since $\varphi \left( \boldsymbol{t^{*}} \right) \propto N$, the last term actually also survives the limit $N \to \infty$.&lt;/p&gt;
&lt;h2 id=&#34;24-taking-stock-of-what-we-have-done&#34;&gt;2.4. Taking stock of what we have done&lt;/h2&gt;
&lt;p&gt;We have derived a closed-form expression Eq. \eqref{eq:afe} for the approximate free energy of a vector-spin model in the limit of large local spin dimension. Let us take a brief moment to reflect on what we have done and touch on some tangential points.&lt;/p&gt;
&lt;h4 id=&#34;241-questioning-steepest-descent-and-the-large-d-limit&#34;&gt;2.4.1. Questioning steepest descent and the large-$D$ limit&lt;/h4&gt;
&lt;p&gt;The result \eqref{eq:afe} is only sensible if steepest descent is a valid thing to do, which depends on how outrageous the landscape defined by the $\varphi$-function \eqref{eq:varphi} really is. More practically, we will also never &lt;em&gt;really&lt;/em&gt; let the vector-spin dimension $D$ tend towards infinity since our goal is to implement a numerical attention-like neural network module. So large but finite vector dimensions better behave as if they were sufficiently close to infinity. We will find out in &lt;a href=&#34;#31-the-algorithm-bold-moves-on-a-tricky-landscape&#34;&gt;Section 3.1&lt;/a&gt; to what extent these assumptions are valid in practice.&lt;/p&gt;
&lt;h4 id=&#34;242-energy-based-models-and-effective-energy-functions&#34;&gt;2.4.2. Energy-based models and effective energy functions&lt;/h4&gt;
&lt;p&gt;Let us take another look at our model&amp;rsquo;s partition function \eqref{eq:fullpartfun} from an energy-based perspective. For ease of notation, let us call the model parameters $\theta \equiv \{ J_{ij} \}$, the spins $\sigma \equiv \{ \boldsymbol{\sigma}_{i} \}$, and the external magnetic fields $h \equiv \{ \boldsymbol{h}_{i} \}$. We can schematically write our model&amp;rsquo;s partition function as&lt;/p&gt;
&lt;p&gt;\begin{align}
Z_{\theta} \left( h \right) = \int \mathrm{d} \sigma \ \mathrm{e}^{ - E_{\theta}\left( \sigma, h \right) }
\end{align}&lt;/p&gt;
&lt;p&gt;where $E_{\theta}\left( \sigma, h \right)$ denotes the energy function Eq. \eqref{eq:vectrandomising}. If we now introduce an energy-based model $p_{\theta} \left( \sigma, h \right) = \mathrm{e}^{-E_{\theta}\left( \sigma, h \right)} / Z_{\theta}$, we can define the marginal distribution&lt;/p&gt;
&lt;p&gt;\begin{align}
p_{\theta} \left( h \right) = \frac{\int \mathrm{d} \sigma \  \mathrm{e}^{-E_{\theta}\left( \sigma, h \right)}}{Z_{\theta}} = \frac{\mathrm{e}^{-E_{\theta}\left( h \right)}}{Z_{\theta}} \label{eq:ph}
\end{align}&lt;/p&gt;
&lt;p&gt;where the applied magnetic fields act as observables and the spins as latent variables. The effective energy $E_{\theta}\left( h \right)$ equals $E_{\theta}\left( h \right) = - \log \int \mathrm{d} \sigma \ \mathrm{e}^{-E_{\theta}\left( \sigma, h \right)} \approx - \log Z^{\ast}_{\theta} \left( h \right)$, where we have used the steepest-descent approximation for the integral. Taking the logarithm of Eq. \eqref{eq:ph}, we find that $\log p_{\theta} \left( h \right) \approx \log Z^{\ast}_{\theta} \left( h \right) - \log \int \mathrm{d} h \ Z^{\ast}_{\theta} \left( h \right)$.&lt;/p&gt;
&lt;h4 id=&#34;243-spin-glasses-and-mean-field-approximation&#34;&gt;2.4.3. Spin glasses and mean-field approximation&lt;/h4&gt;
&lt;p&gt;Ordered systems have a long history in statistical mechanics. Couplings in these models often encode a translation-invariant lattice geometry, e.g. nearest-neighbour interactions between spins living on a $d$-dimensional hypercubic lattice. One reason for this focus is practical: the regularity in these systems enables mathematical physicists to deploy all kinds of tricks and make progress towards some kind of understanding. In contrast, disordered systems, like spin glasses, are a mess and studying them is all about &lt;a href=&#34;https://www.nobelprize.org/prizes/physics/2021/summary/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;finding order where there seems to be none&lt;/a&gt;. From the perspective of spin glasses, we can summarize our approach as follows: we want to arrive at an approximate yet tractable mean-field spin-glass model where its couplings are treated as parameters learned from data&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Fully-connected models like Sherrington-Kirkpatrick spin-glass models (or Eq. \eqref{eq:vectrandomising}) naturally lead to mean-field theory because the couplings $J_{ij}$ encode long-range interactions where every other spin is just a hop away, see e.g. &lt;a href=&#34;https://arxiv.org/abs/1506.07128&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Janiš (2015)&lt;/a&gt;. Intuitively, all-to-all interactions correspond to the mean-field limit of infinite spatial dimension. To see this, consider a spin in a local nearest-neighbour lattice model getting ever more neighbours as the spatial dimension grows: the notion of nearest neighbours melts away and all spins effectively become connected to each other&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;. Fully-connected non-local couplings and the limit of infinite spatial dimension are two sides of the same mean-field coin.&lt;/p&gt;
&lt;h1 id=&#34;3-implementing-approximate-free-energy-minimization&#34;&gt;3. Implementing approximate free-energy minimization&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Code: A PyTorch implementation of the ideas outlined in this blog post is available in the GitHub repository &lt;a href=&#34;https://github.com/mcbal/afem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mcbal/afem&lt;/code&gt;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this section, we turn the equations of the previous section into the algorithmic backbone of a differentiable vector-spin model. We begin by sketching an approximate free-energy minimization algorithm. We then show how to wrap around the spin model to turn it into an attention module.&lt;/p&gt;
&lt;h2 id=&#34;31-the-algorithm-bold-moves-on-a-tricky-landscape&#34;&gt;3.1. The algorithm: bold moves on a tricky landscape&lt;/h2&gt;
&lt;p&gt;Our goal is to compute the steepest-descent approximation of our model&amp;rsquo;s partition function in a differentiable way. Essentially, we need to solve the set of equations&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \varphi \left( \boldsymbol{t} \right)}{\partial t_j} \Biggr\rvert_{t_j = t^{*}_{j}} = 0, \qquad j=1,\ldots,N
\end{align}&lt;/p&gt;
&lt;p&gt;which corresponds to finding a value $\boldsymbol{t^{*}} = \mathrm{argmin}_{\boldsymbol{t}} \varphi \left( \boldsymbol{t} \right)$ for which the scalar function&lt;/p&gt;
&lt;p&gt;\begin{align}
\varphi \left(\boldsymbol{t}; \beta, J_{ij} \right) = \beta \sum_{j=1}^{N} t_{j} - \frac{1}{2} \log \det \left( \boldsymbol{V} \right) + \frac{\beta}{4D} \mathrm{Tr} \left( \boldsymbol{H}^{T} \boldsymbol{V}^{-1} \boldsymbol{H} \right) \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;attains its minimum, or, equivalently, we need to solve for the root of $\nabla \varphi \left( \boldsymbol{t} \right)$.&lt;/p&gt;
&lt;h4 id=&#34;311-initialization-and-normalization&#34;&gt;3.1.1. Initialization and normalization&lt;/h4&gt;
&lt;p&gt;Until now we have not been explicit about the values of the couplings $\boldsymbol{J}$ and inputs $\boldsymbol{H}$. If we want to implement any of this, we have to be more careful. Recall that the energy function of our model looks like
$
E = - \sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} - \sum_{i=1}^{N} \boldsymbol{h}_{i} \cdot \boldsymbol{\sigma}_{i},\nonumber
$
where all spins $\boldsymbol{\sigma}_{i}$ are fixed to norm $\sqrt{D}$. We&amp;rsquo;d like this energy to remain linearly proportional to the the number of lattice sites. Numerically, we observe that stable root-finding is possible when initializing the couplings according to
\begin{equation}
J_{ij} \sim \mathcal{N} (0, 1/\sqrt{ND} )
\end{equation}
The factor $1/\sqrt{N}$ can be explained from spin-glass mean-field theory&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; whereas the $1/\sqrt{D}$ factor follows from additionally normalizing with respect to the vector dimension to ensure $\sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} \sim \mathcal{O}(N)$. One strategy to normalize the inputs $\boldsymbol{H}$ is to feed them into a layer normalization layer so that $\left\lVert \boldsymbol{h}_{i} \right\rVert \sim \mathcal{O}(\sqrt{D})$ and then explicitly dividing by $\sqrt{D}$ to make them $\mathcal{O}(1)$. A practical consequence of these initialization and normalization choices at the level of the energy function is that the $\varphi$-function changes to&lt;/p&gt;
&lt;p&gt;\begin{align}
\varphi \left(\boldsymbol{t}; \beta, J_{ij} \right) = \beta \sum_{j=1}^{N} t_{j} - \frac{1}{2} \log \det \left( \boldsymbol{V} \right) + \frac{\beta}{4} \mathrm{Tr} \left( \boldsymbol{H}^{T} \boldsymbol{V}^{-1} \boldsymbol{H} \right) \label{eq:varphinorm}
\end{align}&lt;/p&gt;
&lt;p&gt;where the prefactor in the last term changed since we decided on explicitly dividing the layer-normalized $\boldsymbol{H}$ by $1/\sqrt{D}$.&lt;/p&gt;
&lt;h4 id=&#34;312-implicit-layers-for-steepest-descent-root-finding&#34;&gt;3.1.2. Implicit layers for steepest-descent root-finding&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s now find the root of the gradient of $\varphi$ in a differentiable way by combining &lt;a href=&#34;http://implicit-layers-tutorial.org/implicit_functions/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implicit layers&lt;/a&gt; with a black-box root-finding algorithm like &lt;a href=&#34;https://en.wikipedia.org/wiki/Newton%27s_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Newton&amp;rsquo;s method&lt;/a&gt;, which requires access to both a function (the gradient of $\varphi$) and its gradient (the Jacobian of the gradient of $\varphi$). We could rely on automatic differentiation to calculate these gradients, but we just as well exploit the fact that we have an analytical expression Eq. \eqref{eq:varphinorm}. Grabbing a coffee and peeking at the &lt;a href=&#34;https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Matrix Cookbook&lt;/a&gt;, we can figure out what happens&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when we wiggle around $t_{i}$ (the gradient vector at $\boldsymbol{t}$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{align}
\left[ \nabla \varphi \left( \boldsymbol{t} \right) \right]_{i} = \beta - \frac{1}{2} \left[ \boldsymbol{V}^{-1} \right]_{ii} - \frac{\beta}{4} \left[ \boldsymbol{V}^{-T} \boldsymbol{H} \boldsymbol{H}^{T} \boldsymbol{V}^{-T} \right]_{ii} \nonumber
\end{align}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when we wiggle around both $t_{i}$ and $t_{j}$ (the symmetric Hessian matrix at $\boldsymbol{t}$)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{align}
\left[ \boldsymbol{J}(\nabla \varphi \left( \boldsymbol{t} \right)) \right]_{ij} = \frac{1}{2} &amp;amp;\left[ \boldsymbol{V}^{-1} \odot \boldsymbol{V}^{-T} \right]_{ij} \nonumber \\
&amp;amp;+ \frac{\beta}{4} \left[ \boldsymbol{V}^{-T} \boldsymbol{H} \boldsymbol{H}^{T} \boldsymbol{V}^{-T} \boldsymbol{V}^{-T} \odot \boldsymbol{I} \right]_{ij} \nonumber \\
&amp;amp;+ \frac{\beta}{4} \left[ \boldsymbol{V}^{-T} \boldsymbol{V}^{-T} \boldsymbol{H} \boldsymbol{H}^{T} \boldsymbol{V}^{-T} \odot \boldsymbol{I} \right]_{ij} \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;Given an initial guess $\boldsymbol{t_{0}} \in \mathbb{R}^{N}_{&amp;gt;0}$ and input data $\boldsymbol{H} \in \mathbb{R}^{N \times D}$, we can now construct a differentiable root-solver which returns $\boldsymbol{t^{*}}$. It is important to keep in mind that the stationary value $\boldsymbol{t^{*}}$ actually depends on $\left(\beta, \boldsymbol{J}, \boldsymbol{H} \right)$ implicitly. Since we make use of implicit layers within an automatic differentation framework, these dependencies are kept track of and are included in the computational graph.&lt;/p&gt;
&lt;h4 id=&#34;313-fun-with-free-energies&#34;&gt;3.1.3. Fun with free energies&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s test the algorithm by initializing a random vector-spin model and applying a random magnetic field at every site. For visualization purposes, we restrict the auxiliary variables to be effectively one-dimensional by defining $\boldsymbol{t} = t \boldsymbol{1}_{N}$ with just a single scalar parameter $t \in \mathbb{R}_{&amp;gt;0}$. We can probe a &lt;code&gt;VectorSpinModel&lt;/code&gt; and get the approximate free energy for a given set of parameters and inputs by running the following script:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  from afem.models import VectorSpinModel

  num_spins, dim = 32, 128
  model = VectorSpinModel(num_spins=num_spins, dim=dim, beta=1.0)

  x = (torch.randn(1, num_spins, dim) / np.sqrt(dim)).requires_grad_()
  t0 = torch.ones(1)

  afe = model(x, t0, return_afe=True).afe
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the forward pass, the root $\boldsymbol{t^{*}}$ is computed and then fed into Eq. \eqref{eq:afe} to calculate the approximate free energy. We can verify that our algorithm is doing something sensible by sweeping across the auxiliary $t$-values and plotting $\varphi$ and its derivatives:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;phi_1d_plot.png&#34; alt=&#34;alt text&#34; title=&#34;Sweep across auxiliary variable&#34;&gt;&lt;/p&gt;
&lt;p&gt;The region close to $t=0$ looks terrifying. In this regime, $t$ is likely not large enough to overshadow the largest eigenvalue of the couplings so we lose positive definiteness and its nice properties. Let&amp;rsquo;s try to stay away from that region by always initializing $\boldsymbol{t}_{0}$ sufficiently far from it. Depending on the parameters and initial guess provided to the solver, one can of course end up in less favourable landscapes where root-solving can become difficult due to zero gradients or extreme sensitivity to initial conditions. Fortunately, when the root-solving step fails, it tends to fail spectacularly.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now sweep across inverse temperature $\beta$ to get some intuition. From the analytical expression of the free energy, we can deduce that for small $\beta$ (high temperature) the entropy term reigns while for large $\beta$ (low temperature) the energy terms take over.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;beta_sweep.gif&#34; alt=&#34;alt text&#34; title=&#34;Sweep across inverse temperature&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally, let&amp;rsquo;s lift the one-dimensional restriction on $\boldsymbol{t}$ and plot $\varphi (\boldsymbol{t})$ for two spins. In that case, $\boldsymbol{t}$ is also just two-dimensional so we can still visualize the optimization landscape.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;phi_2d_plot.png&#34; alt=&#34;alt text&#34; title=&#34;Two-dimensional auxiliary variables&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;32-the-attention-module-probing-spins-with-data&#34;&gt;3.2. The attention module: probing spins with data&lt;/h2&gt;
&lt;p&gt;In the previous section, we showed how to numerically compute the steepest-descent approximation of a vector-spin model&amp;rsquo;s partition function and hence its free energy. Since this approximation is fully differentiable, we can also take derivatives with respect to conjugate variables. Let&amp;rsquo;s use this observation to construct an attention module.&lt;/p&gt;
&lt;h4 id=&#34;321-spin-expectation-values&#34;&gt;3.2.1. Spin expectation values&lt;/h4&gt;
&lt;p&gt;We can calculate spin expectation values or magnetizations from our partition function approximation by differentiating with respect to the applied magnetic fields:&lt;/p&gt;
&lt;p&gt;\begin{align}
\langle \boldsymbol{\sigma}_{i} \rangle = \frac{\mathrm{d} \log Z \left( \boldsymbol{t}, \boldsymbol{H} \right)}{\mathrm{d} \boldsymbol{h}_{i}} = \frac{\partial \varphi}{\partial \boldsymbol{t}} \frac{\partial \boldsymbol{t}}{\partial \boldsymbol{h}_{i}} + \frac{\partial \varphi}{\partial \boldsymbol{h}_{i}} \label{eq:spinevgeneral}
\end{align}&lt;/p&gt;
&lt;p&gt;If we evaluate the partition function approximation at the stationary point $\boldsymbol{t^{\ast}}$, the first term drops out because $\partial_{\boldsymbol{t}} \varphi \rvert_{\boldsymbol{t}=\boldsymbol{t^{\ast}}} = 0$. Assuming that the matrix $\boldsymbol{V}$ (and hence the couplings $\boldsymbol{J}$) do not depend on the inputs $\boldsymbol{H}$, the spin expectation value boils down to&lt;/p&gt;
&lt;p&gt;\begin{align}
\langle \boldsymbol{\sigma}_{i} \rangle = \frac{\partial \varphi}{\partial \boldsymbol{h}_{i}} = \frac{\beta}{2} \sum_{j} \boldsymbol{V}^{-1}_{ij} \boldsymbol{h}_{j} \label{eq:spinev}
\end{align}&lt;/p&gt;
&lt;p&gt;which, for every site, is just a weighted sum of inputs. In the language of transformers, Eq. \eqref{eq:spinev} resembles an update step where $\boldsymbol{V}^{-1}$ can be interpreted as a symmetric attention matrix. Expanding the matrix inverse reveals a residual connection as the zero-th order contribution&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Since the couplings are scalars at the level of the energy function Eq. \eqref{eq:vectrandomising}, getting terms to act on the hidden dimension seems to be impossible. But by considering couplings $\boldsymbol{J}(\boldsymbol{H})$ which do depend on inputs, additional terms can appear in Eq. \eqref{eq:spinev} propagating via dependencies in $\boldsymbol{V}$. Instead of calculating these gradients analytically, we should of course just let our automatic differentiation framework compute them for us.&lt;/p&gt;
&lt;h4 id=&#34;322-wrapping-around-the-spin-model&#34;&gt;3.2.2. Wrapping around the spin model&lt;/h4&gt;
&lt;p&gt;At this point, we have done all the heavy lifting. All that remains is to write a wrapper so that we can use our module just like any other explicit attention module:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  from afem.attention import VectorSpinAttention

  num_spins, dim = 32, 128
  attention = VectorSpinAttention(num_spins=num_spins, dim=dim, beta=1.0)

  x = torch.randn(1, num_spins, dim).requires_grad_()

  attention(x)  # (1, 32, 128)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Inside the forward pass of &lt;code&gt;VectorSpinAttention&lt;/code&gt;, (normalized) inputs are sent to an internal &lt;code&gt;VectorSpinModel&lt;/code&gt; which solves for the saddle point $\boldsymbol{t^{*}}$ and then feeds it into the steepest descent partition function to calculate magnetizations according to Eq. \eqref{eq:spinevgeneral}.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s finish this section by discussing some of the peculiarities of our approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stability and symmetry:&lt;/strong&gt; The root-finding is stable as long as $\det \boldsymbol{V} &amp;gt; 0$, which ensures that $\boldsymbol{V}$ is nonsingular and which is garantueed as long as the quadratic form is positive definite. A quadratic form involving a general $\boldsymbol{V}$ (i.e. with nonsymmetric couplings $\boldsymbol{J}$) is positive definite iff its symmetric part has all positive eigenvalues. When this is no longer the case, things tend to blow up.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scaling:&lt;/strong&gt; Our approach is kind of slow because calculating inverses scales as $\mathcal{O}\left(N^3\right)$. Yet there might be ways to approximate the slow parts of the algorithm similar to how vanilla transformers can be understood to approximate mean-field fixed-point equations&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lack of permutation invariance:&lt;/strong&gt; Our model is not permutation invariant with the default choice of input-independent couplings: every spin has a role to play.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Input-dependent couplings:&lt;/strong&gt; Because our default model assumes coupling-independent couplings $\boldsymbol{J}$, Eq. \eqref{eq:spinev} features just a &amp;ldquo;token-mixing&amp;rdquo; attention operation. Channel-mixing terms can appear when we consider the physically very weird setup where the couplings are made dependent on the applied magnetic fields. One possible choice could be:
\begin{align}
\boldsymbol{J}(\boldsymbol{H}) = \frac{\tanh \left( \boldsymbol{H} \boldsymbol{Q} \boldsymbol{K}^T \boldsymbol{H}^T \cdot \sqrt{D} \right)}{\sqrt{ND}} \nonumber
\end{align}
where $\boldsymbol{Q}$ and $\boldsymbol{K}$ are linear transformations acting on the hidden dimension and where the scaling factors have been inserted because of the normalization conventions we discussed in &lt;a href=&#34;#311-initialization-and-normalization&#34;&gt;Section 3.1.1&lt;/a&gt;. We hypothesize that additional terms in the spin expectation value Eq. \eqref{eq:spinev} arising from input-dependent couplings might be related to channel-mixing feed-forward networks in transformer modules.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;323-comparison-with-vanilla-transformers&#34;&gt;3.2.3. Comparison with vanilla transformers&lt;/h4&gt;
&lt;p&gt;In this final section, let&amp;rsquo;s summarize our approach on a high level by visually comparing it to vanilla transformers and deep equilibrium approaches.&lt;/p&gt;
&lt;img src=&#34;arch_vanilla_deq.png&#34; alt=&#34;Vanilla transformer and deep equilibrium transformer&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;The vanilla transformer &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Vaswani et al. (2017)]&lt;/a&gt; (left above) is an explicit architecture which processes input sequences sequentially through a stack of transformer modules. Deep equilibrium transformers &lt;a href=&#34;https://arxiv.org/abs/1909.01377&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[Bai et al. (2019)]&lt;/a&gt; (right above) compute the output of a transformer module by implicitly solving for the fixed point of $f(z, x) = z$ where $f$ denotes the explicit transformer module. Data is repeatedly inserted by adding it to the current iteration of $z$ inside the module until fixed-point convergence. The  converged fixed point is considered the output of the module. Backpropagation through the iterations of the solver is avoided by using the implicit function theorem to calculate gradients directly at the equilibrium point. Instead of a stack of layers, there&amp;rsquo;s just a single layer.&lt;/p&gt;
&lt;p&gt;But deep equilibrium transformers still treat the transformer module as a black box. In &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms&lt;/a&gt; we looked for a physical spin-model interpretation of the deep equilibrium fixed-point procedure (left below). We argued how the update step of a vanilla transformer module resembled mean-field fixed-point equations of a vector-spin model, explaining the successful pattern of token-mixing, residual connections, normalization layers, and feed-forward or channel-mixing modules from a physical spin systems&#39; perspective.&lt;/p&gt;
&lt;img src=&#34;arch_dia_afem.png&#34; alt=&#34;Deep implicit attention and approximate free-energy minimization&#34; width=&#34;600px&#34;/&gt;
&lt;p&gt;In this work (right above), we continued on the path of spin expectation values but replaced solving mean-field fixed-point equations with directly taking derivatives of the steepest-descent partition function of a particular class of vector-spin models. The fixed-point procedure is replaced with a root-solving step to determine the steepest-descent partition function. The structure of our module&amp;rsquo;s output reveals the same successful transformer-like pattern of token-mixing (attention) and channel-mixing (feed-forward) interspersed with normalization layers and residual connections.&lt;/p&gt;
&lt;h1 id=&#34;4-conclusion&#34;&gt;4. Conclusion&lt;/h1&gt;
&lt;p&gt;In this post, we introduced transformer modules as wrappers around statistical-mechanical vector-spin models. We used implicit layers to construct a class of approximate yet tractable vector-spin models whose couplings act as parameters that can be learned from data. We showed how these models can act as transformer-like attention modules by routing inputs to applied magnetic fields and returning spin expectation values derived from their steepest-descent partition function.&lt;/p&gt;
&lt;p&gt;By zooming out and approaching transformers from a tangential, statistical-mechanical point of view, we were able to develop a physical intuition of transformers that seems hard to arrive at when restricting oneself to perturbing explicit neural network architectures. Recognizing transformer modules as spin models in disguise might not only unify architectural variations but also elucidate the high-level architectural convergence and empirical success of transformers in deep learning.&lt;/p&gt;
&lt;h1 id=&#34;references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/h1&gt;
&lt;p&gt;If you happen to find this work useful, please consider citing it as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{bal2021afem,
  title   = {Transformers from Spin Models: Approximate Free Energy Minimization},
  author  = {Bal, Matthias},
  year    = {2021},
  month   = {October},
  url     = {https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;We could have turned to the mean-field free energies associated with the adaptive TAP equations discussed in &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention&lt;/a&gt;, but we decided on attacking the problem from the steepest-descent angle on the full partition function.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For example, see &lt;em&gt;&lt;a href=&#34;http://blog.math.toronto.edu/GraduateBlog/files/2020/07/ut-thesis-Ko-updated.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Free Energy of Spherical Vector Spin Glasses (Ko, 2018)&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.04441&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Free Energy in the Mixed p-spin Models With Vector Spins (Panchenko, 2015)&lt;/a&gt;&lt;/em&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The original 1968 paper has a small typo here: the $\nu$ in the paper&amp;rsquo;s Eq. (23) should be $\nu^{1/2}$ for the surface area of a $\nu-1$-dimensional sphere with radius $R=\nu^{1/2}$ embedded in $\nu$ dimensions. Using the paper&amp;rsquo;s formula, an annoying $\ln \nu$ term won&amp;rsquo;t cancel out in the limiting free energy calculation.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;In contrast to spin glasses however, we do not (yet want to go full Bayesian and) treat the couplings as drawn from some kind of probability distribution. For now, we settle for obtaining point estimates of model parameters.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;By promoting sparseness in the couplings, a model might become less mean-field-y, which might be one of the reasons behind the sucess of scaled &lt;code&gt;softmax&lt;/code&gt; attention in vanilla transformers.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;From &lt;a href=&#34;https://arxiv.org/abs/1506.07128&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Janiš (2015)&lt;/a&gt;: &lt;em&gt;The mean-field limit to infinite dimensions or long-range interaction introduces a new large scale. To make the thermodynamic limit meaningful the dependence of the energy on this new large scale must be compensated by rescaling the non-local spin exchange so that the energy remains linearly proportional to the volume or the number of lattice sites (spins).&lt;/em&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;We can expand the right-hand side using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Woodbury_matrix_identity#Inverse_of_a_sum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;special case of the Woodbury matrix identity&lt;/a&gt; to find
\begin{align}
\boldsymbol{V}^{-1} &amp;amp;= \left( \mathrm{diag} ( \boldsymbol{t} ) - \boldsymbol{J} \right)^{-1} = \sum_{k=0}^{\infty} \left( \mathrm{diag} \left( \boldsymbol{t}^{-1} \right) \boldsymbol{J} \right)^{k} \mathrm{diag} \left( \boldsymbol{t}^{-1} \right) \nonumber
\end{align}
which converges if the largest absolute value of the eigenvalues of the matrix inside the power-brackets is less than 1. So the spin expectation value looks like a sum of contributions that mix and weigh inputs of different sites.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;As discussed previously in &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms&lt;/a&gt;. In that setting, calculating inverses was sidestepped by approximating part of the solution with a feed-forward neural network.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
