<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mean-Field Theory | mcbal</title>
    <link>https://mcbal.github.io/tag/mean-field-theory/</link>
      <atom:link href="https://mcbal.github.io/tag/mean-field-theory/index.xml" rel="self" type="application/rss+xml" />
    <description>Mean-Field Theory</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>Matthias Bal © 2023</copyright><lastBuildDate>Sun, 19 Jun 2022 09:28:17 +0100</lastBuildDate>
    <image>
      <url>https://mcbal.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Mean-Field Theory</title>
      <link>https://mcbal.github.io/tag/mean-field-theory/</link>
    </image>
    
    <item>
      <title>Spin-Model Transformers</title>
      <link>https://mcbal.github.io/post/spin-model-transformers/</link>
      <pubDate>Sun, 19 Jun 2022 09:28:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/spin-model-transformers/</guid>
      <description>&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-mean-field-theory-of-asymmetric-ising-models-with-binary-spins&#34;&gt;Mean-field theory of asymmetric Ising models with binary spins&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#21-setting-the-scene-the-kinetic-ising-model&#34;&gt;Setting the scene: the kinetic Ising model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#22-mean-field-theory-and-kullback-leibler-divergence&#34;&gt;Mean-field theory and Kullback-Leibler divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#23-the-plefka-expansion-interpolating-distributions&#34;&gt;The Plefka expansion: interpolating distributions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#24-naive-mean-field-and-thouless-anderson-palmer-approximations&#34;&gt;Naive mean-field and Thouless-Anderson-Palmer approximations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#25-a-simple-jax-implementation&#34;&gt;A simple JAX implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins&#34;&gt;Mean-field theory of asymmetric Ising models with vector spins&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#31-vector-spins-distributions-on-hyperspheres&#34;&gt;Vector spins: distributions on hyperspheres&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#32-magnetizations-and-limit-of-large-vector-dimension&#34;&gt;Magnetizations and limit of large vector dimension&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#33-first-order-naive-mean-field-approximation&#34;&gt;First-order naive mean-field approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#34-second-order-thouless-anderson-palmer-approximation&#34;&gt;Second-order Thouless-Anderson-Palmer approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#35-a-simple-jax-implementation&#34;&gt;A simple JAX implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-spin-transformer-modules-a-family-of-transformer-like-modules&#34;&gt;Spin-transformer modules: a family of transformer-like modules&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#41-connecting-the-dots&#34;&gt;Connecting the dots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#42-fast--and-slow-moving-parameters&#34;&gt;Fast- and slow-moving parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#43-a-simple-jax-implementation&#34;&gt;A simple JAX implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#appendices&#34;&gt;Appendices&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#a1-vector-spin-distribution-normalization-constant&#34;&gt;Vector-spin distribution: normalization constant&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a2-vector-spin-distribution-expected-value-first-moment&#34;&gt;Vector-spin distribution: expected value (first moment)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a3-vector-spin-distribution-variance-second-moment&#34;&gt;Vector-spin distribution: variance (second moment)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a4-ratio-of-modified-bessel-functions-of-the-first-kind&#34;&gt;Ratio of modified Bessel functions of the first kind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a5-general-case-partial-derivatives-with-respect-to-alpha&#34;&gt;General case: partial derivatives with respect to $\alpha$&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ &lt;strong&gt;&lt;a href=&#34;https://github.com/mcbal/spin-model-transformers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mcbal/spin-model-transformers&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a series of previous &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog posts&lt;/a&gt;, we have tried to connect the forward pass of a transformer neural-network module to computing mean magnetizations in disordered Ising-like vector-spin models with parameterized couplings and external magnetic fields. According to this perspective, the forward pass of a transformer module can be understood as computing statistical observables given a specific realization of quenched couplings and external magnetic fields while the backward pass nudges the parameterized couplings and external magnetic fields. Physically, the transformer module represents an interacting many-body system modulating its behavior by learning to respond to being probed and driven in all kinds of ways.&lt;/p&gt;
&lt;p&gt;However, both the mean-field message-passing approach of &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms (2021)&lt;/a&gt; and the saddle-point free-energy approach of &lt;a href=&#34;https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers from Spin Models: Approximate Free Energy Minimization (2021)&lt;/a&gt; inherently rely on methods that are only well-defined for spin models with symmetric coupling matrices, whose stochastic dynamics obey detailed balance and converge to a steady-state equilibrium characterized by the Boltzmann distribution. Since the softmax attention matrix in transformer modules is famously asymmetric, we had better come up with a more convincing approach to establish a convincing correspondence.&lt;/p&gt;
&lt;p&gt;To capture spin models with asymmetric coupling matrices, we turn to non-equilibrium spin systems, whose dynamics can be pretty wild yet gentle enough to support regimes where relaxation to a non-equilibrium or near-equilibrium steady state can occur. In the past few decades, dynamical mean-field approaches have been developed for the binary kinetic Ising model, which exhibits non-equilibrium behavior when couplings are asymmetric or when parameters are subject to rapid changes. In this post, we generalize a particular dynamical mean-field approach from binary spins to vector spins and relate the resulting mean-field update equations for the magnetizations to the forward pass of a transformer module. We find that the spin-model structure is rich enough for the update equations to yield residual connections, attention terms, and feed-forward correction terms, motivating a family of physics-inspired transformers.&lt;/p&gt;
&lt;h1 id=&#34;2-mean-field-theory-of-asymmetric-ising-models-with-binary-spins&#34;&gt;2. Mean-field theory of asymmetric Ising models with binary spins&lt;/h1&gt;
&lt;p&gt;In this section, we review known results on mean-field theory approaches to capturing the stochastic dynamics of binary kinetic Ising models. Readers familiar with this framework can skip ahead to &lt;a href=&#34;#3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins&#34;&gt;Section 3&lt;/a&gt; where we develop a generalization to vector spins. We primarily follow the discussion outlined in &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;A unifying framework for mean-field theories of asymmetric kinetic Ising systems&lt;/em&gt; (Aguilera et al., 2021)&lt;/a&gt;. We implement the mean-field update equations for the mean magnetizations in JAX and run a few numerical experiments.&lt;/p&gt;
&lt;h2 id=&#34;21-setting-the-scene-the-kinetic-ising-model&#34;&gt;2.1. Setting the scene: the kinetic Ising model&lt;/h2&gt;
&lt;img src=&#34;binary_spins.png&#34; alt=&#34;Random Ising model configuration with binary spins&#34; width=&#34;250px&#34;/&gt;
&lt;p&gt;We consider a kinetic Ising model describing a system made up of $N$ interacting binary spins $s_{i,t} \in \{-1, 1\}$ that evolve in discrete time steps $t$ according to synchronous dynamics, i.e. all spins get updated at the same time in parallel. Given a configuration $\mathbf{s}_{t-1} = \{ s_{1,t-1}, s_{2,t-1}, \ldots, s_{N,t-1} \}$ at time $t-1$, we consider the spins $\mathbf{s}_{t}$ at time $t$ to be conditionally independent random variables captured by a discrete-time Markov chain transition probability&lt;/p&gt;
&lt;p&gt;\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{\sum_{s_{i,t}} \mathrm{e}^{s_{i,t} h_{i,t}}} = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{2 \cosh h_{i,t}}, \label{eq:pcond}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the effective external field is given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
h_{i,t} = x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1}.
\end{equation}&lt;/p&gt;
&lt;p&gt;Here, the parameters $\mathbf{x}$ represent the (possibly time-dependent) local external fields at each site while the coupling parameters $\mathbf{J}$ are a specific realization of quenched disorder encoding the interactions between pairs of spins. Using the probability mass function of the previous state $P( \mathbf{s}_{t-1} )$ we can write the distribution of the current state as&lt;/p&gt;
&lt;p&gt;\begin{equation}
P( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1}} P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P( \mathbf{s}_{t-1} ), \label{eq:marginal}
\end{equation}&lt;/p&gt;
&lt;p&gt;which, when applied recursively, traces the evolution of the system starting from some initial distribution $P( \mathbf{s}_{0} )$. Unless we turn off the couplings by setting $\mathbf{J} = \mathbf{0}$, the marginal distribution $P( \mathbf{s}_{t} )$ is not factorized and tends to be quite complicated. Our goal is to compute statistical properties of the system, such as the mean magnetizations&lt;/p&gt;
&lt;p&gt;\begin{equation}
m_{i,t} = \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t} ),
\end{equation}&lt;/p&gt;
&lt;p&gt;as well as correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
C_{ik,t} = \sum_{\mathbf{s}_{t}} s_{i,t} s_{k,t} P( \mathbf{s}_{t} ) - m_{i,t} m_{k,t},
\end{equation}&lt;/p&gt;
&lt;p&gt;and delayed correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{il,t} = \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1}} s_{i,t} s_{l,t-1} P( \mathbf{s}_{t}, \mathbf{s}_{t-1} ) - m_{i,t} m_{l,t-1}.
\end{equation}&lt;/p&gt;
&lt;p&gt;Since the above expressions involve summing over a large amount of possible spin configurations, they are not very useful in practice. So we will try to approximate the tricky marginal distribution $P( \mathbf{s}_{t} )$ defined in Eq. \eqref{eq:marginal} using a mean-field theory approach.&lt;/p&gt;
&lt;h2 id=&#34;22-mean-field-theory-and-kullback-leibler-divergence&#34;&gt;2.2. Mean-field theory and Kullback-Leibler divergence&lt;/h2&gt;
&lt;p&gt;Mean-field theory tries to approximate a complicated object ${\color{red}P}$ by wiggling around the parameters of a simple, analytically tractable parameterized ansatz ${\color{green}Q_{\theta}}$ to get as close as possible to ${\color{red}P}$. At risk of inducing headaches in mathematicians by calling everything a manifold, we can picture what is going on geometrically as trying to approximate a target probability distribution $P( \mathbf{s}_{t} \vert \mathbf{x}, \mathbf{J})$ and its statistical properties $\mathbf{m}_{t}$, $\mathbf{C}_{t}$, and $\mathbf{D}_{t}$ by restricting ourselves to a submanifold of tractable probability distributions. A particularly convenient submanifold is that of factorized models, where each point on the submanifold corresponds to a distribution parameterized by a vector $\boldsymbol{\theta}_{t}$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} \theta_{i,t}}}{2 \cosh \theta_{i,t}}, \label{eq:q}
\end{equation}&lt;/p&gt;
&lt;p&gt;so that the mean magnetizations are simply given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
m_{i,t} = \tanh \theta_{i,t}  \label{eq:meanmagstanh}
\end{equation}&lt;/p&gt;
&lt;p&gt;as there are no couplings between spins. The factorized model $Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}^{*}_{t} )$ that minimizes the Kullback-Leibler (KL) divergence&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) = \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log \frac{P( \mathbf{s}_{t})}{Q_{\theta}( \mathbf{s}_{t})} \label{eq:kl}
\end{equation}&lt;/p&gt;
&lt;p&gt;has mean magnetizations $\mathbf{m}_{t}$ identical to those of the target distribution $P( \mathbf{s}_{t})$ since, for all spins $i=1,2,\ldots,N$, we find that&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}} &amp;amp;= - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \frac{\partial \log Q_{\theta}( \mathbf{s}_{t}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}}  \\
&amp;amp;= - \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t}) + \tanh \theta^{*}_{i,t} \\
&amp;amp;= -m^{{\color{red}P}}_{i,t} + m^{{\color{green}Q_{\theta^{*}}}}_{i,t} = 0, \label{eq:klm}
\end{align}&lt;/p&gt;
&lt;p&gt;where $m^{{\color{red}P}}_{i,t}$ and $m^{{\color{green}Q_{\theta^{*}}}}_{i,t}$ respectively denote the expectation values of $s_{i,t}$ with respect to ${\color{red}P}$ and ${\color{green}Q_{\theta^{*}}}$. Indeed, minimizing $D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}})$ tries to cover the modes of ${\color{red}P}$ by moment matching since the expectation value in Eq. \eqref{eq:kl} is calculated with respect to ${\color{red}P}$.&lt;/p&gt;
&lt;h2 id=&#34;23-the-plefka-expansion-interpolating-distributions&#34;&gt;2.3. The Plefka expansion: interpolating distributions&lt;/h2&gt;
&lt;p&gt;Great, but is it even possible to find the parameters&lt;/p&gt;
&lt;p&gt;\begin{equation}
\DeclareMathOperator*{\argmin}{arg\,min}
\boldsymbol{\theta}^{*}_{t} = \argmin_{\boldsymbol{\theta}_{t}} \left( - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log Q_{\theta}( \mathbf{s}_{t}) \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;that minimize the KL divergence? Well, that&amp;rsquo;s going to be hard, unless you already know the target distribution $P( \mathbf{s}_{t})$, or you have a clever way of approximately evaluating the expectation value of $\log {\color{green}Q_{\theta}}$ with respect to ${\color{red}P}$. So let us introduce some more distributions to get around this issue. To apply the Plefka expansion to our problem, we introduce the conditional distribution&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_{\alpha}( \mathbf{s}_{t}\vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N}  \frac{\mathrm{e}^{s_{i,t} h_{i,t}(\alpha) }}{2 \cosh h_{i,t}(\alpha)}, \label{eq:pcondalt}
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
h_{i,t}(\alpha) = (1-\alpha) \theta_{i,t} + \alpha \left( x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1} \right), \label{eq:pcondalth}
\end{equation}&lt;/p&gt;
&lt;p&gt;parameterized by a scalar $\alpha$ interpolating between $P_{\alpha=0}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} )$ (Eq. \eqref{eq:q}) and $P_{\alpha=1}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} )$ (Eq. \eqref{eq:pcond}). Using Eq. \eqref{eq:pcondalt}, we can construct an approximate marginal distribution $P_{\alpha}( \mathbf{s}_{t})$, leading to $\alpha$-dependent statistical properties $\mathbf{m}_{t}(\alpha)$, $\mathbf{C}_{t}(\alpha)$, and $\mathbf{D}_{t}(\alpha)$ for the approximate system. The Plefka expansion then boils down to writing these properties as Taylor series expansions around the factorized model $\alpha=0$. For the mean magnetizations, the expansion up to $n$-th order looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}_{t}(\alpha) = \mathbf{m}_{t}(\alpha=0) + \sum_{k=1}^{n} \frac{\alpha^k}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}), \label{eq:mtaylor}
\end{equation}&lt;/p&gt;
&lt;p&gt;where all coefficients in the expansion are functions of $\boldsymbol{\theta}_{t}$ via Eq. \eqref{eq:pcondalth}. The mean-field approximation is computed by setting $\alpha=1$ so that the original marginal distribution is recovered and Eq. \eqref{eq:klm} holds, which implies that $\mathbf{m}_{t}(\alpha=1) = \mathbf{m}_{t}(\alpha=0)$ and thus&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{k=1}^{n} \frac{1}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}) = 0. \label{eq:mftheta}
\end{equation}&lt;/p&gt;
&lt;p&gt;Finally, we solve Eq. \eqref{eq:mftheta} for $\boldsymbol{\theta}_{t}$ to find the mean-field values $\boldsymbol{\theta}^{*}_{t}$ of the parameters of the distribution Eq. \eqref{eq:q}. Physically, we are tuning the effective external magnetic fields of the factorized ansatz to $\boldsymbol{\theta}^{*}_{t}$ so that its approximate mean magnetizations get as close as possible to the true ones.&lt;/p&gt;
&lt;h2 id=&#34;24-naive-mean-field-and-thouless-anderson-palmer-approximations&#34;&gt;2.4. Naive mean-field and Thouless-Anderson-Palmer approximations&lt;/h2&gt;
&lt;p&gt;We now consider first and second order approximations of the mean magnetizations Eq. \eqref{eq:mtaylor} to recover respectively the naive mean-field and Thouless-Anderson-Palmer (TAP) approximations for the binary kinetic Ising model. The starting point is a Plefka expansion around factorized models at times $t-1$ and $t$. From Eq. \eqref{eq:marginal} and Eq. \eqref{eq:pcondalt}, we construct a marginal probability distribution&lt;/p&gt;
&lt;p&gt;\begin{equation}
P^{[t-1:t]}_{\alpha}( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ),
\end{equation}&lt;/p&gt;
&lt;p&gt;interpolating between $P^{[t-1:t]}_{\alpha=0}( \mathbf{s}_{t} ) = Q( \mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\alpha=1}( \mathbf{s}_{t} ) = P( \mathbf{s}_{t} )$. The corresponding mean magnetizations are&lt;/p&gt;
&lt;p&gt;\begin{align}
m_{i,t}(\alpha) &amp;amp;= \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1},\mathbf{s}_{t-2}} s_{i,t} \, P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ) \\
&amp;amp;= \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} \tanh h_{i,t}(\alpha) \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} )
\end{align}&lt;/p&gt;
&lt;p&gt;Following Eq. \eqref{eq:mftheta}, the first-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} = \left( 1-m^{2}_{i,t} \right) \left( -\theta_{i,t} + x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right) = 0,
\end{equation}&lt;/p&gt;
&lt;p&gt;so that $\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1}$ and we end up with the naive mean-field equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{m_{i,t} = \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right)} \label{eq:naivem}
\end{equation}&lt;/p&gt;
&lt;p&gt;Again following Eq. \eqref{eq:mftheta}, the second-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} + \frac{1}{2} \frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial\alpha^2} = 0,
\end{equation}&lt;/p&gt;
&lt;p&gt;where the second-order derivative, neglecting terms higher than $\mathcal{O}(\alpha^2)$, is&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial\alpha^2} \approx -2 m_{i,t} \left( 1-m^{2}_{i,t} \right) \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{equation}
\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;and we end up with the TAP mean-field equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{m_{i,t} = \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right) \right)} \label{eq:tapm}
\end{equation}&lt;/p&gt;
&lt;p&gt;which includes the so-called Onsager correction term. The mean-field equations obtained above can also be elegantly derived using a Legendre transformation of the generating functional of the set of trajectories of the model, as outlined in e.g. &lt;a href=&#34;https://arxiv.org/abs/1103.1044&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Dynamical TAP equations for non-equilibrium Ising spin glasses (2011)&lt;/em&gt;&lt;/a&gt;. We can also derive second-order TAP approximations of the correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
C_{ik,t} = \begin{cases}
1 - m^{2}_{i,t}  &amp;amp; i = k \\
\left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{k,t} \right) \sum_{j} J_{ij} J_{kj} \left( 1-m^{2}_{j,t-1} \right) &amp;amp; i \neq k \label{eq:tapc}
\end{cases}
\end{equation}&lt;/p&gt;
&lt;p&gt;and delayed correlations&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{il,t} = J_{il} \left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{l,t-1} \right) \left( 1 + 2 J_{il} m_{i,t} m_{l,t-1} \right). \label{eq:tapd}
\end{equation}&lt;/p&gt;
&lt;p&gt;We refer to &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Aguilera et al., 2020)&lt;/a&gt; for full derivations of the above mean-field results as well as variations based on different approximations of the marginal distribution $P( \mathbf{s}_{t} )$.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In summary, given the mean magnetizations $\mathbf{m}_{t-1}$ of the system at time $t-1$, we can use equations \eqref{eq:tapm} \eqref{eq:tapc} \eqref{eq:tapd} to compute a tuple $(\mathbf{m}_{t},\mathbf{C}_{t},\mathbf{D}_{t})$ of approximate statistical properties  of the system at time $t$. The time evolution of the system can be captured at the mean-field level by recursively computing $\mathbf{m}_{t}$ starting from an initial state $\mathbf{m}_{0}$ (with approximation errors likely accumulating over the course of the time evolution).&lt;/p&gt;
&lt;h2 id=&#34;25-a-simple-jax-implementation&#34;&gt;2.5. A simple JAX implementation&lt;/h2&gt;
&lt;p&gt;To get more insight into what is going on, let us turn the mean-field update equations \eqref{eq:naivem} and \eqref{eq:tapm} for the mean magnetizations into code. But before we show a few plots, we need to know a bit more background about the model we are about to simulate. In &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Aguilera et al., 2020)&lt;/a&gt;, the authors derive a solution of the asymmetric version of the kinetic &lt;a href=&#34;https://en.wikipedia.org/wiki/Spin_glass#Sherrington%E2%80%93Kirkpatrick_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sherrington-Kirkpatrick mean-field spin-glass model&lt;/a&gt; using a generating functional or dynamical partition function approach to capture the distribution of trajectories. They consider the same kinetic Ising model as in Eq. \eqref{eq:pcond} but with an inverse temperature parameter $\beta$ in the exponentials:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta s_{i,t} h_{i,t}}}{2 \cosh \beta h_{i,t}}. \label{eq:pcondwithbeta}
\end{equation}&lt;/p&gt;
&lt;p&gt;For Gaussian couplings $J_{ij} \sim \mathcal{N}\left( J_{\mu} / N, J^{2}_{\sigma} / N\right)$ and uniformly distributed external magnetic fields $x_{i} \sim \mathcal{U}(-X_{0}, X_{0})$, they show the existence of a ferromagnetic phase transition. In particular for $X_{0}=0.5$, $J_{\mu}=1.0$, and $J_{\sigma}=0.1$, a phase transition happens when tuning $\beta$ to a critical value $\beta_{c} \approx 1.1108$.&lt;/p&gt;
&lt;h3 id=&#34;simulating-magnetization-trajectories&#34;&gt;Simulating magnetization trajectories&lt;/h3&gt;
&lt;p&gt;We first present a JAX implementation of the mean-field time evolution of the magnetizations according to the model described above. We use &lt;code&gt;jax.lax.scan&lt;/code&gt; to implement the time evolution and &lt;code&gt;jax.vmap&lt;/code&gt; to parallelize trajectories starting from a batch of initial magnetization configurations $\mathbf{m}_{0}$. For the second-order TAP equations, &lt;code&gt;jaxopt&lt;/code&gt;&amp;rsquo;s Anderson acceleration is used to find the fixed point magnetizations $\mathbf{m}_{t}$ given $\mathbf{m}_{t-1}$.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial

import jax
import jax.numpy as jnp

from jaxopt import AndersonAcceleration


def update_naive_mf(m0, _, x, J):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (22).&amp;quot;&amp;quot;&amp;quot;
    m1 = jnp.tanh(x + jnp.einsum(&amp;quot;i j, j -&amp;gt; i&amp;quot;, J, m0))
    return m1, m0


def update_tap_mf(m0, _, x, J):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (26).&amp;quot;&amp;quot;&amp;quot;

    def tap(m, _m0, _x, _J):
        return jnp.tanh(
            _x
            + jnp.einsum(&amp;quot;i j, j -&amp;gt; i&amp;quot;, _J, _m0)
            - m * jnp.einsum(&amp;quot;i j, j -&amp;gt; i&amp;quot;, _J**2, (1.0 - _m0**2))
        )

    m1 = (
        AndersonAcceleration(fixed_point_fun=tap, tol=1e-3, maxiter=10)
        .run(m0, m0, x, J)
        .params
    )
    return m1, m0


def time_evolution(m0, steps, update_fun):
    final_carry, stacked_outputs = jax.lax.scan(update_fun, init=m0, xs=steps)
    return final_carry, stacked_outputs


def init_params(key, N, beta, X0, J_mu, J_sigma):
    x_key, J_key = jax.random.split(key)
    x = jax.random.uniform(x_key, shape=(N,), minval=-beta * X0, maxval=beta * X0)
    J = beta * J_mu * N**-1 + beta * J_sigma * N**-0.5 * jax.random.normal(
        J_key, shape=(N, N)
    )
    return x, J


def simulate(
    key, m0, steps, beta, X0=0.5, J_mu=1.0, J_sigma=0.1, update_fun=update_tap_mf
):
    x, J = init_params(key, m0.shape[-1], beta, X0, J_mu, J_sigma)
    wrapped_time_evolution = partial(
        time_evolution,
        steps=steps,
        update_fun=partial(update_fun, x=x, J=J),
    )
    final_carry, stacked_outputs = jax.vmap(wrapped_time_evolution)(m0)
    return final_carry, stacked_outputs
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;naive-mean-field-vs-thouless-anderson-palmer-tap&#34;&gt;Naive mean-field vs. Thouless-Anderson-Palmer (TAP)&lt;/h3&gt;
&lt;p&gt;We fix the seed and randomly initialize model parameters $\mathbf{x}$ and $\mathbf{J}$ to simulate $N=512$ spins at the critical temperature $\beta_{c}$ for $t=128$ time steps starting from an all-ones initial state. We first consider the naive mean-field update step.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The left axis shows the individual magnetization trajectories for each spin plotted horizontally while the red line associated to the right axis describes the average of the magnetizations across all spins for each time step. We observe convergence to what looks like a &lt;em&gt;non-equilibrium / near-equilibrium steady state&lt;/em&gt; (NESS).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Comparing the naive first-order mean-field update equations to the second-order Thouless-Anderson-Palmer (TAP) ones, we observe lower values for the mean magnetization across all spins, which &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Aguilera et al., 2020)&lt;/a&gt; showed to be closer to ground truth values (not shown) obtained via sampling and averaging spin configurations.&lt;/p&gt;
&lt;h3 id=&#34;sampling-trajectories&#34;&gt;Sampling trajectories&lt;/h3&gt;
&lt;p&gt;Let us consider 100 randomly-initialized initial states and simulate their associated trajectories in three different model regimes: below the critical point ($\beta=\beta_c / 2 $), at the critical point ($\beta=\beta_c$), and above the critical point ($\beta=2 \beta_c$).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We observe that the trajectories of randomly-initialized initial states converge to identical final states in each regime. These final states map to a simple ferromagnetic Ising phase diagram, where a high-temperature disordered phase $\langle m_{i,t} \rangle \to 0$ (left) is separated from a low-temperature locally-ordered phase $\langle m_{i,t} \rangle \to \pm 1$ (right) by a critical point (center). The behavior around $\beta=\beta_{c}$ is pretty interesting: &lt;em&gt;the non-trivial steady state looks like an attractor implicitly encoded in the dynamics of the model&lt;/em&gt;. If we were to parameterize the couplings, we could train the system to act as an associative memory.&lt;/p&gt;
&lt;h3 id=&#34;sampling-model-parameters&#34;&gt;Sampling model parameters&lt;/h3&gt;
&lt;p&gt;We now go back to considering just a single trajectory since we just saw that trajectories seem to converge to the same final steady-state magnetizations for fixed model parameters. To get a feel for the variation of these values across different realizations of model parameters, we plot the absolute value&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; $| \langle m_{i} \rangle |$ of the final steady-state magnetizations across 100 samples of model parameters and a range of inverse temperatures. We are using JAX, so we can easily sample model parameters by &lt;code&gt;vmap&lt;/code&gt;&amp;lsquo;ing the random key fed into the &lt;code&gt;simulate&lt;/code&gt; function followed by another &lt;code&gt;vmap&lt;/code&gt; to sweep across $\beta$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;binary_plot_4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Every curve in the above plot describes the final steady-state value of the &amp;ldquo;order parameter&amp;rdquo; $| \langle m_{i} \rangle |$ for a fixed set of model parameters sweeping across $\beta$. We observe a greater spread of values near the critical point and hence an improved capacity to map input external fields to a range of output magnetizations. If we were to let the number of spins $N \to \infty$ and average over a large number of model parameter samples, the finite-size results above would probably transform into a sharp curve with zero magnetization below the critical point and a sudden non-zero magnetization emerging at the critical point.&lt;/p&gt;
&lt;h1 id=&#34;3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins&#34;&gt;3. Mean-field theory of asymmetric Ising models with vector spins&lt;/h1&gt;
&lt;p&gt;We now transpose the binary-spin results of the previous section to a setting where local spin degrees of freedom are $D$-dimensional vector spins restricted to wiggle around on $(D-1)$-dimensional spheres. We start by generalizing the conditional distribution Eq. \eqref{eq:pcondalt} to vector spins. Next, we motivate the limit of large vector dimension and derive first-order and second-order mean-field update equations for the mean magnetizations. We finish this section with a JAX implementation and some toy numerical experiments.&lt;/p&gt;
&lt;h2 id=&#34;31-vector-spins-distributions-on-hyperspheres&#34;&gt;3.1. Vector spins: distributions on hyperspheres&lt;/h2&gt;
&lt;img src=&#34;vector_spins.png&#34; alt=&#34;Random Ising model configuration with vector spins&#34; width=&#34;250px&#34;/&gt;
&lt;p&gt;A vector-spin equivalent of Eq. \eqref{eq:pcondalt} looks something like&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \mathbf{h}_{i,t}(\alpha)}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s}_{i,t} \; \mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \mathbf{h}_{i,t}(\alpha)} }, \label{eq:pcondaltvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;where we immediately included an inverse temperature $\beta$ like in Eq. \eqref{eq:pcondwithbeta}. A vector-spin equivalent of Eq. \eqref{eq:pcondalth} is&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{h}_{i,t}(\alpha) = (1-\alpha) \boldsymbol{\theta}_{i,t} + \alpha \left( \mathbf{x}_{i,t} + \sum_{j=1}^{N} J_{ij} \mathbf{s}_{j,t-1} \right) \equiv \boldsymbol{\theta}_{i,t} + \alpha \Delta \mathbf{h}_{i,t},  \label{eq:pcondalthvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $S_{D-1}(R) = \{ x \in \mathbb{R}^{D} : \lVert x \rVert = R \}$ denotes the $(D-1)$-dimensional sphere with radius $R$ embedded in $D$ dimensions. Let us focus on the distribution for a single site and drop all subscripts and dependencies for clarity:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }. \label{eq:pcondsinglesitevector}
\end{equation}&lt;/p&gt;
&lt;p&gt;The normalization constant in the denominator can be shown to be (see &lt;a href=&#34;#a1-vector-spin-distribution-normalization-constant&#34;&gt;Appendix A.1&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } \equiv Z(\beta, R, \lVert \mathbf{h}\rVert) \label{eq:partfun}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $I_{\nu}(z)$ denotes the modified Bessel function of the first kind and $\lVert \mathbf{h} \rVert = \sqrt{\mathbf{h} \cdot \mathbf{h}}$. Physically, we can think of this single-site distribution as measuring dot-product alignment to an effective external magnetic field $\mathbf{h}$ at inverse temperature $\beta$.&lt;/p&gt;
&lt;p&gt;If we consider spins living on the unit sphere $R=1$ as well as unit vectors $\mathbf{h}$, the distribution boils down to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;von Mises–Fisher distribution&lt;/a&gt; with mean direction $\boldsymbol{\mu} \equiv \mathbf{h}$ and &lt;a href=&#34;https://en.wikipedia.org/wiki/Concentration_parameter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;concentration parameter&lt;/a&gt; $\kappa \equiv \beta$. This distribution is unimodal for $\kappa &amp;gt; 0$ and can be derived from restricting an isotropic multivariate Gaussian to the unit hypersphere. The greater the value of $\kappa$ (the inverse temperature $\beta$), the higher the concentration of the distribution around the mean direction $\boldsymbol{\mu}$ (the more the spin tends to align to the effective external field $\mathbf{h}$). Though instead of a fixed parameter $\boldsymbol{\mu}$, we have a very funky parameter Eq. \eqref{eq:pcondalthvector} that depends on all other spins to spice things up.&lt;/p&gt;
&lt;h2 id=&#34;32-magnetizations-and-limit-of-large-vector-dimension&#34;&gt;3.2. Magnetizations and limit of large vector dimension&lt;/h2&gt;
&lt;p&gt;Before we derive mean-field approximations for the mean magnetizations of our vector-spin system, let us first consider the decoupled $\alpha \to 0$ limit of the distribution Eq. \eqref{eq:pcondaltvector},&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \boldsymbol{\theta}_{i,t}}}{Z_{i,t}\left(\beta, R, \lVert \boldsymbol{\theta}_{i,t} \rVert\right)},
\end{equation}&lt;/p&gt;
&lt;p&gt;and find an expression for its mean magnetizations. For every decoupled site, the mean magnetization can be shown to be (see &lt;a href=&#34;#a2-vector-spin-distribution-expected-value-first-moment&#34;&gt;Appendix A.2&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}_{i,t} = \frac{I_{D/2}(\beta R \lVert \boldsymbol{\theta}_{i,t} \rVert)}{I_{D/2 - 1}(\beta R \lVert \boldsymbol{\theta}_{i,t} \rVert)} \frac{R \boldsymbol{\theta}_{i,t}}{\lVert \boldsymbol{\theta}_{i,t} \rVert} \equiv \boldsymbol{\varphi} \left(\boldsymbol{\theta}_{i,t}\right), \label{eq:meanmagsbessels}
\end{equation}&lt;/p&gt;
&lt;p&gt;which plays the role of $m_{i,t} = \tanh \theta_{i,t}$ in the binary setting, see Eq. \eqref{eq:meanmagstanh}. Looking ahead at turning the above equation into code, we note that there exist &lt;a href=&#34;https://www.jstor.org/stable/2005830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;efficient algorithms&lt;/a&gt; to compute the ratio of modified Bessel functions of the first kind. We implement a fast JAX version in &lt;a href=&#34;#a4-ratio-of-modified-bessel-functions-of-the-first-kind&#34;&gt;Appendix A.4&lt;/a&gt; and show numerically how the ratio flattens out quickly for large values of the order $\nu = D/2 -1$, motivating some kind of large-order expansion.&lt;/p&gt;
&lt;p&gt;Remember that our goal is to make a connection to transformer neural networks. Since the vector dimension in dense transformer modules tends be somewhere between $\mathcal{O}(10^2)$ and $\mathcal{O}(10^5)$, it is not nonsensical to focus on the large vector dimension limit. A relevant uniform asymptotic expansion of the ratio of modified Bessel functions of the first kind is &lt;a href=&#34;https://link.springer.com/article/10.1007/BF02764812&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Kiefer &amp;amp; Weiss, 1972)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{I_{\nu+\alpha}(\nu x)}{I_{\nu}(\nu x)} = \left( \frac{x}{1+\sqrt{1+x^2}} \right)^{\alpha} \left( 1 - \frac{1+\alpha\sqrt{1+x^2}}{2(1+x^2)} \frac{\alpha}{\nu} + \mathcal{O}\left( \frac{1}{\nu^2} \right) \right)
\end{align}&lt;/p&gt;
&lt;p&gt;Indeed, if we choose to tie the radius $R$ of our little spins to their vector dimension $D$ via&lt;/p&gt;
&lt;p&gt;\begin{align}
\nu=D/2-1=R^2,
\end{align}&lt;/p&gt;
&lt;p&gt;we can apply the leading order of the asymptotic expansion for $\alpha=1$ to \eqref{eq:meanmagsbessels} to find&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}^{D \to \infty}_{i,t} \approx \frac{\beta}{1+\gamma( \lVert \boldsymbol{\theta}_{i,t} \rVert )} \boldsymbol{\theta}_{i,t} \equiv \boldsymbol{\varphi}^{D \to \infty} \left(\boldsymbol{\theta}_{i,t}\right). \label{eq:largedevmag}
\end{equation}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\gamma \left(\lVert \boldsymbol{\theta}_{i,t} \rVert\right) = \sqrt{1+\beta^2 \lVert \boldsymbol{\theta}_{i,t} \rVert^2 / R^2 },
\end{align}&lt;/p&gt;
&lt;p&gt;From here on, we will default to using the large-$D$ approximation because keeping track of (derivatives of) Bessel functions gets boring real quick. We refer to &lt;a href=&#34;#a5-general-case-partial-derivatives-with-respect-to-alpha&#34;&gt;Appendix A.5&lt;/a&gt; for some truly outrageous expressions pertaining to the general case valid for all $D&amp;gt;1$.&lt;/p&gt;
&lt;h2 id=&#34;33-first-order-naive-mean-field-approximation&#34;&gt;3.3. First-order naive mean-field approximation&lt;/h2&gt;
&lt;p&gt;All right, let&amp;rsquo;s go. Closely mimicking the binary case, we start from the following approximated marginal probability distribution&lt;/p&gt;
&lt;p&gt;\begin{equation}
P^{[t-1:t]}_{\alpha}( \mathbf{s}_{t} ) = \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ),
\end{equation}&lt;/p&gt;
&lt;p&gt;interpolating between $P^{[t-1:t]}_{\alpha=0}( \mathbf{s}_{t} ) = Q( \mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\alpha=1}( \mathbf{s}_{t} ) = P( \mathbf{s}_{t} )$. Our lazy integral notation $\int \mathrm{d} \mathbf{s}_{t}$ should be understood as $\int \prod_{i=1}^{N} \mathrm{d}^{D} \mathbf{s}_{i, t}$, i.e. integrating over all the little spins at a fixed time $t$. The estimated mean magnetizations are&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbf{m}_{i,t}(\alpha) &amp;amp;= \int \mathrm{d} \mathbf{s}_{t} \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; \mathbf{s}_{i,t} P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ) \nonumber\\
&amp;amp;= \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ).
\end{align}&lt;/p&gt;
&lt;p&gt;The first-order derivative with respect to $\alpha$ is then given by&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha)}{\partial\alpha} = \int &amp;amp;\mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \Biggl( \frac{\partial\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial\alpha} \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) \nonumber\\
&amp;amp;+ \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, \frac{\partial P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial\alpha} \Biggr) P( \mathbf{s}_{t-2} ), \label{eq:mitfirstorderalpha}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha} = \frac{\beta}{1+\gamma \left(\lVert \mathbf{h}_{i,t}(\alpha) \rVert\right)} \Delta \mathbf{h}_{i,t} - \frac{\beta}{R^2} \frac{ \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right) }{ \gamma \left(\lVert \mathbf{h}_{i,t}(\alpha) \rVert\right) } \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \label{eq:firstorderphialpha}
\end{align}&lt;/p&gt;
&lt;p&gt;Evaluating \eqref{eq:mitfirstorderalpha} at $\alpha=0$, the second term drops out because the first-order derivative of $P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )$ becomes independent of $\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)$ and $\int \mathrm{d} \mathbf{s}_{t-1} P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )=1$. We thus end up with&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} = \frac{\beta}{1+\gamma \left(\lVert \boldsymbol{\theta}_{i,t} \rVert\right)}\boldsymbol{v}_{i,t} - \frac{\beta}{R^2}\frac{\left( \mathbf{m}_{i,t} \cdot \boldsymbol{v}_{i,t} \right)}{\gamma \left(\lVert \boldsymbol{\theta}_{i,t} \rVert\right)} \mathbf{m}_{i,t} \label{eq:mfirstorderalphazero}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{v}_{i,t} = -\boldsymbol{\theta}_{i,t} + \mathbf{x}_{i,t} + \sum_{j=1}^{N} J_{ij} \mathbf{m}_{j,t-1} \label{eq:vmf}
\end{align}&lt;/p&gt;
&lt;p&gt;captures the result of integrating $\Delta \mathbf{h}_{i,t}$ over the spins $\mathbf{s}_{t-1}$. Following Eq. \eqref{eq:mftheta}, the first-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[ \alpha \frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} \right]_{\alpha=1} = \mathbf{0} + \left[ \mathcal{O}\left(\alpha^2\right)\right]_{\alpha=1},\label{eq:firstorderapproxreqs}
\end{equation}&lt;/p&gt;
&lt;p&gt;so that we are encouraged to set $\boldsymbol{v}_{i,t}=0$ and hence $\boldsymbol{\theta}^{*}_{i,t} = \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1}$, leading to the naive mean-field equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{ \mathbf{m}_{i,t} = \frac{\beta \left( \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \right)}{1+\sqrt{1+\beta^2 \lVert \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \rVert^2 / R^2 }} } \label{eq:naivemvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;Looking ahead at the transformer-module correspondence in &lt;a href=&#34;#4-a-family-of-transformer-like-modules&#34;&gt;Section 4&lt;/a&gt;, we squint our eyes and recognize a scaled sum of a residual connection and an attention term. No feed-forward terms though.&lt;/p&gt;
&lt;p&gt;Before moving on to the second-order approximation, let us end this section with an interesting observation about Eq. \eqref{eq:mfirstorderalphazero}. In &lt;a href=&#34;#a3-vector-spin-distribution-variance-second-moment&#34;&gt;Appendix A.3&lt;/a&gt;, we show that the variance matrix of a single spin in the large-$D$ limit equals a rank-1 perturbation of a diagonal matrix&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var} [ \mathbf{s}_{i,t} ] &amp;amp;= \frac{\mathbb{1}}{1+\gamma \left(\lVert \mathbf{h}_{i,t}(\alpha) \rVert\right)} - \frac{ \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \otimes \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) }{ R^2 \gamma \left(\lVert \mathbf{h}_{i,t}(\alpha) \rVert\right) }, \label{eq:spinvariance}
\end{align}&lt;/p&gt;
&lt;p&gt;Taking the $\alpha \to 0$ limit of the above expressions, we can reinterpret Eq. \eqref{eq:mfirstorderalphazero} as the matrix-vector multiplication of the decoupled spin&amp;rsquo;s variance matrix with $\boldsymbol{v}_{i,t}$,&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} = \beta \mathrm{Var} [ \mathbf{s}_{i,t} ] \boldsymbol{v}_{i,t}.
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;34-second-order-thouless-anderson-palmer-approximation&#34;&gt;3.4. Second-order Thouless-Anderson-Palmer approximation&lt;/h2&gt;
&lt;p&gt;Let us now try to find out whether going to the second-order approximation spits out additional Onsager feed-forward like correction terms in the update equations for the magnetizations.&lt;/p&gt;
&lt;p&gt;Again following Eq. \eqref{eq:mftheta}, the second-order approximation should satisfy&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[ \alpha \frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} \right]_{\alpha=1} + \left[ \frac{\alpha^2}{2} \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha^2}\right]_{\alpha=1} = \mathbf{0} + \left[ \mathcal{O}\left(\alpha^3\right)\right]_{\alpha=1}, \label{eq:secondorderconstraint}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the second-order derivative is given by&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha)}{\partial\alpha^2} = \int &amp;amp;\mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \Biggl( \frac{\partial^{2}\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial\alpha^2} \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) \nonumber\\
&amp;amp;+ 2\frac{\partial\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial\alpha} \, \frac{\partial P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial\alpha} \nonumber \\
&amp;amp;+ \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, \frac{\partial^{2} P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial\alpha^2} \Biggr) P( \mathbf{s}_{t-2} ). \label{eq:mhasecordder}
\end{align}&lt;/p&gt;
&lt;p&gt;Evaluated at $\alpha=0$, the third term in the expression above will drop out because the derivative becomes independent of $\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)$ and $\int \mathrm{d} \mathbf{s}_{t-1} P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )=1$.&lt;/p&gt;
&lt;p&gt;The first term in Eq. \eqref{eq:mhasecordder} can be shown to look something like&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha^2} = &amp;amp; \frac{\beta^2}{R^4} \frac{ 1+\gamma_{i,t}(\alpha) }{ \gamma_{i,t}(\alpha)^3 } \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right)^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta}{R^2} \frac{1}{\gamma_{i,t}(\alpha)} \left( \frac{\partial\boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha} \cdot \Delta \mathbf{h}_{i,t} \right) \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta}{R^2} \frac{1}{\gamma_{i,t}(\alpha)} \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))  \cdot \Delta \mathbf{h}_{i,t} \right) \frac{\partial\boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha} \nonumber \\
&amp;amp;- \frac{\beta^2}{R^2} \frac{1}{\gamma_{i,t}(\alpha)^2 + \gamma_{i,t}(\alpha) } \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right) \Delta \mathbf{h}_{i,t},
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\gamma_{i,t} (\alpha) \equiv \gamma\left( \lVert \mathbf{h}_{i,t}(\alpha) \rVert \right) = \sqrt{1+\beta^2 \lVert \mathbf{h}_{i,t}(\alpha) \rVert^2 / R^2 },
\end{align}&lt;/p&gt;
&lt;p&gt;which, after substituting the first-order derivative Eq. \eqref{eq:firstorderphialpha}, simplifies to&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))}{\partial\alpha^2} = &amp;amp; \frac{\beta^2}{R^4} \frac{ 1+3\gamma_{i,t}(\alpha) }{ \gamma_{i,t}(\alpha)^3 } \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \Delta \mathbf{h}_{i,t} \right)^2 \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta^2}{R^2} \frac{1}{\gamma_{i,t}(\alpha)^2 + \gamma_{i,t}(\alpha)} \left( \Delta \mathbf{h}_{i,t} \cdot \Delta \mathbf{h}_{i,t} \right) \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \nonumber \\
&amp;amp;- \frac{\beta^2}{R^2} \frac{2}{\gamma_{i,t}(\alpha)^2 + \gamma_{i,t}(\alpha)} \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha))  \cdot \Delta \mathbf{h}_{i,t} \right) \Delta \mathbf{h}_{i,t} . \label{eq:secondorderphialpha}
\end{align}&lt;/p&gt;
&lt;p&gt;The second term in Eq. \eqref{eq:mhasecordder} contains non-vanishing contributions in the $\alpha \to 0$ limit coming from the $\sum_{j=1}^{N} J_{ij} \mathbf{s}_{j, t-1}$ terms in $\Delta \mathbf{h}_{i,t}$. One can show that the surviving terms in the integrand are proportional to&lt;/p&gt;
&lt;p&gt;\begin{align}
\sum_{j} J_{ij} \Biggl( &amp;amp;\frac{2 \beta^2}{1+\gamma_{i,t}(\alpha)} \frac{\partial\mathbf{m}_{j, t-1}(\alpha)}{\partial\alpha} \nonumber \\
&amp;amp;- \frac{2 \beta^2}{R^2 \gamma_{i,t}(\alpha)} \left( \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \cdot \frac{\partial\mathbf{m}_{j, t-1}(\alpha)}{\partial\alpha} \right) \boldsymbol{\varphi}(\mathbf{h}_{i,t}(\alpha)) \Biggr),
\end{align}&lt;/p&gt;
&lt;p&gt;which we can ignore since they are $\mathcal{O}(\alpha)$ on their own, and thus $\mathcal{O}(\alpha^3)$ when multiplied with $\alpha^2$ in the second-order approximation.&lt;/p&gt;
&lt;p&gt;Before taking the $\alpha \to 0$ limit of whatever is left in Eq. \eqref{eq:mhasecordder}, we list a few useful tricks to make the evaluation easier. First of all, we use Eq. \eqref{eq:vmf} to introduce the following sneaky substitution&lt;/p&gt;
&lt;p&gt;\begin{align}
\Delta \mathbf{h}_{i,t} = -\boldsymbol{\theta}_{i,t} + \mathbf{x}_{i,t} + \sum_{j=1}^{N} J_{ij} \mathbf{s}_{j,t-1} = \boldsymbol{v}_{i,t} + \sum_{j=1}^{N} J_{ij} \left( \mathbf{s}_{j,t-1} - \mathbf{m}_{j,t-1} \right),
\end{align}&lt;/p&gt;
&lt;p&gt;which conveniently separates terms with fluctuating spin variables from magnetizations that can be pulled out of the integrals. Secondly, all terms that contain only one spin variable with a dependence looking like $\mathbf{s}_{j,t-1} - \mathbf{m}_{j,t-1}$ drop out because, schematically,&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbf{s}_{j,t-1} - \mathbf{m}_{j,t-1} \overset{\int \mathrm{d} \mathbf{s}_{t-1}}{\to} \boldsymbol{\varphi}(\mathbf{h}_{j,t}(\alpha)) - \mathbf{m}_{j,t-1} \overset{\alpha \to 0}{\to} \mathbf{0}.
\end{align}&lt;/p&gt;
&lt;p&gt;Thirdly, since the $\alpha \to 0$ limit decouples all spins $\mathbf{s}_{t-1}$, any term containing dot products $(\mathbf{s}_{j,t-1}-\mathbf{m}_{j,t-1}) \cdot (\mathbf{s}_{k,t-1}-\mathbf{m}_{k,t-1})$ of two spin variables is zero for $j \neq k$ and equal to $R^2 - \mathbf{m}^2_{j,t-1}$ for $j=k$. We will also encounter terms containing (tensor contractions with) outer products $(\mathbf{s}_{j,t-1}-\mathbf{m}_{j,t-1}) \otimes (\mathbf{s}_{k,t-1}-\mathbf{m}_{k,t-1})$, which we can think of as projection operators. For $j \neq k$, these and similar terms again evaluate to zero, while, for $j=k$, we get the variance contributions we mentioned previously in Eq. \eqref{eq:spinvariance} at the end of the previous section.&lt;/p&gt;
&lt;p&gt;Finally, we take the $\alpha \to 0$ limit of Eq. \eqref{eq:mhasecordder} only to end up with the following mess:&lt;/p&gt;
&lt;p&gt;\begin{align}
&amp;amp;\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha^2} = \label{eq:secondordercorrections} \\
\end{align}
\begin{align}
&amp;amp;\hspace{-1em}\frac{\beta^2}{R^4} \frac{1+3\gamma_{i,t}(0)}{\gamma_{i,t}(0)^3} \left( \left( \mathbf{m}_{i,t} \cdot \mathbf{v}_{i,t} \right)^2 + \sum_{j} J_{ij}^2 \left( \frac{\mathbf{m}_{i,t}^2}{1+\gamma_{i,t-1}(0)} - \frac{\left(\mathbf{m}_{i,t}\cdot\mathbf{m}_{j,t-1}\right)^2}{R^2 \gamma_{i,t-1}(0)} \right) \right) \mathbf{m}_{i,t}  \nonumber \\
&amp;amp;\hspace{-1em}- \frac{\beta^2}{R^2} \frac{1}{\gamma_{i,t}^2 (0) + \gamma_{i,t}(0)} \left( \mathbf{v}_{i,t}^2 + \sum_{j} J_{ij}^2 \left( R^2 - \mathbf{m}_{j,t-1}^2 \right) \right) \mathbf{m}_{i,t} \nonumber \\
&amp;amp;\hspace{-1em}- \frac{\beta^2}{R^2} \frac{2}{\gamma_{i,t}^2 (0) + \gamma_{i,t}(0)} \Biggr( \mathbf{v}_{i,t} \otimes \mathbf{v}_{i,t} + \sum_{j} J_{ij}^2 \left( \frac{\mathbb{1}}{1+\gamma_{i,t-1}(0)} - \frac{\mathbf{m}_{j,t-1}\otimes\mathbf{m}_{j,t-1}}{R^2 \gamma_{i,t-1}(0)} \right) \Biggr) \mathbf{m}_{i,t} \nonumber
\end{align}&lt;/p&gt;
&lt;p&gt;At this point, it is too late. We should have remembered that the second-order approximation lives in the neighborhood of the first-order approximation. We probably ended up doing too much work by taking terms into account that are of higher order in $\alpha$. We can always drop terms later on if it turns out they are neglible at $\mathcal{O}(\alpha^2)$.&lt;/p&gt;
&lt;p&gt;To get to the second-order mean-field equations for the magnetizations, we have to solve Eq. \eqref{eq:secondorderconstraint} for the optimal parameters $\boldsymbol{\theta}^{*}_{i,t}$, i.e.,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} + \frac{1}{2} \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha^2} = \mathbf{0} + \mathcal{O}\left(\alpha^3\right).
\end{equation}&lt;/p&gt;
&lt;p&gt;Let us substitute $\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha}$ from Eq. \eqref{eq:mfirstorderalphazero} but keep $\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha^2}$ for generality,&lt;/p&gt;
&lt;p&gt;\begin{align}
\beta \left( \frac{\mathbb{1}}{1+\gamma_{i,t}(0)} - \frac{\mathbf{m}_{i,t}\otimes\mathbf{m}_{i,t}}{R^2 \gamma_{i,t}(0)} \right) \mathbf{v}_{i,t} + \frac{1}{2} \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha^2} = \mathbf{0} + \mathcal{O}\left(\alpha^3\right),
\end{align}&lt;/p&gt;
&lt;p&gt;so that we can then isolate $\boldsymbol{\theta}_{i,t}$ in $\mathbf{v}_{i,t}$ to find&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\theta}_{i,t} = \mathbf{x}_{i,t} &amp;amp;+ \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \nonumber \\
&amp;amp;+ \frac{1+\gamma_{i,t}(0)}{2\beta} \left( \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha^2} + \frac{\mathbf{m}_{i,t} \cdot \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha^2}}{\frac{R^2 \gamma_{i,t}(0)}{1+\gamma_{i,t}(0)} - \mathbf{m}_{i,t}^2} \mathbf{m}_{i,t} \right),\label{eq:ftheta}
\end{align}&lt;/p&gt;
&lt;p&gt;where we have used the &lt;a href=&#34;https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sherman–Morrison formula&lt;/a&gt; to compute the inverse of the variance matrix. Since the expression on the right-hand side &lt;em&gt;also&lt;/em&gt; depends on $\boldsymbol{\theta}_{i,t}$, we seem to have stumbled upon a set of fixed-point equations which we should solve for $\boldsymbol{\theta}^{*}_{i,t}$,&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\theta}_{i,t} = \mathbf{f} (\boldsymbol{\theta}_{i,t}, \mathbf{x}_{i,t}, \mathbf{m}_{i,t}, \mathbf{m}_{t-1}), \label{eq:thetafp}
\end{align}&lt;/p&gt;
&lt;p&gt;where the function $\mathbf{f}$ is given by the right-hand side of Eq. \eqref{eq:ftheta}. The second-order mean-field equations then become &lt;em&gt;yet another&lt;/em&gt; set of fixed-point equations&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}_{i,t} = \boldsymbol{\varphi} \left(\boldsymbol{\theta}^{*}_{i,t}(\mathbf{x}_{i,t}, \mathbf{m}_{i,t}, \mathbf{m}_{t-1})\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;because of the dependence of $\boldsymbol{\theta}^{*}_{i,t}$ on $\mathbf{m}_{i,t}$. Similar to the binary TAP approximation Eq. \eqref{eq:tapm}, this dependency suggests that we should solve for fixed-point magnetizations $\mathbf{m}^{*}_{i,t}$. However, in contrast to the binary case, the dependence here is &lt;em&gt;implicit&lt;/em&gt; since $\boldsymbol{\theta}^{*}_{i,t}$ is itself obtained from solving fixed-point equations Eq. \eqref{eq:thetafp}, which, in turn, also depend on $\mathbf{m}_{i,t}$.&lt;/p&gt;
&lt;p&gt;The problem setup looks like a &lt;a href=&#34;https://en.wikipedia.org/wiki/Bilevel_optimization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bi-level optimization problem&lt;/a&gt;, where the solutions to the inner-level fixed-point equations are fed as parameters to the outer-level fixed-point equations. Because of the hierarchical relationship and the implicit dependence of the outer solution on the inner problem&amp;rsquo;s parameters, bi-level optimization can be potentially computationally demanding and unstable. Let us try to sidestep this dreadfulness by writing all instances of $\boldsymbol{\theta}_{i,t}$ in Eq. \eqref{eq:ftheta} in terms of $\mathbf{m}_{i,t}$ by inverting Eq \eqref{eq:largedevmag} so that, for $\mathbf{m}^2_{i,t} &amp;lt; R^2$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{\theta}_{i,t} = \frac{2 R^2}{\beta \left( R^2 - \mathbf{m}^2_{i,t} \right)} \mathbf{m}_{i,t},\label{eq:invphi}
\end{equation}&lt;/p&gt;
&lt;p&gt;leading to a set of fixed-point equations in terms of only $\mathbf{m}_{i,t}$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boxed{\mathbf{m}_{i,t} = \boldsymbol{\varphi} \left( \mathbf{f} (\mathbf{x}_{i,t}, \mathbf{m}_{i,t}, \mathbf{m}_{t-1})\right) } \label{eq:tapmvector}
\end{equation}&lt;/p&gt;
&lt;p&gt;Looking ahead at the transformer-module correspondence in &lt;a href=&#34;#4-a-family-of-transformer-like-modules&#34;&gt;Section 4&lt;/a&gt;, we recognize a scaled sum of a residual connection, an attention term, and a self-consistent expression in terms of magnetizations and couplings taking on the role of the feed-forward network. Interestingly, these second-order correction terms require &lt;em&gt;no additional free parameters&lt;/em&gt; since they are &lt;em&gt;fully determined by the mean-field structure&lt;/em&gt; of the underlying spin model.&lt;/p&gt;
&lt;h2 id=&#34;35-a-simple-jax-implementation&#34;&gt;3.5. A simple JAX implementation&lt;/h2&gt;
&lt;p&gt;We now turn to a JAX implementation of the mean-field time evolution of the magnetizations according to the vector-spin model introduced in the previous sections. Compared to the binary-spin simulations of &lt;a href=&#34;#25-a-simple-jax-implementation&#34;&gt;Section 2.5&lt;/a&gt;, we will not attempt to precisely tune the vector-spin model since computing its critical temperature and quirky phase-diagram properties is well beyond the scope of this work. We will instead take an empirical approach and play around with a numerical implementation to figure out what works. Along the way, we provide some physical intuition.&lt;/p&gt;
&lt;h3 id=&#34;simulating-magnetization-trajectories-1&#34;&gt;Simulating magnetization trajectories&lt;/h3&gt;
&lt;p&gt;The JAX reference implementation looks very similar to the binary-spin case. Essentially, we have to keep track of an additional vector dimension and replace the update equations with the vector equivalents introduced in the previous sections. We deliberately do not fiddle with the hyperparameters of the fixed-point solver &lt;code&gt;AndersonAcceleration&lt;/code&gt; to ensure robustness of exploratory results.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial

import jax
import jax.numpy as jnp
from jaxopt import AndersonAcceleration


def _gamma(x, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (39).&amp;quot;&amp;quot;&amp;quot;
    return jnp.sqrt(1 + beta**2 * jnp.sum(x**2, axis=-1, keepdims=True) / R**2)


def _phi(theta, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (38).&amp;quot;&amp;quot;&amp;quot;
    return beta / (1 + _gamma(theta, beta, R)) * theta


def update_naive_mf(m0, _, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (47).&amp;quot;&amp;quot;&amp;quot;
    theta = x + jnp.einsum(&amp;quot;i j, j d -&amp;gt; i d&amp;quot;, J, m0)
    m1 = _phi(theta, beta, R)
    return m1, m0


def _inv_phi(m, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (64).&amp;quot;&amp;quot;&amp;quot;
    return 2 * R**2 / (beta * (R**2 - jnp.sum(m**2, axis=-1, keepdims=True))) * m


def _d2_m_d_alpha_2(m1, m0, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (58).&amp;quot;&amp;quot;&amp;quot;
    g0 = _gamma(_inv_phi(m0, beta, R), beta, R)
    g1 = _gamma(_inv_phi(m1, beta, R), beta, R)
    v = -_inv_phi(m1, beta, R) + x + jnp.einsum(&amp;quot;i j, j d -&amp;gt; i d&amp;quot;, J, m0)

    return (
        (beta**2 * (1 + 3 * g1))
        / (R**4 * g1**3)
        * (
            jnp.einsum(&amp;quot;i d, i d -&amp;gt; i&amp;quot;, m1, v)[:, None] ** 2
            + jnp.einsum(
                &amp;quot;i j, i d -&amp;gt; i d&amp;quot;,
                J**2,
                jnp.sum(m1**2, axis=-1, keepdims=True),
            )
            / (1 + g0)
            - jnp.einsum(
                &amp;quot;i j, i d, j d, i e, j e -&amp;gt; i&amp;quot;,
                J**2,
                m1,
                m0,
                m1,
                m0,
            )[:, None]
            / (R**2 * g0)
        )
        * m1
        - (beta**2)
        / (R**2 * (g1**2 + g1))
        * (
            jnp.sum(v**2, axis=-1, keepdims=True)
            + jnp.einsum(
                &amp;quot;i j, j -&amp;gt; i&amp;quot;,
                J**2,
                R**2 - jnp.sum(m0**2, axis=-1),
            )[:, None]
        )
        * m1
        - 2.0
        * beta**2
        / (R**2 * (g1**2 + g1))
        * (
            jnp.einsum(&amp;quot;i d, i d, i f -&amp;gt; i f&amp;quot;, v, m1, v)
            + jnp.einsum(&amp;quot;i j, i d -&amp;gt; i d&amp;quot;, J**2, m1 / (1 + g0))
            - jnp.einsum(
                &amp;quot;i j, i d, j d, j f -&amp;gt; i f&amp;quot;,
                J**2,
                m1,
                m0,
                m0,
            )
            / (R**2 * g0)
        )
    )


def _f(m1, m0, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (61).&amp;quot;&amp;quot;&amp;quot;
    g1 = _gamma(_inv_phi(m1, beta, R), beta, R)
    d2_m_d_alpha_2 = _d2_m_d_alpha_2(m1, m0, x, J, beta, R)

    ff = (
        (1 + g1)
        / (2 * beta)
        * (
            d2_m_d_alpha_2
            + (
                jnp.einsum(&amp;quot;i d, i d -&amp;gt; i&amp;quot;, m1, d2_m_d_alpha_2)[:, None]
                / ((R**2 * g1) / (1 + g1) - jnp.sum(m1**2, axis=-1, keepdims=True))
                * m1
            )
        )
    )
    return x + jnp.einsum(&amp;quot;i j, j d -&amp;gt; i d&amp;quot;, J, m0) + ff


def update_tap_mf(m0, _, x, J, beta, R):
    &amp;quot;&amp;quot;&amp;quot;See Eq. (65).&amp;quot;&amp;quot;&amp;quot;

    def tap(m1, _m0, _x, _J, _beta, _R):
        return _phi(_f(m1, _m0, _x, _J, _beta, _R), _beta, _R)

    m1 = (
        AndersonAcceleration(fixed_point_fun=tap, tol=1e-3, maxiter=100)
        .run(_phi(x + J @ m0, beta, R), m0, x, J, beta, R)
        .params
    )

    return m1, m0


def time_evolution(m0, steps, update_fun):
    final_carry, stacked_outputs = jax.lax.scan(update_fun, init=m0, xs=steps)
    return final_carry, stacked_outputs


def simulate(x, J, m0, steps, beta, R, update_fun=update_tap_mf):
    wrapped_time_evolution = partial(
        time_evolution,
        steps=steps,
        update_fun=partial(update_fun, x=x, J=J, beta=beta, R=R),
    )
    final_carry, stacked_outputs = jax.vmap(wrapped_time_evolution)(m0)
    return final_carry, stacked_outputs
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;playing-with-parameter-scales-an-exploration&#34;&gt;Playing with parameter scales: an exploration&lt;/h3&gt;
&lt;p&gt;To get a feel for the complexity, let us visualize a $N=64$ sample of a coupling matrix $\mathbf{J} \in \mathbb{R}^{N \times N}$ drawn from $\mathcal{N}\left( 0, 1/N \right)$ using a visually appealing yet utterly pointless ball-of-yarn plot:&lt;/p&gt;
&lt;img src=&#34;vector_plot_1.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;We randomly initialize the external magnetic fields $\mathbf{x} \in \mathbb{R}^{N \times D}$ and coupling matrix $\mathbf{J} \in \mathbb{R}^{N \times N}$ by drawing from respectively $\mathcal{N}\left( 0, 1\right)$  and $\mathcal{N}\left( 0, 1/N \right)$ and simulate $N=1024$ $(D=512)-$dimensional vector spins at inverse temperature $\beta=1.0$ for $t=20$ time steps starting from an intial state $\mathbf{m}_{0} \in \mathbb{R}^{N \times D}$ of all-ones vectors. We choose to normalize all $\mathbf{x}$ vectors to lie on the spherical shell at radius $R$, so that $\mathbf{x}_{i} \to R \mathbf{x}_{i} / \lVert\mathbf{x}_{i}\rVert$. We apply the same external magnetic fields at all time steps ($\mathbf{x}_{t} \equiv \mathbf{x}$, $\forall t \geq 0$) so that the probing of the system is time-independent and relentless.&lt;/p&gt;
&lt;p&gt;We first consider the first-order naive mean-field update equations. To visualize a set of vectors evolving in time, we track their directionalities with respect to reference states using cosine similarities and their magnitudes using Euclidean norms.&lt;/p&gt;
&lt;img src=&#34;vector_plot_2.png&#34; width=&#34;250px&#34;/&gt;
&lt;p&gt;The top plot shows the cosine-similarity alignments of individual magnetization trajectories $\mathbf{m}_{i,t}$ compared to respectively $\mathbf{m}_{i,t-1}$ (green, magnetizations at previous time step to track convergence), $\mathbf{m}_{0}$ (yellow, magnetizations at initial time step to track drift from initial conditions), and $\mathbf{x}_{i}$ (blue, time-independent external magnetic fields to track alignment with the &amp;ldquo;residual stream&amp;rdquo;). The bottom plot tracks the evolution of the norms of $\mathbf{m}_{i,t}$ during time evolution. From the tracked metrics, we observe convergence to what looks like a &lt;em&gt;non-equilibrium / near-equilibrium steady state&lt;/em&gt; (NESS) with magnetizations remaining dynamically stable at the mean-field level.&lt;/p&gt;
&lt;p&gt;To compare the naive first-order mean-field update equations to the second-order Thouless-Anderson-Palmer (TAP) ones, we plot the mean magnetization trajectories across all sites and add shading to denote the spread of maximum and minimum values.&lt;/p&gt;
&lt;img src=&#34;vector_plot_3.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;We observe that the final TAP magnetizations are slightly different for our particular choice of parameters. The Onsager correction term seems to account for at least some correlations, lowering the local effective mean field and hence the magnitude of the magnetizations. If we lower the temperature to $\beta = 2.0$ while keeping all other parameters fixed, the difference becomes more pronounced:&lt;/p&gt;
&lt;img src=&#34;vector_plot_4.png&#34; width=&#34;400px&#34;/&gt;
&lt;p&gt;Lowering the temperature further while keeping all other parameters fixed starts leading to convergence issues for the TAP equations. If we go back to $\beta=1.0$ but (1) increase the random interaction strengths by doubling the elements of the coupling matrix and (2) reduce the influence of the random external magnetic fields by normalizing all $\mathbf{x}$ vectors to lie on the unit sphere, we end up in a regime where we observe that the naive mean-field equations have trouble converging whereas the TAP magnetizations quickly settle into a small-norm fixed-point solution:&lt;/p&gt;
&lt;img src=&#34;vector_plot_5.png&#34; width=&#34;400px&#34;/&gt;
&lt;h3 id=&#34;playing-with-parameter-scales-an-explanation&#34;&gt;Playing with parameter scales: an explanation&lt;/h3&gt;
&lt;p&gt;To better understand the behavior of the system, we focus on the inverse temperature $\beta$, the magnitudes $\lVert\mathbf{x}_{i,t}\rVert$ of the external magnetic fields, the scale of the coupling matrix elements $J_{ij}$, and the vector-spin radius $R=\sqrt{D/2-1}$. The latter is fixed for fixed dimension $D$ and provides a natural length scale. In spin-glas mean-field theory, the random coupling matrix is usually chosen to have a variance of $1/N$ to ensure the existence of a proper thermodynamic limit. The magnitudes of the external magnetic fields determine to what extent the vector spins will try to align with their imposed external environment or yield to the influence of their neighbours. The relation between the scales of the couplings and the fields should be such that meaningful competition between the external magnetic fields and the intrinsic spin-spin interactions can occur. Finally, the system&amp;rsquo;s overall behavior is further governed by the thermal noise introduced via the inverse temperature $\beta$.&lt;/p&gt;
&lt;p&gt;Revisiting the magnetization equation Eq. \eqref{eq:largedevmag},&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{m}_{i,t} = \boldsymbol{\varphi} \left(\boldsymbol{\theta}_{i,t}\right) = \frac{\beta}{1+\sqrt{1+\beta^2 \lVert \boldsymbol{\theta}_{i,t} \rVert^2 / R^2 }} \boldsymbol{\theta}_{i,t},
\end{equation}&lt;/p&gt;
&lt;p&gt;we observe that the infinite-temperature limit $\beta \to 0$ pushes the magnitude of the magnitization to $0$ whereas the zero-temperature limit $\beta \to \infty$ snaps to the spherical shell at radius $R$. We can plot the norm of this equation for different values of $\beta$ as a function of $\lVert\boldsymbol{\theta}\rVert$ in $D=512$ dimensions below. The dashed horizontal and vertical lines indicate the value of $R=\sqrt{D/2-1}\approx 15.9687$.&lt;/p&gt;
&lt;img src=&#34;vector_plot_6.png&#34; width=&#34;350px&#34;/&gt;
&lt;p&gt;This plot partly explains why the TAP equations start showing convergence issues at lower temperatures. Large values of $\beta$ push the norm of the magnetizations towards $R$, but that in turn leads to $\boldsymbol{\theta}$ blowing up because of the $R^2-\mathbf{m}^2_{i,t}$ factors in the denominators of Eq. \eqref{eq:ftheta} and Eq. \eqref{eq:invphi}. This is no surprise since the Plefka expansion is in fact a high-temperature expansion. Indeed, if we write out the mean-field update equations, we find that the first-order terms scale as $\beta$ and the second-order terms as $\beta^2$. Additionally, we know from mean-field theory of binary spin glasses that the TAP equations break down when crossing the so-called de Almeida-Thouless line (AT line) in the $(\beta, x)$ phase diagram. Assuming an &lt;a href=&#34;https://arxiv.org/abs/1003.5599&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;equivalent transition exists in for vector-spin glasses&lt;/a&gt;, it might be worth rederiving the Onsager term like was done for binary spins in &lt;a href=&#34;https://arxiv.org/abs/1509.01229&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Opper et al., 2015)&lt;/a&gt; to make sure its time indices are more geared towards convergence. But even then we would still not be able to cross the AT line and find mean-field solutions at lower temperatures.&lt;/p&gt;
&lt;p&gt;But we have to ask ourselves whether we actually care about this low-temperature failure mode for our purposes. Do we want a spin-transformer module to inhabit a complex spin-glass phase full of local minima containing frozen disordered spins that cannot respond to external magnetic fields? No. We would like our system to be able to fluidly and adaptively respond to its environment.&lt;/p&gt;
&lt;h1 id=&#34;4-spin-transformer-modules-a-family-of-transformer-like-modules&#34;&gt;4. Spin-transformer modules: a family of transformer-like modules&lt;/h1&gt;
&lt;p&gt;In this final section, we propose a physics-inspired class of transformer modules based on the mean-field update equations for the vector-spin magnetizations derived in the previous section. We highlight conceptual similarities, physical interpretations, and potential benefits of exploiting spin-model structure to reduce parameter count.&lt;/p&gt;
&lt;h2 id=&#34;41-connecting-the-dots&#34;&gt;4.1. Connecting the dots&lt;/h2&gt;
&lt;p&gt;Following &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms (2021)&lt;/a&gt; and &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/#5-why-dont-we-just-probe-a-vector-spin-system-with-data&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers Are Secretly Collectives of Spin Systems (2021)&lt;/a&gt;, we interpret a transformer module as a differentiable vector-spin system that is driven by data and whose collective behavior can be shaped through training. Intuitively, there is little difference here compared to the work mentioned above: we still probe a spin system and observe its response. But, technically and conceptually, the shift to dynamical mean-field expressions enables us to solidify the correspondence by moving past symmetric coupling matrices and equilibrium free energies.&lt;/p&gt;
&lt;p&gt;We define a &lt;em&gt;spin-transformer module&lt;/em&gt; as a wrapper around a vector-spin model where module inputs $\mathbf{x} \in \mathbb{R}^{N \times D}$ get routed to external magnetic fields. Inside the module, we evolve a set of initial magnetizations in time using either the first-order (Eq. \eqref{eq:naivemvector}) or the second-order (Eq. \eqref{eq:tapmvector}) mean-field update equations. Only the second-order update equations exhibit feed-forward-like corrections. We choose to relentlessly apply the same external magnetic fields at all time steps ($\mathbf{x}_{t} \equiv \mathbf{x}$, $\forall t \geq 0$) and construct input-dependent couplings using the row-stochastic attention matrix,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbf{J}(\mathbf{x}) = \mathrm{softmax}\left( \frac{\boldsymbol{x} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{x}^{T}}{\sqrt{D}} \right). \label{eq:softmaxcouplings}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\boldsymbol{W}_{\boldsymbol{Q}}$ and $\boldsymbol{W}_{\boldsymbol{K}}$ denote linear query- and key-mappings. Adding bias terms to these linear transformations would introduce intrinsic interactions between the spins that persist even in the absence of the external magnetic fields. Essentially, we recognize the softmax attention matrix as a parametrized flavor of the (asymmetric) coupling matrix of a vector-spin model. The external magnetic fields thus not only affect the vector spins directly, but also indirectly by altering the interaction strengths between them. This setup leads to a highly adaptive system where the interaction landscape itself is dynamically shaped by the inputs.&lt;/p&gt;
&lt;img src=&#34;arch_comparison.png&#34; alt=&#34;Comparison between vanilla transformer module and spin-transformer module&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;What does the spin-transformer module return? The within-module time evolution is said to converge when the mean magnetizations collectively reach some kind of &lt;em&gt;non-equilibrium / near-equilibrium steady-state&lt;/em&gt; (NESS), which is not guaranteed a priori and requires us to make sure the couplings, inverse temperature, and normalizations are sensibly chosen. In fact, it might very well be the case that, for the parameter regimes we would want to consider, the behavior of the vector-spin model is quite equilibrium-like, and this is probably what we want to aim for anyway given that oscillations, instabilities, and divergences are always lurking close by in the perilous phase spaces of these systems. If the within-module time evolution converges, we return the magnetizations $\mathbf{m}_{\mathrm{NESS}} \in \mathbb{R}^{N \times D}$ as module outputs. Instead of time evolving for a number of steps until convergence, we could also try hunting for the NESS directly by assuming it exists and solving for it as if it were a fixed point of the time evolution.&lt;/p&gt;
&lt;p&gt;To wrap up this section, we list a few conceptual similarities and features below to close the gap between vector-spin models and transformer modules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Attention heads:&lt;/strong&gt; Multiple attention heads can be implemented by embedding $N_{h}$ coupling matrices into a head-block-diagonal coupling tensor. Effectively, this operation stacks $N_{h}$ smaller-dimensional spin models where each submodel processes a disjoint $D_{h}-$dimensional piece of the full $D-$dimensional vector space. Mixing between subspaces can occur because (1) each individual coupling matrix is still constructed from query and key mappings $\mathbb{R}^{D} \to \mathbb{R}^{N_{h} \times D_{h}}$ acting on the full input space, and (2) the dot products in the second-order correction terms Eq. \eqref{eq:secondordercorrections} naturally mix channels.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Causal masks:&lt;/strong&gt; Since we identify the attention matrix with the spin model&amp;rsquo;s couplings, autoregressive modeling can be done by applying the appropriate triangular mask to the coupling matrix instead. The causal structure is preserved during the within-module time evolution. More generally, we expect any kind of masking that can be done on the level of the attention matrix to transfer to the coupling matrix.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cross-attention:&lt;/strong&gt; The framework described above implements self-attention by constructing both queries and keys from the inputs $x$ according to Eq. \eqref{eq:softmaxcouplings}. Decoder layers in encoder-decoder models, however, rely on cross-attention, where keys (and values) from the encoder output are sent to the decoder input as context. We can accommodate this scenario by feeding the spin-transformer module an additional set of context vectors $\mathbf{c}$ to build the coupling matrix, i.e.,&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\mathbf{J}(\mathbf{x}, \mathbf{c}) = \mathrm{softmax}\left( \frac{\boldsymbol{x} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{c}^{T}}{\sqrt{D}} \right). \label{eq:crosssoftmaxcouplings}
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normalization:&lt;/strong&gt; A flavor of &lt;a href=&#34;https://github.com/lucidrains/x-transformers#root-mean-square-layer-normalization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Root Mean Square Layer Normalization&lt;/a&gt; (RMSNorm) naturally appears in expression Eq. \eqref{eq:largedevmag} for the magnetization in the limit of large vector dimension as well as in all the mean-field update equations derived from it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Queries, keys, and values:&lt;/strong&gt; The &lt;em&gt;queries&lt;/em&gt; and &lt;em&gt;keys&lt;/em&gt; are used to define the interactions between the spins from the external magnetic fields via Eq. \eqref{eq:softmaxcouplings}. In a sense, these linear transformations remain quite arbitrary since our framework is agnostic to the nature of the coupling matrix. But the &lt;em&gt;values&lt;/em&gt; do have an interpretation as the magnetizations $\mathbf{m}_{t-1}$ at the previous time step, or, in case of convergence, the steady-state magnetizations $\mathbf{m}^{\mathrm{NESS}}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;42-fast--and-slow-moving-parameters&#34;&gt;4.2. Fast- and slow-moving parameters&lt;/h2&gt;
&lt;p&gt;We now provide some additional physical intuition. As mentioned ad nauseam in &lt;a href=&#34;https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms (2021)&lt;/a&gt;  and &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers Are Secretly Collectives of Spin Systems (2021)&lt;/a&gt;, each example in a batch of sequential data can be thought of as probing a spin-transformer module in a particular way. The response of the many-body system depends on the context provided by the applied external fields. We can tune the collective response behavior by parametrizing the couplings and making sure the whole probe-response stack is differentiable.&lt;/p&gt;
&lt;img src=&#34;spin_model_transformer_module.png&#34; alt=&#34;Focus on a spin-transformer module in a stack of layers&#34; width=&#34;450px&#34;/&gt;
&lt;p&gt;Physically, the &lt;em&gt;fast-moving&lt;/em&gt; parameterized couplings $\mathbf{J}(\mathbf{x})$ are determined by the &lt;em&gt;fast-moving&lt;/em&gt; parameterized external fields $\mathbf{x}$, which, in a stack of transformer modules, depend on the magnetizations of the previous layer and ultimately on the input data. The external fields act as an environment of contextual patterns that gets transformed instantly into the values of the coupling matrix, effectively inducing some kind of state of quenched disorder. The &lt;em&gt;slow-moving&lt;/em&gt; parameters are those receiving gradient updates during training, e.g., the query-key matrices in the softmax couplings. On the level of a spin-transformer module, training can be understood as &lt;em&gt;shaping the input-dependent distribution of coupling parameters&lt;/em&gt; by amassing information from a huge amount of quenched disorder realizations, sculpting a spin glass with data.&lt;/p&gt;
&lt;h2 id=&#34;43-a-simple-jax-implementation&#34;&gt;4.3. A simple JAX implementation&lt;/h2&gt;
&lt;p&gt;Let us wrap up this post with some code showing how one could implement a spin-transformer module based on the recipe described above. We choose to normalize input vectors to have norm $R$, and, because of this choice, we set the softmax temperature in the couplings Eq. \eqref{eq:softmaxcouplings} to $1$ instead of $\sqrt{D}$ to make sure the scale of the matrix elements is similar as in scaled dot-product attention. As we have seen in &lt;a href=&#34;#35-a-simple-jax-implementation&#34;&gt;Section 3.5&lt;/a&gt;, lowering the norm of the input vectors decreases the strength the applied magnetic fields and increases the influence of the spin-spin interactions. Other normalization conventions might turn out to work better in actual training scenarios. Additionally, since different flavors of mean-field approximations lead to different update equations for the magnetizations, we want to stress that the approach we took in this post is just one possible option, which might not be the most useful one in practice.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;We use &lt;a href=&#34;https://github.com/patrick-kidger/equinox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;equinox&lt;/code&gt;&lt;/a&gt; to implement our neural network modules. We could replace the fixed-step &lt;code&gt;lax.scan&lt;/code&gt; time evolution of &lt;a href=&#34;#35-a-simple-jax-implementation&#34;&gt;Section 3.5&lt;/a&gt; with an &lt;code&gt;equinox.internal.while_loop&lt;/code&gt; to implement early-stopping when convergence occurs in a way that supports reverse-mode autodifferentiation. But then we would have to make sure to stop gradients so that only the values of the final iteration, corresponding to the steady-state magnetizations $\mathbf{m}^{\mathrm{NESS}}$, contribute to the gradient computation. To make things easier in the implementation below, we are going to assume the NESS exists and solve for it as if it were a fixed point of the time evolution. Implicit differentation of the fixed-point solver then takes care of the (near-)equilibrium gradients. So we only need the following function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def vector_tap_fp(m0, x, J, beta, R, tol: float = 1e-3, maxiter: int = 100):
    &amp;quot;&amp;quot;&amp;quot;Find fixed-point vector magnetizations of second-order mean-field update equations.&amp;quot;&amp;quot;&amp;quot;

    def _fun(m, _x, _J, _beta, _R):
        return _phi(_f(m, m, _x, _J, _beta, _R), _beta, _R)

    return (
        AndersonAcceleration(
            fixed_point_fun=_fun,
            tol=tol,
            maxiter=maxiter,
        )
        .run(_phi(x + J @ m0, beta, R), x, J, beta, R)
        .params
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We implement a spin-transformer module by wrapping a little boilerplate around the &lt;code&gt;vector_tap_fp&lt;/code&gt; function. We construct the spin-model couplings from the input vectors and mimic multi-head attention by &lt;code&gt;vmap&lt;/code&gt;&amp;lsquo;ing the magnetizations&amp;rsquo; fixed-point solving across &lt;code&gt;num_heads&lt;/code&gt; spin models where each one acts on an equal-size subspace of the full vector dimension.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from functools import partial
from typing import Callable

import equinox as eqx

from einops import rearrange


class SpinTransformerModule(eqx.Module):
    dim: int
    dim_head: int
    num_heads: int
    scale: float
    to_qk: eqx.Module
    vector_tap_fp: Callable

    def __init__(
        self,
        *,
        dim,
        num_heads,
        beta,
        key,
    ):
        super().__init__()

        self.dim = dim
        self.num_heads = num_heads
        self.dim_head = dim // num_heads
        self.scale = (self.dim_head / 2 - 1) ** 0.5

        self.to_qk = eqx.nn.Linear(
            dim, 2 * self.dim_head * num_heads, use_bias=False, key=key
        )
        self.vector_tap_fp = partial(
            vector_tap_fp, beta=beta, R=(self.dim_head / 2 - 1) ** 0.5
        )

    def _J(self, x, mask=None):
        x = rearrange(x, &amp;quot;... h n d -&amp;gt; ... n (h d)&amp;quot;, h=self.num_heads)

        q, k = jnp.split(jax.vmap(self.to_qk)(x), 2, axis=-1)
        q, k = map(
            lambda t: rearrange(t, &amp;quot;... n (h d) -&amp;gt; ... h n d&amp;quot;, h=self.num_heads), (q, k)
        )

        sim = jnp.einsum(&amp;quot;... i d, ... j d -&amp;gt; ... i j&amp;quot;, q, k)

        if mask is not None:
            sim = jnp.where(mask, sim, jnp.finfo(sim.dtype).min)

        return jax.nn.softmax(sim, axis=-1)

    def __call__(self, x, mask=None):
        x = rearrange(x, &amp;quot;... n (h d) -&amp;gt; ... h n d&amp;quot;, h=self.num_heads, d=self.dim_head)
        x = self.scale * x / jnp.linalg.norm(x, axis=-1, keepdims=True)

        m0 = jnp.ones_like(x)
        m0 = m0 / jnp.linalg.norm(m0, axis=-1, keepdims=True)

        return rearrange(
            jax.vmap(self.vector_tap_fp, in_axes=(0, 0, 0))(
                m0, x, self._J(x, mask=mask)
            ),
            &amp;quot;... h n d -&amp;gt; ... n (h d)&amp;quot;,
        )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&amp;rsquo;s run a forward pass of the spin-transformer module&amp;hellip;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;key = jax.random.PRNGKey(2666)
x_key, mod_key = jax.random.split(key)

x = jax.random.normal(x_key, shape=(1, 512, 512))
transformer_module = SpinTransformerModule(dim=512, num_heads=1, beta=2.0, key=mod_key)

print(jax.vmap(transformer_module)(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[[[ 0.46483648  0.3805422  -0.44913006 ...  0.02650307 -0.36570293
    0.23443604]
  [-0.37061682 -0.42315483  0.1197958  ...  0.6265602  -0.61598897
    0.5583689 ]
  [ 0.21803643  0.17418407  0.22512378 ... -0.82831764  0.13957487
    0.17361565]
  ...
  [-0.03738704  0.10310851 -0.12114237 ... -0.17507279  0.30361462
    0.09653477]
  [ 0.4211655  -0.20545821  0.12954816 ...  0.74708706 -0.35752055
   -0.5818469 ]
  [ 1.149747   -0.6245326  -0.28383803 ...  0.31866318 -0.13622926
    0.52548647]]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&amp;hellip; and a backward pass.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@eqx.filter_jit
def loss_fn(model, x):
    return jnp.mean(jax.vmap(model)(x))

print(eqx.filter_grad(loss_fn)(module, x).to_qk.weight)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[[ 6.84143470e-06  1.26781670e-04  3.00350985e-05 ... -2.42774186e-05
   6.56897682e-05 -1.09572255e-04]
 [ 2.77053477e-04 -1.62737968e-04 -9.00395680e-05 ... -8.95370322e-05
  -4.99462512e-05  5.35702784e-05]
 [-1.52689070e-04 -1.44067290e-05  1.77498405e-05 ... -1.35530383e-04
   7.19401141e-05  1.22722937e-04]
 ...
 [-4.90037055e-05 -1.04181963e-04  4.73747787e-06 ... -8.87275892e-05
  -5.93782897e-06 -4.02471051e-05]
 [-4.34355170e-05  3.30054972e-05  1.77152877e-04 ... -1.20974844e-04
  -1.17946729e-04  4.90189996e-06]
 [-3.79099110e-05 -1.06873820e-04 -8.71618904e-05 ...  4.89293416e-05
   8.51267905e-05 -1.46996666e-04]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Going beyond a single spin-transformer module, we can stack modules sequentially to create a spin-transformer model using the &lt;a href=&#34;https://docs.kidger.site/equinox/tricks/#improve-compilation-speed-with-scan-over-layers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scan-over-layers trick&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SpinTransformer(eqx.Module):
    modules: SpinTransformerModule

    def __init__(self, depth, dim, num_heads, beta, key):
        keys = jax.random.split(key, depth)

        make_modules = lambda k: SpinTransformerModule(
            dim=dim, num_heads=num_heads, beta=beta, key=k
        )
        self.modules = eqx.filter_vmap(make_modules)(keys)

    def __call__(self, x):
        dynamic_modules, static_modules = eqx.partition(self.modules, eqx.is_array)

        def f(_x, _dynamic_module):
            module = eqx.combine(_dynamic_module, static_modules)
            return module(_x), None

        out, _ = jax.lax.scan(f, x, dynamic_modules)
        return out


transformer = SpinTransformer(depth=6, dim=512, num_heads=8, beta=1.0, key=mod_key)
print(jax.vmap(transformer)(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;[[[ 0.20396525 -0.06002701 -0.24426042 ...  0.25347382 -0.01503923
   -0.15146086]
  [-0.3552067  -0.4154298  -0.2159235  ...  0.68296695 -0.18692644
    0.20893992]
  [-0.03525298 -0.11836862 -0.13671912 ... -0.22646151  0.18905625
   -0.05829766]
  ...
  [-0.11216182 -0.26305646 -0.31211302 ...  0.27817503  0.25123474
   -0.11120855]
  [ 0.17170963 -0.33360714 -0.12762357 ...  0.70538384 -0.04229175
   -0.5447842 ]
  [ 0.5191558  -0.5662918  -0.33646253 ...  0.4568781  -0.04439414
    0.18843232]]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;5-conclusion&#34;&gt;5. Conclusion&lt;/h1&gt;
&lt;p&gt;In this post, we have shown how &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intuitive ideas connecting spin models to transformers&lt;/a&gt; can be generalized to capture asymmetric coupling matrices like softmax attention. We observed that dynamical mean-field descriptions of vector-spin models exhibit structure capable of yielding residual connections, attention terms, and feed-forward-like correction terms, motivating a physics-inspired class of spin-transformer modules. By blending ideas from deep learning and statistical mechanics, we hope our work can help open up broader interdisciplinary bridges to improve our understanding of learning and generalization in transformer neural networks.&lt;/p&gt;
&lt;p&gt;From a theoretical point of view, it would be interesting to further explore and develop connections to the physics of vector spin glasses. Computationally, we look forward to experiments at scale to get more insight into potential benefits and bottlenecks of spin-transformer models in terms of &lt;a href=&#34;https://twitter.com/YiTayML/status/1714315484357857766&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;efficiency&lt;/a&gt;, representational power, and scaling behavior. In any case, it is fun to think about transformers as a collective of driven, disordered vector-spin models whose response behavior can be shaped by learning parameterized interactions, gradually steering a cascade of near-equilibrium steady-state magnetizations towards solving a given objective.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;A non-exhaustive list of references and inspiration includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;F. Nicoletti, Low energy excitations of vector spin glasses, PhD thesis (2023) &lt;a href=&#34;https://arxiv.org/abs/2306.09228&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2306.09228&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;M. Aguilera, S.A. Moosavi, and H. Shimazaki, A unifying framework for mean-field theories of asymmetric kinetic Ising systems, &lt;em&gt;Nat Commun&lt;/em&gt; &lt;strong&gt;12&lt;/strong&gt;, 1197 (2021) &lt;a href=&#34;https://arxiv.org/abs/2002.04309&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2002.04309&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Y. Roudi and J. Hertz, Dynamical TAP equations for non-equilibrium Ising spin glasses, &lt;em&gt;J. Stat. Mech.&lt;/em&gt;, P03031 (2011) &lt;a href=&#34;https://arxiv.org/abs/1103.1044&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1103.1044&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;H.J. Kappen and J.J. Spanjers, Mean field theory for asymmetric neural networks, &lt;em&gt;Phys. Rev. E&lt;/em&gt; &lt;strong&gt;61&lt;/strong&gt;, 5658 (2000)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;G. Parisi, Asymmetric neural networks and the process of learning, &lt;em&gt;J. Phys. A: Math. Gen.&lt;/em&gt; &lt;strong&gt;19&lt;/strong&gt; L675 (1986)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;If you happen to find this work useful, please consider citing it as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{bal2023spinmodeltransformers,
  title   = {Spin-Model Transformers},
  author  = {Bal, Matthias},
  year    = {2023},
  month   = {December},
  url     = {https://mcbal.github.io/post/spin-model-transformers}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;appendices&#34;&gt;Appendices&lt;/h1&gt;
&lt;h2 id=&#34;a1-vector-spin-distribution-normalization-constant&#34;&gt;A.1. Vector-spin distribution: normalization constant&lt;/h2&gt;
&lt;p&gt;We consider the single-site vector-spin distribution Eq. \eqref{eq:pcondsinglesitevector}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }.
\end{equation}&lt;/p&gt;
&lt;p&gt;Let $Z(\beta, R, \mathbf{h})=\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}$. We switch to $D$-dimensional spherical coordinates to make our life easier and use rotational symmetry to choose the polar axis parallel to $\mathbf{h}$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = R^{D-1} \int_{\Omega} \int_{0}^{\pi} \mathrm{d}^{D-2} \Omega \;\mathrm{d}\theta \; \mathrm{e}^{\beta R h \cos \theta } \sin^{D-2} \theta ,
\end{equation}&lt;/p&gt;
&lt;p&gt;where $h=\lVert\mathbf{h}\rVert$ and where $\int_{\Omega} \mathrm{d}^{D-2} \Omega$ represents the integral over all other spherical angles, which coincides with the surface area of the unit sphere in $D-1$ dimensions,&lt;/p&gt;
&lt;p&gt;\begin{equation}
S_{D-1} = \frac{2\pi^{\frac{D-1}{2}}}{\Gamma\left( \frac{D-1}{2} \right)},
\end{equation}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = \frac{2 \pi^{\frac{D-1}{2}} R^{D-1}}{\Gamma\left( \frac{D-1}{2} \right)} \int_{0}^{\pi} \mathrm{d}\theta \; \mathrm{e}^{\beta R h \cos \theta } \sin^{D-2} \theta .
\end{equation}&lt;/p&gt;
&lt;p&gt;If we now let $u = \cos \theta$, then&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = \frac{2 \pi^{\frac{D-1}{2}} R^{D-1}}{\Gamma\left( \frac{D-1}{2} \right)} \int_{-1}^{1} \mathrm{d}u \; \mathrm{e}^{\beta R h u } \left(1 - u^2\right)^{(D-3)/2} .
\end{equation}&lt;/p&gt;
&lt;p&gt;Recognizing &lt;a href=&#34;https://dlmf.nist.gov/10.32#i&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;an integral representation of the modified Bessel function of the first kind&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;\begin{equation}
I_{\nu}(z) = \frac{2^{-\nu}}{\sqrt{\pi}\, \Gamma\left(\nu+\frac{1}{2}\right)} z^{\nu} \int_{-1}^{1} \mathrm{d}t \; \mathrm{e}^{\pm zt} \left(1-t^2\right)^{\nu-\frac{1}{2}},
\end{equation}&lt;/p&gt;
&lt;p&gt;we identify $\nu = D/2 - 1$ and $z = \beta R h$ to find&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z(\beta, R, h) = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R h) }{ \left(\beta h\right)^{D/2-1} }.
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;a2-vector-spin-distribution-expected-value-first-moment&#34;&gt;A.2. Vector-spin distribution: expected value (first moment)&lt;/h2&gt;
&lt;p&gt;We consider the single-site vector-spin distribution Eq. \eqref{eq:pcondsinglesitevector}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }.
\end{equation}&lt;/p&gt;
&lt;p&gt;Starting from the expression of the normalization constant Eq. \eqref{eq:partfun},&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } = Z(\beta, R, \lVert \mathbf{h}\rVert) ,
\end{equation}&lt;/p&gt;
&lt;p&gt;we write the expected value as&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathbb{E}_{p} [ \mathbf{s} ] = \frac{1}{Z} \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathbf{s} \, \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}
\end{equation}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbb{E}_{p} [ \mathbf{s} ] = \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \left( \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert\mathbf{h} \rVert) }{ \left(\beta \lVert\mathbf{h}\rVert \right)^{D/2-1} } \right)
\end{align}&lt;/p&gt;
&lt;p&gt;which evaluates to&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbb{E}_{p} [ \mathbf{s} ] = \left( \frac{I&amp;rsquo;_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert)}{I_{D/2 - 1}(\beta R \lVert\mathbf{h}\rVert)} - \frac{ D/2-1 }{ \beta R \lVert\mathbf{h}\rVert} \right) \frac{R \mathbf{h}}{\lVert\mathbf{h}\rVert}.
\end{align}&lt;/p&gt;
&lt;p&gt;Using the &lt;a href=&#34;https://dlmf.nist.gov/10.29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modified Bessel function recurrence relations&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;\begin{align}
I_{\nu-1}(z) - I_{\nu+1}(z) &amp;amp;= \frac{2\nu}{z} I_{\nu}(z), \label{eq:irecurr}\\
I_{\nu-1}(z) + I_{\nu+1}(z) &amp;amp;= 2 I&#39;_{\nu}(z), \label{eq:irecurrderiv}
\end{align}&lt;/p&gt;
&lt;p&gt;we end up with&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbb{E}_{p} [ \mathbf{s} ] = \frac{I_{D/2}(\beta R \lVert \mathbf{h}\rVert)}{I_{D/2 - 1}(\beta R \lVert\mathbf{h}\rVert)} \frac{R \mathbf{h}}{\lVert\mathbf{h}\rVert}\equiv \boldsymbol{\varphi} (\mathbf{h}). \label{eq:app:expectedvalue}
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;a3-vector-spin-distribution-variance-second-moment&#34;&gt;A.3. Vector-spin distribution: variance (second moment)&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ &lt;strong&gt;TODO:&lt;/strong&gt; Add variance for general case.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We consider the single-site vector-spin distribution Eq. \eqref{eq:pcondsinglesitevector}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p ( \mathbf{s} ; \beta, \mathbf{h}) = \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }.
\end{equation}&lt;/p&gt;
&lt;p&gt;Using the expression of the normalization constant Eq. \eqref{eq:partfun},&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } = Z(\beta, R, \lVert \mathbf{h}\rVert) ,
\end{equation}&lt;/p&gt;
&lt;p&gt;we write the symmetric outer-product variance matrix as&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \frac{1}{Z} \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} \, ( \mathbf{s} - \mathbb{E}_{p} [ \mathbf{s} ])( \mathbf{s} - \mathbb{E}_{p} [ \mathbf{s} ])^{T} \\
&amp;amp;= \frac{1}{\beta^2 Z} \frac{ \partial^2 }{ \partial \mathbf{h} \partial \mathbf{h}^{T} } \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} - \mathbb{E}_{p} [ \mathbf{s} ] \mathbb{E}_{p} [ \mathbf{s} ]^{T},
\end{align}&lt;/p&gt;
&lt;p&gt;so that&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \left( Z \mathbb{E}_{p} [ \mathbf{s} ]^{T} \right) - \mathbb{E}_{p} [ \mathbf{s} ] \mathbb{E}_{p} [ \mathbf{s} ]^{T}, \\
&amp;amp;= \frac{1}{\beta} \frac{ \partial }{ \partial \mathbf{h} } \mathbb{E}_{p} [ \mathbf{s} ]^{T},
\end{align}&lt;/p&gt;
&lt;p&gt;which evaluates to&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \ldots \label{eq:app:var}
\end{align}&lt;/p&gt;
&lt;p&gt;for the general case with the expected value given by Eq. \eqref{eq:app:expectedvalue} and to&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathrm{Var}_{p} [ \mathbf{s} ] &amp;amp;= \frac{\mathbb{1}}{1+\gamma(\mathbf{h})} - \frac{\beta^2\mathbf{h} \otimes \mathbf{h}}{R^2\gamma(\mathbf{h})\left(1+\gamma(\mathbf{h})\right)^2}\\
&amp;amp;= \frac{\mathbb{1}}{1+\gamma(\mathbf{h})} - \frac{\boldsymbol{\varphi} (\mathbf{h}) \otimes \boldsymbol{\varphi}(\mathbf{h})}{R^2\gamma(\mathbf{h})}
\end{align}&lt;/p&gt;
&lt;p&gt;for the large-$D$ limit with the expected value given by Eq. \eqref{eq:largedevmag}, where&lt;/p&gt;
&lt;p&gt;\begin{align}
\gamma(\mathbf{h}) = \sqrt{1+\beta^{2}\lVert\mathbf{h}\rVert^{2}/R^2}
\end{align}&lt;/p&gt;
&lt;h2 id=&#34;a4-ratio-of-modified-bessel-functions-of-the-first-kind&#34;&gt;A.4. Ratio of modified Bessel functions of the first kind&lt;/h2&gt;
&lt;p&gt;To compute the ratio $I_{\nu+1}(x) / I_{\nu}(x)$ of modified Bessel functions of the first kind for $\nu \geq 0$ and $x \geq 0$, we implement a &lt;a href=&#34;https://github.com/mcbal/spin-model-transformers/blob/main/spin_model_transformers/bessel.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;JAX version&lt;/a&gt; of the algorithm described in &lt;a href=&#34;https://www.jstor.org/stable/2005830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Amos, 1974)&lt;/a&gt;. A pseudocode implementation can be found in &lt;a href=&#34;https://isas.iar.kit.edu/pdf/ACC13_Kurz.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(Kurz et al., 2013)&lt;/a&gt;. We compare our implementation against explicitly calculating the ratio using &lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.ive.html#scipy.special.ive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;scipy.special.ive&lt;/code&gt;&lt;/a&gt; across a range of orders $\nu$ for several different values of $x$ to get a feel for its behavior.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;bessel_plot_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We observe a satisfying agreement between the two approaches. For $x=\sqrt{\nu}$, the ratio takes on very small values for large orders. For $x=\nu^2$, the oppositive happens and we see saturation. The case $x=\nu$ seems to sit in between, which suggests it might be opportune to fix the radius of our little spins to $R=\sqrt{D}$ so that with $\lVert\mathbf{h}\rVert \sim \mathcal{O}(\sqrt{D})$ we might maximize the &amp;ldquo;sensitivity&amp;rdquo; of the expected value. In this regime, we can get away with &lt;a href=&#34;https://link.springer.com/article/10.1007/BF02764812&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;known asymptotic expansions&lt;/a&gt; for large $\nu$ given that the ratio flattens out quickly.&lt;/p&gt;
&lt;h2 id=&#34;a5-general-case-partial-derivatives-with-respect-to-alpha&#34;&gt;A.5. General case: partial derivatives with respect to $\alpha$&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;✨ &lt;strong&gt;TODO:&lt;/strong&gt; Clean up and verify (haha, no).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are interested in computing the first-order and second-order derivative with respect to $\alpha$ of the function&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{\varphi}(\mathbf{h}(\alpha)) = \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \mathbf{h}(\alpha)}{\lVert \mathbf{h}(\alpha) \rVert},
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\mathbf{h}(\alpha) = \boldsymbol{\theta} + \alpha \Delta \mathbf{h}$. Using&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial \lVert \mathbf{h}(\alpha) \rVert}{\partial\alpha} = \frac{\mathbf{h}(\alpha) \cdot \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert}
\end{equation}&lt;/p&gt;
&lt;p&gt;and Eqs. \eqref{eq:irecurr}-\eqref{eq:irecurrderiv}, we find&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha} = \beta &amp;amp;\lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \nonumber \\
&amp;amp;+ \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert} \label{eq:generalgradalphafirstorder}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation}
\lambda_{D} (x) = \frac{I^2_{D/2-1}(x)}{I^2_{D/2}(x)} - \frac{D}{x} \frac{I_{D/2-1}(x)}{I_{D/2}(x)} - 1. \label{eq:app:lambda}
\end{equation}&lt;/p&gt;
&lt;p&gt;For the second-order derivative, we need to slog through even more tedious algebra,&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha^2}
= \beta &amp;amp;\frac{\partial}{\partial\alpha}\biggl( \lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \biggr) \nonumber \\
&amp;amp;+ \frac{\partial}{\partial\alpha}\biggl( \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert} \biggr) ,
\end{align}&lt;/p&gt;
&lt;p&gt;which eventually leads to something like&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha^2}
= -2\beta^2 &amp;amp; \, \kappa_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right)^{2} \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \nonumber \\
&amp;amp;+ \beta \lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \frac{\partial\boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha} \cdot \Delta \mathbf{h} \right) \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \nonumber \\
&amp;amp;+ \beta \lambda_{D} (\beta R \lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \frac{\partial\boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha} \nonumber \\
&amp;amp;- \frac{D}{\lVert \mathbf{h}(\alpha) \rVert^2} \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \Delta \mathbf{h} , \label{eq:generalgradalphasecondorder}
\end{align}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{align}
\kappa_{D} (x) = \lambda^2_{D} (x) + \left( 1 + \frac{D/2 + 1}{x} \frac{I_{D/2-1}(x)}{I_{D/2}(x)} \right) \lambda_{D} (x) + \frac{1}{x} \frac{I_{D/2-1}(x)}{I_{D/2}(x)}.
\end{align}&lt;/p&gt;
&lt;p&gt;Equation \eqref{eq:generalgradalphasecondorder} can be further simplified by substituting the first-order derivative Eq. \eqref{eq:generalgradalphafirstorder} and further simplifying the resulting expression. The derivation of the mean-field equations proceeds in a similar fashion as in the main text, but uses \eqref{eq:generalgradalphafirstorder} and  \eqref{eq:generalgradalphasecondorder} as expressions for the partial derivatives instead of their large-$D$ approximations.&lt;/p&gt;
&lt;p&gt;Another useful derivative is that of the single-site probability distribution \eqref{eq:pcondsinglesitevector},&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{\partial}{\partial\alpha} \left( \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} } \right) =  \frac{\partial}{\partial\mathbf{h}(\alpha)} \left( \frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} } \right) \cdot \Delta \mathbf{h},
\end{align}&lt;/p&gt;
&lt;p&gt;which evaluates to&lt;/p&gt;
&lt;p&gt;\begin{align}
\beta \left( \mathbf{s} - \boldsymbol{\varphi}\left(\mathbf{h}(\alpha)\right) \right) \cdot \Delta \mathbf{h} \frac{ \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} }{ \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}(\alpha)} }
\end{align}&lt;/p&gt;
&lt;p&gt;and can be used to calculate derivatives of the conditional distribution \eqref{eq:pcondaltvector}.&lt;/p&gt;
&lt;h1 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h1&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;We plot the absolute value to get rid of artificial &amp;ldquo;jumps&amp;rdquo; between the two branches. These occur because all models are simulated independently when sweeping across $\beta$ and the some combinations of initial state and model parameters might just happen to bounce to the other branch when $\beta$ changes in the $\beta &amp;gt; \beta_c$ regime.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</title>
      <link>https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/</link>
      <pubDate>Wed, 07 Apr 2021 15:17:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;✨ Update (November 2021):&lt;/strong&gt; &lt;em&gt;Consider reading &lt;a href=&#34;https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers Are Secretly Collectives of Spin Systems&lt;/a&gt; for a high-level overview of some of the ideas outlined in this post.&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-mean-field-theory-for-disordered-systems&#34;&gt;Mean-field theory for disordered systems&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#21-random-ising-models-or-boltzmann-machines-or-&#34;&gt;Random Ising models (or Boltzmann machines or &amp;hellip;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#22-adaptive-thouless--anderson--palmer-mean-field-theory&#34;&gt;Adaptive Thouless&amp;ndash;Anderson&amp;ndash;Palmer mean-field theory&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-attention-as-a-fixed-point-method&#34;&gt;Attention as a fixed-point method&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#31-generalizing-spin-models-to-vector-degrees-of-freedom&#34;&gt;Generalizing spin models to vector degrees of freedom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#32-deep-implicit-attention-attention-as-a-collective-response&#34;&gt;Deep implicit attention: attention as a collective response&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#33-slow-and-explicit-solving-the-adaptive-tap-equations&#34;&gt;Slow and explicit: solving the adaptive TAP equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#34-fast-and-neural-parametrizing-the-onsager-self-correction-term&#34;&gt;Fast and neural: parametrizing the Onsager self-correction term&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-a-mean-field-theory-perspective-on-transformers&#34;&gt;A mean-field theory perspective on transformers&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#41-parametrizing-the-couplings-sparse-graph-structure-from-inputs&#34;&gt;Parametrizing the couplings: sparse graph structure from inputs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#42-softmax-attention-does-a-single-naive-mean-field-update-step&#34;&gt;Softmax attention does a single, naive mean-field update step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#43-feed-forward-layer-corrects-naive-mean-field-update&#34;&gt;Feed-forward layer corrects naive mean-field update&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#44-mean-field-theory-framework-for-transformer-architectures&#34;&gt;Mean-field theory framework for transformer architectures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#45-comparison-with-energy-based-perspective&#34;&gt;Comparison with energy-based perspective&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-conclusion-and-outlook&#34;&gt;Conclusion and outlook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-related-work&#34;&gt;Related work&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Code: A reference PyTorch implementation of the ideas outlined in this blog post is available in the repository &lt;a href=&#34;https://github.com/mcbal/deep-implicit-attention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;mcbal/deep-implicit-attention&lt;/code&gt;&lt;/a&gt;. Comments welcome.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To explore progress beyond the cage of softmax attention, we have previously looked at energy-based perspectives on attention mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Energy-Based Perspective on Attention Mechanisms in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer Attention as an Implicit Mixture of Effective Energy-Based Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention as Energy Minimization: Visualizing Energy Landscapes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main take-away so far has been that you can think of softmax attention as implementing a single, big gradient step of some energy function and that training transformers is akin to meta-learning how to best tune a stack of attention and feed-forward modules to perform well on some auxiliary (meta-)task(s). But what can an energy-based perspective actually provide beyond quaint and hand-wavy statements like &lt;em&gt;implicit energy landscapes are sculpted every time you train a transformer&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this post, we approach attention in terms of the &lt;em&gt;collective response of a statistical-mechanical system&lt;/em&gt;. Attention is interpreted as an inner-loop fixed-point optimization step which returns the approximate response of a system being probed by data. This response is a differentiable compromise between the system&amp;rsquo;s internal dynamics and the data it&amp;rsquo;s being exposed to. To better respond to incoming data, outer-loop optimization steps can nudge the interactions and the self-organizing behaviour of the system.&lt;/p&gt;
&lt;p&gt;To implement our proposal, we combine old ideas and new technology to construct a family of attention mechanisms based on fixed points. We use &lt;a href=&#34;https://arxiv.org/abs/1909.01377&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deep equilibrium models&lt;/a&gt; to solve a set of self-consistent mean-field equations of a vector generalization of the random Ising spin-model. By approximating these equations, we arrive at simplified update steps which mirror the vanilla transformer architecture. We conclude by showing how transformers can be understood from a mean-field theory perspective.&lt;/p&gt;
&lt;h1 id=&#34;2-mean-field-theory-for-disordered-systems&#34;&gt;2. Mean-field theory for disordered systems&lt;/h1&gt;
&lt;p&gt;In physics, &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean-field_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mean-field theory&lt;/a&gt; is an approximation method to study models made up of many individual degrees of freedom that interact with each other. Mean-field theory approximates the effect of the environment on any given individual degree of freedom by a single, averaged effect, and thus reduces a many-body problem to an (effective) one-body problem. This is a drastic approximation. Whether mean-field theory a sensible thing to do depends on the problem and the properties of your variational ansatz.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Mean-field theory &amp;amp; variational methods:&lt;/strong&gt; From the point of view of variational methods, mean-field theory tries to approximate a complicated object (like a partition function of a statistical-mechanical system) by wiggling around the parameters of a tractable variational ansatz to get as close as possible to the real thing. You can picture this process as projecting down a complicated object living in a high-dimensional space to its shadow in an easier-to-handle subspace (&lt;em&gt;I can hear a mathematician fainting in the background&lt;/em&gt;). This effectively reduces the problem to optimizing for the best possible approximation within your variational class. A lot of mean-field machinery also shows up in probability theory, statistics, and machine learning where it appears in belief propagation, approximate variational inference, expectation propagation, etc.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the next two subsections, we introduce random Ising models and sketch a physics-inspired approach to deal with disordered models using mean-field theory. In &lt;a href=&#34;#3-attention-as-a-fixed-point-method&#34;&gt;Section 3&lt;/a&gt; we will then generalize these results to vector spin degrees of freedom and propose two flavours of attention models.&lt;/p&gt;
&lt;h2 id=&#34;21-random-ising-models-or-boltzmann-machines-or-&#34;&gt;2.1. Random Ising models (or Boltzmann machines or &amp;hellip;)&lt;/h2&gt;
&lt;p&gt;The random Ising model is a prototypical model in the study of spin glasses and disordered random systems, where it is often referred to as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Spin_glass#The_model_of_Sherrington_and_Kirkpatrick&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sherrington–Kirkpatrick model&lt;/a&gt;, famous for its replica-method solution by Giorgio Parisi in 1979. Its energy function with external field for $N$ classical, binary spin variables looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = \sum_{i,j} J_{ij} S_{i} S_{j} + \sum_{i} x_{i} S_{i}, \label{eq:randomising}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the couplings $J_{ij}$ between degrees of freedom are randomly distributed according to some probability distribution and self-interactions are absent ($J_{ii} = 0$). The external magnetic fields $x_{i}$ provide a preferential direction of alignment at every local site. Since the elements in the coupling matrix can have both negative and positive signs, the system is said to have both frustrated ferro- as well as antiferromagnetic couplings. The model defined by \eqref{eq:randomising} is also known as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boltzmann machine&lt;/a&gt; or a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hopfield_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield network&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In contrast with disordered systems, we expect the couplings in the context of artificial neural networks to no longer be randomly drawn from a distribution but to reflect structure and organization between spins after being exposed to data. The system should self-organize in order to better respond to incoming data.&lt;/p&gt;
&lt;p&gt;A cartoon of a spin configuration of a 7-spin system looks something like
&lt;img src=&#34;binary_ising.png&#34; alt=&#34;Random Ising model configuration with binary spins&#34; width=&#34;250px&#34;/&gt;
where we have only drawn the connections strongest in absolute value. It&amp;rsquo;s helpful to think of classical spin degrees of freedom as arrows. For vector spins, we can imagine lifting the up/down restriction and letting the arrows rotate freely.&lt;/p&gt;
&lt;h2 id=&#34;22-adaptive-thouless--anderson--palmer-mean-field-theory&#34;&gt;2.2. Adaptive Thouless&amp;ndash;Anderson&amp;ndash;Palmer mean-field theory&lt;/h2&gt;
&lt;p&gt;One of the approaches physicists have come up with to tackle disordered random systems with pairwise interactions like those in Eq. \eqref{eq:randomising} is &lt;a href=&#34;https://doi.org/10.1080/14786437708235992&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thouless&amp;ndash;Anderson&amp;ndash;Palmer (TAP) mean-field theory (1977)&lt;/a&gt;. The TAP equations improve mean-field theory results by adding a so-called &lt;em&gt;Onsager self-correction term&lt;/em&gt; calculated from the couplings&#39; distribution.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.64.056131&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Opper and Winther (2001)&lt;/a&gt; adapted this method to probabilisic modeling to be able to deal with scenarios where the distribution of the couplings between spins is not known a priori. To compensate for the lack of knowledge of the couplings distribution, they introduced a self-consistent computation to adapt the Onsager correction to the &lt;em&gt;actual&lt;/em&gt; couplings using the cavity method and linear response relations. We will sketch the adaptive TAP approach below but refer to &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.64.056131&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Opper and Winther (2001)&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1409.6179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raymond, Manoel, and Opper (2014)&lt;/a&gt; for more details and derivations.&lt;/p&gt;
&lt;h3 id=&#34;single-site-partition-function-from-cavity-method&#34;&gt;Single-site partition function from cavity method&lt;/h3&gt;
&lt;p&gt;The adaptive TAP equations can be derived using the cavity method, where a cavity field distribution is introduced to rewrite the marginal distributions of the spins. The cavity corresponds to the &amp;ldquo;hole&amp;rdquo; left by removing a single spin. By assuming a Gaussian cavity distribution in the large connectivity limit, one can show that the single-site partition function looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z_{0}^{(i)} = \int \mathrm{d} S \ \rho_{i}\left(S\right) \exp \left[ S \left( a_{i} + x_{i} \right) + \frac{V_{i} S^2}{2}  \right]
\end{equation}&lt;/p&gt;
&lt;p&gt;where the $a_i$ denote &lt;em&gt;cavity means&lt;/em&gt; and the $V_i$ &lt;em&gt;cavity variances&lt;/em&gt;. The single-site partition function can be integrated to yield an explicit expression after choosing well-behaved priors $\rho_{i}(S)$ for the spins. For binary spins $S=\pm 1$, we can pick $\rho_{i}(S)=\frac{1}{2}\left( \delta(S-1) + \delta(S+1) \right)$ to find&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z_{0}^{(i)} = \cosh \left( a_{i} + x_{i} \right). \label{eq:partfunbinaryspins}
\end{equation}&lt;/p&gt;
&lt;h3 id=&#34;cavity-means-and-onsager-correction-term&#34;&gt;Cavity means and Onsager correction term&lt;/h3&gt;
&lt;p&gt;The cavity means can be shown to be given by
\begin{equation}
a_{i} = \sum_{j} J_{ij} \langle S_{j} \rangle - V_{i} \langle S_{i} \rangle. \label{eq:cavitymean}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the last term is the &lt;em&gt;Onsager correction term&lt;/em&gt;, a self-correction term for every spin which depends on the cavity variances.&lt;/p&gt;
&lt;h3 id=&#34;cavity-variances-and-linear-response&#34;&gt;Cavity variances and linear response&lt;/h3&gt;
&lt;p&gt;The cavity variances are determined self-consistently, i.e. by calculating the same quantity in two different ways and demanding the obtained expressions to be equal. To do this, we introduce the matrix of susceptibilities&lt;/p&gt;
&lt;p&gt;\begin{equation}
\chi_{ij} = \langle S_{i} S_{j} \rangle - \langle S_{i} \rangle \langle S_{j} \rangle  = \frac{\partial^2}{\partial x_{i}\partial x_{j}} \log Z_{0}^{(i)}
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The susceptibility matrix $\chi_{ij}$ is a covariance matrix and should thus be positive semi-definite, which is criterion for the mean-field solution be consistent. As soon this property is lost, the fixed-point procedure will no longer be stable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Its diagonal elements $\chi_{ii}$ can be obtained both from the explicit calculation of the spin variances from the partition function&lt;/p&gt;
&lt;p&gt;\begin{equation}
\chi_{ii} = \langle S_{i}^2 \rangle - \langle S_{i} \rangle^2 = \frac{\partial^2}{\partial x_{i}^2} \log Z_{0}^{(i)} \label{eq:chiii}
\end{equation}&lt;/p&gt;
&lt;p&gt;but also from a linear response calculation assuming fixed $V_i$,&lt;/p&gt;
&lt;p&gt;\begin{align}
\chi_{ij} = \frac{\partial \langle S_{i} \rangle}{\partial x_{j}} = \frac{\partial \langle S_{i} \rangle}{\partial x_{i}} \left( \delta_{ij} + \sum_{k} \left( J_{ik} - V_{k} \delta_{ik} \right) \chi_{kj} \right)  \label{eq:chiijlinrespexp}
\end{align}&lt;/p&gt;
&lt;p&gt;which can be solved for $\chi_{ij}$ to yield
\begin{equation}
\chi_{ij} =  \left[ \left( \boldsymbol{\Lambda} - \boldsymbol{J} \right)^{-1} \right]_{ij} \label{eq:chiijlinresp}
\end{equation}
where
\begin{align}
\boldsymbol{\Lambda} = \mathrm{diag} \left( \Lambda_1, \ldots, \Lambda_{N} \right),\\
\Lambda_i = V_i + \left( \frac{\partial \langle S_{i} \rangle}{\partial x_{i}} \right)^{-1}.
\end{align}&lt;/p&gt;
&lt;p&gt;The cavity variances $V_i$ are then determined by equating \eqref{eq:chiii} to the diagonal elements of \eqref{eq:chiijlinresp} and solving the following consistency condition for $V_i$
\begin{equation}
\frac{1}{\Lambda_i - V_i} =  \left[ \left( \boldsymbol{\Lambda} - \boldsymbol{J} \right)^{-1} \right]_{ii}.  \label{eq:viselfcons}
\end{equation}&lt;/p&gt;
&lt;p&gt;Given updated values for the cavity means $a_i$ and the cavity variances $V_i$, spin means and spin variances can then be updated as follows:&lt;/p&gt;
&lt;p&gt;\begin{align}
\langle S_{i} \rangle &amp;amp;= \frac{\partial}{\partial x_{i}} \log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}),\\
\langle S_{i}^2 \rangle - \langle S_{i} \rangle^2 &amp;amp;= \frac{\partial^2}{\partial x_{i}^2} \log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}),
\end{align}&lt;/p&gt;
&lt;p&gt;These equations reduce to explicit expressions given an explicit expression for $Z_{0}^{(i)}$. For the binary-spin partition function \eqref{eq:partfunbinaryspins} where $S=\pm 1$, we get a set of fixed-point equations for the spin means that look like&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle S_{i} \rangle = \tanh \left( \sum_{j} J_{ij} \langle S_{j} \rangle - V_{i} \langle S_{i} \rangle + x_{i} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;with spin variances $\chi_{ii} = 1 - \langle S_{i} \rangle^2$.&lt;/p&gt;
&lt;h1 id=&#34;3-attention-as-a-fixed-point-method&#34;&gt;3. Attention as a fixed-point method&lt;/h1&gt;
&lt;p&gt;In this section, we attempt to generalize the mean-field equations obtained in the previous section to random Ising-like models with vector spin degrees of freedom. We then recognize the physical system as an attention model and provide both a slow, explicit implementation and a faster, neural one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Code: A reference PyTorch implementation of the models outlined below is available in the repository &lt;a href=&#34;https://github.com/mcbal/deep-implicit-attention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;deep-implicit-attention&lt;/code&gt;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;31-generalizing-spin-models-to-vector-degrees-of-freedom&#34;&gt;3.1. Generalizing spin models to vector degrees of freedom&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s return to our Ising model cartoon and replace the scalar spin degrees of freedom $S_i$ at every site with vectors $\boldsymbol{S}_i \in \mathbb{R}^d$, which we visualize using arrows below&lt;/p&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;Random Ising model configuration with vector spins&#34; width=&#34;250px&#34;/&gt;
&lt;p&gt;Let&amp;rsquo;s consider a system of $N$ $d$-dimensional spins and let&amp;rsquo;s label site indices with $i,j,\ldots$ and internal vector-space indices with Greek letters $\alpha,\beta,\ldots$. We let the coupling weight matrix become a tensor $\boldsymbol{J}_{ij} = J_{ij}^{\alpha\beta}$ (matrices coupling every pair of sites) and remove self-couplings by enforcing the couplings&#39; block-diagonal to be zero. Additionally, we can symmetrize both the internal dimension and the sites to end up with $N(N-1)/2$ times $d(d+1)/2$ effective free parameters for the couplings. If we also turn the external fields into vectors, we obtain a vector generalization of Eq. \eqref{eq:randomising}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = \sum_{i,j} \boldsymbol{S}_{i}^{T} \boldsymbol{J}_{ij} \boldsymbol{S}_{j} + \sum_{i} \boldsymbol{X}_{i} \cdot \boldsymbol{S}_{i}. \label{eq:vectrandomising}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;32-deep-implicit-attention-attention-as-a-collective-response&#34;&gt;3.2. Deep implicit attention: attention as a collective response&lt;/h2&gt;
&lt;p&gt;Remember that our goal is to understand attention as the collective response of a statistical-mechanical system. Let&amp;rsquo;s now relate vector models like Eq. \eqref{eq:vectrandomising} to attention models by treating the external magnetic fields $\boldsymbol{X}_{i}$ as input data. Batches of sequences applied to every site act as probes for the system, pushing its behaviour into a certain direction. The system&amp;rsquo;s mean-field average magnetizations $\langle \boldsymbol{S}_{i} \rangle$ are an approximation of the collective response at every site: what is the expected value of this particular vector spin? We interpret solving mean-field equations for $\langle \boldsymbol{S}_{i} \rangle$ in the presence of input injections $\boldsymbol{X}_{i}$ as an attention operation. If the whole system is differentiable, we can tune the couplings $\boldsymbol{J}_{ij}$ in an outer-loop optimization to steer the system&amp;rsquo;s behaviour to better&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; respond to future incoming data.&lt;/p&gt;
&lt;h2 id=&#34;33-slow-and-explicit-solving-the-adaptive-tap-equations&#34;&gt;3.3. Slow and explicit: solving the adaptive TAP equations&lt;/h2&gt;
&lt;p&gt;What changes do we have to make to the adaptive TAP mean-field equations to turn them into a vector-based attention module and how can we implement them? Let&amp;rsquo;s explicitly enumerate the objects introduced in &lt;a href=&#34;#22-adaptive-thouless--anderson--palmer-mean-field-theory&#34;&gt;Section 2.2&lt;/a&gt; together with their (generalized) tensor shapes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Iteratively determined fixed-point variables&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spin means $\langle \boldsymbol{S}_{i} \rangle = \left[ \langle \boldsymbol{S}_{i} \rangle \right]^{\alpha}$ &lt;code&gt;(batch_size, N, d)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Cavity variances $\boldsymbol{V}_{i} = V_{i}^{\alpha\beta}$ &lt;code&gt;(N, d, d)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Other variables calculated during fixed-point iteration&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cavity means $\boldsymbol{a}_{i} = a_{i}^{\alpha}$ &lt;code&gt;(batch_size, N, d)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Spin variances $\langle \boldsymbol{S}_{i}^2 \rangle - \langle \boldsymbol{S}_{i} \rangle^2 = \boldsymbol{\chi}_{ii} = \chi_{ii}^{\alpha\beta}$ &lt;code&gt;(N, d, d)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For every site, the scalar spin and cavity variances have turned into $d \times d$ (inverse) covariance matrices on the level of the local dimension. Note that the &amp;ldquo;system properties&amp;rdquo; in the above list have no batch size: their values are identical across all examples and capture the properties of the system irrespective of the input injections $\boldsymbol{X}_i$.&lt;/p&gt;
&lt;p&gt;The vector translation of the single-site partition function looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z_{0}^{(i)} = \int \mathrm{d}^{d} \boldsymbol{S} \  \rho_{i}\left(\boldsymbol{S}\right) \exp \left[ \boldsymbol{S} \cdot \left( \boldsymbol{a}_{i} + \boldsymbol{X}_{i} \right) + \frac{1}{2} \boldsymbol{S}^T  \boldsymbol{V}_{i} \boldsymbol{S} \right]
\end{equation}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{a}_{i} = \sum_{j} \boldsymbol{J}_{ij} \langle \boldsymbol{S}_{j} \rangle - \boldsymbol{V}_{i}\langle \boldsymbol{S}_{i} \rangle. \label{eq:veccavmeans}
\end{equation}&lt;/p&gt;
&lt;p&gt;Spin means and variances are then computed from&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i} \rangle = \frac{\partial}{\partial\boldsymbol{X}_{i}} \log Z_{0}^{(i)} (\boldsymbol{X}_{i}, \boldsymbol{a}_{i}, \boldsymbol{V}_{i})
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i}^2 \rangle - \langle \boldsymbol{S}_{i} \rangle^2 = \frac{\partial^2}{\partial\boldsymbol{X}_{i}^2} \log Z_{0}^{(i)} (\boldsymbol{X}_{i}, \boldsymbol{a}_{i}, \boldsymbol{V}_{i})
\end{equation}&lt;/p&gt;
&lt;p&gt;As a spin prior $\rho_{i}\left(\boldsymbol{S}\right)$, we pick a simple diagonal multivariate Gaussian $\mathcal{N} \left( \boldsymbol{\mu} = \boldsymbol{0}_{d}, \boldsymbol{\Sigma}= \boldsymbol{1}_{d \times d} \right)$ at every site, leading to the explicit equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i} \rangle = \left( \boldsymbol{\Sigma}^{-1} - \boldsymbol{V}_{i} \right)^{-1} \left( \boldsymbol{a}_{i} + \boldsymbol{X}_{i} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i}^2 \rangle - \langle \boldsymbol{S}_{i} \rangle^2 = \left( \boldsymbol{\Sigma}^{-1} - \boldsymbol{V}_{i} \right)^{-1}
\end{equation}&lt;/p&gt;
&lt;h3 id=&#34;generalizing-the-cavity-variance-calculation&#34;&gt;Generalizing the cavity variance calculation&lt;/h3&gt;
&lt;p&gt;The cavity variance computation can be done by generalizing Eqs. \eqref{eq:chiijlinrespexp}&amp;ndash;\eqref{eq:chiijlinresp} and solving the following system of equations for $\boldsymbol{\chi}_{ij}$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left( \delta_{ik} \otimes \boldsymbol{1}_{d} - \boldsymbol{\Sigma}_{i} \boldsymbol{J}_{ik} + \boldsymbol{\Sigma}_{i} \boldsymbol{V}_{i} \delta_{ik} \right)\boldsymbol{\chi}_{kj} = \boldsymbol{\Sigma}_{i} \delta_{ij}
\end{equation}&lt;/p&gt;
&lt;p&gt;The generalization of the self-consistency condition Eq \eqref{eq:viselfcons} is then obtained by solving $\boldsymbol{\chi}_{ii} \boldsymbol{V}_{i} = \boldsymbol{\chi}_{ii} \boldsymbol{\Lambda}_{i} - \boldsymbol{1}_{N \times d \times d}$ for $\boldsymbol{V}_{i}$, where $ \boldsymbol{\Lambda}_{i} = \boldsymbol{V}_{i} + \boldsymbol{\Sigma}^{-1}$ is computed using the current values of $\boldsymbol{V}_{i}$. The price to pay for this added complexity is a computational cost of $O(N^3d^3)$ and an excruciatingly slow backward pass. The algorithm works, but it ain&amp;rsquo;t pretty.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Implementation:&lt;/strong&gt; To avoid &lt;code&gt;torch.solve&lt;/code&gt; crashing on singular matrices during the fixed-point calculation, we found it crucial for stability and learning behaviour to initialize the couplings $J_{ij}^{\alpha\beta} \sim \mathcal{N}(0, \sigma^2)$ with small values $\sigma^2 = 1 / (N*d^2)$ to ensure $|J| \sim \mathcal{O}(1)$. It&amp;rsquo;s also beneficial if the sources satisfy $|\boldsymbol{X}_{i}| \sim \mathcal{O}(1)$ so that terms are balanced in the update step, all together adding up to $\mathcal{O}(1)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;34-fast-and-neural-parametrizing-the-onsager-self-correction-term&#34;&gt;3.4. Fast and neural: parametrizing the Onsager self-correction term&lt;/h2&gt;
&lt;p&gt;Can we somehow approximate the slow and explicit calculation of the cavity variances? Since $\boldsymbol{z}^{*} = \left( \langle \boldsymbol{S}_{i}^{*} \rangle, \boldsymbol{V}_{i}^{*} \right)$ at the fixed point, the Onsager self-correction term in Eq. \eqref{eq:veccavmeans} converges to a constant vector $\boldsymbol{V}_{i}^{*}\langle \boldsymbol{S}_{i}^{*} \rangle$ for every site. We propose to make a bold move by getting rid of the cavity variables altogether and reducing the equations for the fixed-point update step to&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i} \rangle = \sum_{j} \boldsymbol{J}_{ij} \langle \boldsymbol{S}_{j} \rangle - f_{\theta} \left( \langle \boldsymbol{S}_{i} \rangle \right) + \boldsymbol{X}_{i}, \label{eq:diaupdate}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $f_{\theta}$ is a neural network parametrizing the action of the cavity variances on the spin means. Since the parameters $\theta$ stay fixed during the inner-loop fixed-point calculation, we have effectively lifted the optimization of the self-correction term to the outer-loop, which also optimizes the weights $\boldsymbol{J}_{ij}$.&lt;/p&gt;
&lt;p&gt;All of this starts to look an awful lot like a transformer module. Before discussing an explicit comparison in &lt;a href=&#34;#4-a-mean-field-theory-perspective-on-transformers&#34;&gt;Section 4&lt;/a&gt;, let&amp;rsquo;s finish this section with a simple example model.&lt;/p&gt;
&lt;h3 id=&#34;simple-example-mnist&#34;&gt;Simple example: MNIST&lt;/h3&gt;
&lt;p&gt;A simple image classification model for MNIST using a convolutional feature extractor and a deep implicit attention layer could look something like&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MNISTNet(nn.Module):
    def __init__(self, dim=10, dim_conv=32, num_spins=16):
        super(MNISTNet, self).__init__()

        self.to_patch_embedding = nn.Sequential(
            nn.Conv2d(1, dim_conv, kernel_size=3),  # -&amp;gt; 26 x 26
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2),  # -&amp;gt; 12 x 12
            nn.Conv2d(dim_conv, dim_conv, kernel_size=3),  # -&amp;gt; 10 x 10
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2),  # -&amp;gt; 4 x 4
            Rearrange(
                &#39;b c h w -&amp;gt; b (h w) c&#39;
            ),
            nn.Linear(dim_conv, dim)
        )
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.deq_atn = nn.Sequential(
            DEQFixedPoint(
                DEQMeanFieldAttention(
                    num_spins=num_spins+1,
                    dim=dim,
                    weight_sym_internal=True,
                    weight_sym_sites=False,
                    lin_response=True,
                ),
                anderson,
                solver_fwd_max_iter=40,
                solver_fwd_tol=1e-4,
                solver_bwd_max_iter=40,
                solver_bwd_tol=1e-4,
            ),
        )
        self.final = nn.Linear(dim, 10)

    def forward(self, x):
        x = self.to_patch_embedding(x)
        cls_tokens = self.cls_token.repeat(x.shape[0], 1, 1)
        x = torch.cat((cls_tokens, x), dim=1)
        x = self.deq_atn(x)
        return self.final(x[:, 0, :])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ViT-style classification token is interpreted as an additional site in the system, which is probed with a learnable input injection that is shared across examples. The model uses the classification token&amp;rsquo;s output response to do the final classification. The system has to self-organize its behaviour so that the classification token gets all the information it needs.&lt;/p&gt;
&lt;img src=&#34;vit_mnist.gif&#34; alt=&#34;ViT-style model with deep implicit attention layer on MNIST&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;You can &lt;a href=&#34;https://github.com/mcbal/deep-implicit-attention/blob/549ef3c76ccd1a7b7df6af3eeebb540abb7f7f31/examples/mnist.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;train&lt;/a&gt; this small model (26k parameters) on MNIST to find a test set accuracy hovering around 99.1%. The animation above shows a graph reflecting the (directed) connection strengths between spins during training as measured by the Frobenius norms of the matrices $\boldsymbol{J}_{ij}$. Almost all major organization of connections is seen to happen in the first few iterations. One imagines the model getting frustrated at zeros which &lt;em&gt;really&lt;/em&gt; look like nines and just flat-out refusing to remember edge cases out of spite.&lt;/p&gt;
&lt;h1 id=&#34;4-a-mean-field-theory-perspective-on-transformers&#34;&gt;4. A mean-field theory perspective on transformers&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s conclude this post by applying the mean-field theory perspective on attention to the transformer architecture. Schematically, a vanilla transformer module looks like&lt;/p&gt;
&lt;img src=&#34;vanilla_transformer_module.png&#34; alt=&#34;Vanilla transformer module&#34; width=&#34;200px&#34;/&gt;
&lt;p&gt;which consists of an attention module acting on all vectors in the sequence input followed by a feed-forward layer acting &amp;ldquo;locally&amp;rdquo; across individual vectors in the sequence, mixed with some residual connections and layer normalizations.&lt;/p&gt;
&lt;h2 id=&#34;41-parametrizing-the-couplings-sparse-graph-structure-from-inputs&#34;&gt;4.1. Parametrizing the couplings: sparse graph structure from inputs&lt;/h2&gt;
&lt;p&gt;Transformers can be interpreted as fully-connected graph neural networks acting on sets of vectors. Inside an attention module, the row-stochastic attention matrix corresponds to a particular parametrization of the couplings&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_{ij} = \left[\mathrm{softmax}\left( \frac{\boldsymbol{X} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{X}^{T}}{\sqrt{d}} \right)\right]_{ij}. \label{eq:softmaxcouplings}
\end{equation}&lt;/p&gt;
&lt;p&gt;which swaps storing explicit coupling weights for parameters of linear query-key transformations. By dynamically determining the connectivity of the sites based on the inputs $\boldsymbol{X}$ according to Eq. \eqref{eq:softmaxcouplings}, the coupling weights are no longer completely free parameters. The introduction of queries and keys can be seen as a neural network approach to &amp;ldquo;amortizing&amp;rdquo; the coupling tensor while the softmax temperature promotes sparsity. Multiple attention heads correspond to imposing a block-diagonal structure in the hidden dimensions of the couplings: the dot product gets cut into disjoint pieces, one for each attention head.&lt;/p&gt;
&lt;h2 id=&#34;42-softmax-attention-does-a-single-naive-mean-field-update-step&#34;&gt;4.2. Softmax attention does a single, naive mean-field update step&lt;/h2&gt;
&lt;p&gt;Looking at the update step \eqref{eq:diaupdate} and the softmax couplings \eqref{eq:softmaxcouplings}, we observe that the softmax attention module does a single, naive mean-field update step without a self-correction term. Ignoring layer normalizations, the attention update step for every input vector looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{X}&#39;_{i} = \sum_{j} \left[ \mathrm{softmax} \left( \frac{\boldsymbol{X} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{X}^{T}}{\sqrt{d}} \right) \right]_{ij} \left[ \boldsymbol{X} \boldsymbol{W}_{\boldsymbol{V}} \right]_{j} + \boldsymbol{X}_{i}, \nonumber
\label{eq:vanilla-attention}
\end{equation}&lt;/p&gt;
&lt;p&gt;where, crucially, the residual connection is responsible for adding the source term to the update step. Without a residual connection, the applied magnetic field is effectively turned off and the signal would only be able to propagate via the coupling term.&lt;/p&gt;
&lt;h2 id=&#34;43-feed-forward-layer-corrects-naive-mean-field-update&#34;&gt;4.3. Feed-forward layer corrects naive mean-field update&lt;/h2&gt;
&lt;p&gt;Looking at the Onsager self-correction term $f_{\theta} \left( \langle \boldsymbol{S}_{i} \rangle \right)$ in the update step \eqref{eq:diaupdate}, we observe that the full transformer attention module emerges when we substitute $\langle \boldsymbol{S}_{i} \rangle$ for its naive mean-field value, leading to&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathrm{Attention}(\boldsymbol{X})_{i} = \boldsymbol{X}&#39;_{i} + \mathrm{FeedForward}\left( \boldsymbol{X}&#39;_{i} \right),
\end{equation}&lt;/p&gt;
&lt;p&gt;with $\boldsymbol{X}&#39;_{i}$ defined above. Again, the residual connection appears to be crucial for the structure of the mean-field theory equations to match the vanilla transformer module&amp;rsquo;s architecture. As previously discussed in &lt;a href=&#34;#34-fast-and-neural-parametrizing-the-onsager-self-correction-term&#34;&gt;Section 3.4&lt;/a&gt;, we hypothesize that feed-forward networks in transformer modules &amp;ldquo;amortize&amp;rdquo; the linear response self-corrections.&lt;/p&gt;
&lt;h2 id=&#34;44-mean-field-theory-framework-for-transformer-architectures&#34;&gt;4.4. Mean-field theory framework for transformer architectures&lt;/h2&gt;
&lt;p&gt;Within the general mean-field (or &lt;a href=&#34;https://arxiv.org/abs/2105.02180&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;approximate message-passing&lt;/a&gt;) structure outlined above, there is considerable freedom in parametrizing the interaction and self-correction terms. Most transformer papers parametrize the self-correction terms with a feed-forward layer, i.e. some variation of an MLP. In &lt;a href=&#34;https://arxiv.org/abs/2105.01601&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt; the authors went even further and dropped the softmax parametrization of the interaction term to approximate the full action of summing over couplings with an MLP as well. Related papers like &lt;a href=&#34;https://arxiv.org/abs/2105.08050&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pay Attention to MLPs&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2105.03404&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResMLP: Feedforward networks for image classification with data-efficient training&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2105.03824&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/a&gt; can all be considered as explorations of different parametrizations of the mean-field interaction terms. In the large-scale regime, it seems like the softmax attention module can be swapped for just about any function which mixes tokens as long as the structure of residual connections and self-correction terms is preserved.&lt;/p&gt;
&lt;h2 id=&#34;45-comparison-with-energy-based-perspective&#34;&gt;4.5. Comparison with energy-based perspective&lt;/h2&gt;
&lt;p&gt;In a previous post on &lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Energy-Based Perspective on Attention Mechanisms in Transformers&lt;/a&gt;, we introduced a picture of attention modules in transformers as stacks of energy functions which are defined dynamically at every layer depending on the outputs of the previous layer (so ultimately on the inputs of the first layer). Looking back, this interpretation feels kind of forced and is also unable to explain the presence of skip connections and fully-connected layers surrounding the attention modules. The mean-field perspective seems more interesting since it (1) relies on just one layer (one energy function) whose fixed-point (an infinite amount of &amp;ldquo;layers&amp;rdquo;) gets calculated, and (2) explains the presence of skip connections (source terms) and fully-connected layers (amortized self-correction terms).&lt;/p&gt;
&lt;h1 id=&#34;5-conclusion-and-outlook&#34;&gt;5. Conclusion and outlook&lt;/h1&gt;
&lt;p&gt;We have shown how attention can be understood as the mean-field response of Ising-like spin systems being probed by data. By thinking of incoming data as applied magnetic fields and the output of attention modules as spin expectation values, attention can be interpreted as a fixed-point optimization process solving for a compromise between a system&amp;rsquo;s internal dynamics and the data it&amp;rsquo;s being exposed to. Since the whole system is differentiable, we can optimize the interaction weights in an outer loop to nudge the system&amp;rsquo;s behaviour.&lt;/p&gt;
&lt;p&gt;We have also seen how transformers fit into the mean-field theory framework. For scalability, transformers introduce two additional constraints/approximations on top of the mean-field approximation: (1) replacing explicit couplings with parametrized couplings that are dynamically computed from the input via linear transformations (softmax query-key-value attention), and (2) replacing the expensive self-consistent computation of Onsager self-correction terms with a neural network (feed-forward layer).&lt;/p&gt;
&lt;p&gt;Looking ahead, the methods introduced in this post could provide ways to implicitly train mean-field approximations of Boltzmann machines and have them serve as distributed attention modules in larger interconnected systems. To go beyond mean-field approaches, it could be interesting to look at tensor network approaches. Conceptually, the physical interpretation of attention as an interacting many-body system modulating its behaviour by &lt;em&gt;learning to respond to being driven in particular ways&lt;/em&gt; is fun to think about.&lt;/p&gt;
&lt;h1 id=&#34;6-related-work&#34;&gt;6. Related work&lt;/h1&gt;
&lt;p&gt;A non-exhaustive list of references and inspiration includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On deep equilibrium models: &lt;a href=&#34;https://arxiv.org/abs/1909.01377&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Equilibrium Models&lt;/a&gt; (2019) by Shaojie Bai, Zico Kolter, Vladlen Koltun and &lt;a href=&#34;https://implicit-layers-tutorial.org/deep_equilibrium_models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 4: Deep Equilibrium Models&lt;/a&gt; of the &lt;a href=&#34;http://implicit-layers-tutorial.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Layers - Neural ODEs, Deep Equilibrium Models, and Beyond workshop (NeurIPS 2020)&lt;/a&gt; by Zico Kolter, David Duvenaud, and Matt Johnson&lt;/li&gt;
&lt;li&gt;On the adaptive Thouless-Anderson-Palmer (TAP) mean-field approach in disorder physics: &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.64.056131&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive and self-averaging Thouless-Anderson-Palmer mean-field theory for probabilistic modeling&lt;/a&gt; (2001) by Manfred Opper and Ole Winther&lt;/li&gt;
&lt;li&gt;On variational inference, iterative approximation algorithms, expectation propagation, mean-field methods and belief propagation: &lt;a href=&#34;https://arxiv.org/abs/1409.6179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Expectation Propagation&lt;/a&gt; (2014) by Jack Raymond, Andre Manoel, Manfred Opper&lt;/li&gt;
&lt;li&gt;On Boltzmann machines and mean-field theory: &lt;a href=&#34;https://doi.org/10.1162/089976698300017386&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Learning in Boltzmann Machines Using Linear Response Theory&lt;/a&gt; (1998) by H. J. Kappen and
F. B. Rodríguez and &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.58.2302&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mean-field theory of Boltzmann machine learning&lt;/a&gt; (1998) by Toshiyuki Tanaka&lt;/li&gt;
&lt;li&gt;On approximate message passing (AMP) methods in statistics: &lt;a href=&#34;https://arxiv.org/abs/2105.02180&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A unifying tutorial on Approximate Message Passing&lt;/a&gt; (2021) by Oliver Y. Feng, Ramji Venkataramanan, Cynthia Rush, Richard J. Samworth: the example on page 2 basically describes how transformers implement approximate message passing: an iterative algorithm with a &amp;ldquo;denoising&amp;rdquo; step (attention) followed by a &amp;ldquo;memory term&amp;rdquo; or Onsager correction term (feed-forward layer)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/h1&gt;
&lt;p&gt;If you happen to find this work useful, please consider citing it as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{bal2021deepimplicitattention,
  title   = {Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms},
  author  = {Bal, Matthias},
  year    = {2021},
  month   = {May},
  url     = {https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/},
}
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Whatever &amp;ldquo;better&amp;rdquo; means depends on the system&amp;rsquo;s (meta-)loss function, e.g. predicting corrupted tokens BERT-style or aligning representations to a teacher BYOL/DINO-style.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
