<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Emergent Collective Computational Capabilities | mcbal</title>
    <link>https://mcbal.github.io/tag/emergent-collective-computational-capabilities/</link>
      <atom:link href="https://mcbal.github.io/tag/emergent-collective-computational-capabilities/index.xml" rel="self" type="application/rss+xml" />
    <description>Emergent Collective Computational Capabilities</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>Matthias Bal © 2021</copyright><lastBuildDate>Wed, 07 Apr 2021 15:17:17 +0100</lastBuildDate>
    <image>
      <url>https://mcbal.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Emergent Collective Computational Capabilities</title>
      <link>https://mcbal.github.io/tag/emergent-collective-computational-capabilities/</link>
    </image>
    
    <item>
      <title>Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</title>
      <link>https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/</link>
      <pubDate>Wed, 07 Apr 2021 15:17:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/</guid>
      <description>&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-mean-field-theory-for-disordered-systems&#34;&gt;Mean-field theory for disordered systems&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#21-random-ising-models-or-boltzmann-machines-or-&#34;&gt;Random Ising models (or Boltzmann machines or &amp;hellip;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#22-adaptive-thouless--anderson--palmer-mean-field-theory&#34;&gt;Adaptive Thouless&amp;ndash;Anderson&amp;ndash;Palmer mean-field theory&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-attention-as-a-fixed-point-method&#34;&gt;Attention as a fixed-point method&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#31-generalizing-spin-models-to-vector-degrees-of-freedom&#34;&gt;Generalizing spin models to vector degrees of freedom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#32-deep-implicit-attention-attention-as-a-collective-response&#34;&gt;Deep implicit attention: attention as a collective response&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#33-slow-and-explicit-solving-the-adaptive-tap-equations&#34;&gt;Slow and explicit: solving the adaptive TAP equations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#34-fast-and-neural-parametrizing-the-onsager-self-correction-term&#34;&gt;Fast and neural: parametrizing the Onsager self-correction term&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-a-mean-field-theory-perspective-on-transformers&#34;&gt;A mean-field theory perspective on transformers&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#41-parametrizing-the-couplings-sparse-graph-structure-from-inputs&#34;&gt;Parametrizing the couplings: sparse graph structure from inputs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#42-softmax-attention-does-a-single-naive-mean-field-update-step&#34;&gt;Softmax attention does a single, naive mean-field update step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#43-feed-forward-layer-corrects-naive-mean-field-update&#34;&gt;Feed-forward layer corrects naive mean-field update&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-conclusion-and-outlook&#34;&gt;Conclusion and outlook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-related-work&#34;&gt;Related work&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;To explore progress beyond the cage of softmax attention, we have previously looked at energy-based perspectives on attention mechanisms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Energy-Based Perspective on Attention Mechanisms in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer Attention as an Implicit Mixture of Effective Energy-Based Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention as Energy Minimization: Visualizing Energy Landscapes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main take-away so far has been that you can think of softmax attention as implementing a single, big gradient step of some energy function and that training transformers is akin to meta-learning how to best tune a stack of attention and feed-forward modules to perform well on some auxiliary (meta-)task(s). But what can an energy-based perspective actually provide beyond quaint and hand-wavy statements like &lt;em&gt;implicit energy landscapes are sculpted every time you train a transformer&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;In this post, we approach attention in terms of the &lt;em&gt;collective response of a statistical-mechanical system&lt;/em&gt;. Attention is interpreted as an inner-loop fixed-point optimization step which returns the approximate response of a system being probed by data. This response is a differentiable compromise between the system&amp;rsquo;s internal dynamics and the data it&amp;rsquo;s being exposed to. To better respond to incoming data, outer-loop optimization steps can nudge the interactions and the self-organizing behaviour of the system.&lt;/p&gt;
&lt;p&gt;To implement our proposal, we combine old ideas and new technology to construct a family of attention mechanisms based on fixed points. We use &lt;a href=&#34;https://arxiv.org/abs/1909.01377&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;deep equilibrium models&lt;/a&gt; to solve a set of self-consistent mean-field equations of a vector generalization of the random Ising spin-model. By approximating these equations, we arrive at simplified update steps which mirror the vanilla transformer architecture. We conclude by showing how transformers can be understood from a mean-field theory perspective.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Code: A reference PyTorch implementation of the ideas outlined in this blog post is available in the repository &lt;a href=&#34;https://github.com/mcbal/deep-implicit-attention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;deep-implicit-attention&lt;/code&gt;&lt;/a&gt;. Comments welcome.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;2-mean-field-theory-for-disordered-systems&#34;&gt;2. Mean-field theory for disordered systems&lt;/h1&gt;
&lt;p&gt;In physics, &lt;a href=&#34;https://en.wikipedia.org/wiki/Mean-field_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mean-field theory&lt;/a&gt; is an approximation method to study models made up of many individual degrees of freedom that interact with each other. Mean-field theory approximates the effect of the environment on any given individual degree of freedom by a single, averaged effect, and thus reduces a many-body problem to an (effective) one-body problem. This is a drastic approximation. Whether mean-field theory a sensible thing to do depends on the problem and the properties of your variational ansatz.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Mean-field theory &amp;amp; variational methods:&lt;/strong&gt; From the point of view of variational methods, mean-field theory tries to approximate a complicated object (like a partition function of a statistical-mechanical system) by wiggling around the parameters of a tractable variational ansatz to get as close as possible to the real thing. You can picture this process as projecting down a complicated object living in a high-dimensional space to its shadow in an easier-to-handle subspace (&lt;em&gt;I can hear a mathematician fainting in the background&lt;/em&gt;). This effectively reduces the problem to optimizing for the best possible approximation within your variational class. A lot of mean-field machinery also shows up in probability theory, statistics, and machine learning where it appears in belief propagation, approximate variational inference, expectation propagation, etc.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the next two subsections, we introduce random Ising models and sketch a physics-inspired approach to deal with disordered models using mean-field theory. In &lt;a href=&#34;#3-attention-as-a-fixed-point-method&#34;&gt;Section 3&lt;/a&gt; we will then generalize these results to vector spin degrees of freedom and propose two flavours of attention models.&lt;/p&gt;
&lt;h2 id=&#34;21-random-ising-models-or-boltzmann-machines-or-&#34;&gt;2.1. Random Ising models (or Boltzmann machines or &amp;hellip;)&lt;/h2&gt;
&lt;p&gt;The random Ising model is a prototypical model in the study of spin glasses and disordered random systems, where it is often referred to as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Spin_glass#The_model_of_Sherrington_and_Kirkpatrick&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sherrington–Kirkpatrick model&lt;/a&gt;, famous for its replica-method solution by Giorgio Parisi in 1979. Its energy function with external field for $N$ classical, binary spin variables looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = \sum_{i,j} J_{ij} S_{i} S_{j} + \sum_{i} x_{i} S_{i}, \label{eq:randomising}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the couplings $J_{ij}$ between degrees of freedom are randomly distributed according to some probability distribution and self-interactions are absent ($J_{ii} = 0$). The external magnetic fields $x_{i}$ provide a preferential direction of alignment at every local site. Since the elements in the coupling matrix can have both negative and positive signs, the system is said to have both frustrated ferro- as well as antiferromagnetic couplings. The model defined by \eqref{eq:randomising} is also known as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boltzmann machine&lt;/a&gt; or a &lt;a href=&#34;https://en.wikipedia.org/wiki/Hopfield_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield network&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In contrast with disordered systems, we expect the couplings in the context of artificial neural networks to no longer be randomly drawn from a distribution but to reflect structure and organization between spins after being exposed to data. The system should self-organize in order to better respond to incoming data.&lt;/p&gt;
&lt;p&gt;A cartoon of a spin configuration of a 7-spin system looks something like
&lt;img src=&#34;binary_ising.png&#34; alt=&#34;Random Ising model configuration with binary spins&#34; width=&#34;250px&#34;/&gt;
where we have only drawn the connections strongest in absolute value. It&amp;rsquo;s helpful to think of classical spin degrees of freedom as arrows. For vector spins, we can imagine lifting the up/down restriction and letting the arrows rotate freely.&lt;/p&gt;
&lt;h2 id=&#34;22-adaptive-thouless--anderson--palmer-mean-field-theory&#34;&gt;2.2. Adaptive Thouless&amp;ndash;Anderson&amp;ndash;Palmer mean-field theory&lt;/h2&gt;
&lt;p&gt;One of the approaches physicists have come up with to tackle disordered random systems with pairwise interactions like those in Eq. \eqref{eq:randomising} is &lt;a href=&#34;https://doi.org/10.1080/14786437708235992&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Thouless&amp;ndash;Anderson&amp;ndash;Palmer (TAP) mean-field theory (1977)&lt;/a&gt;. The TAP equations improve mean-field theory results by adding a so-called &lt;em&gt;Onsager self-correction term&lt;/em&gt; calculated from the couplings&#39; distribution.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.64.056131&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Opper and Winther (2001)&lt;/a&gt; adapted this method to probabilisic modeling to be able to deal with scenarios where the distribution of the couplings between spins is not known a priori. To compensate for the lack of knowledge of the couplings distribution, they introduced a self-consistent computation to adapt the Onsager correction to the &lt;em&gt;actual&lt;/em&gt; couplings using the cavity method and linear response relations. We will sketch the adaptive TAP approach below but refer to &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.64.056131&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Opper and Winther (2001)&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1409.6179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Raymond, Manoel, and Opper (2014)&lt;/a&gt; for more details and derivations.&lt;/p&gt;
&lt;h3 id=&#34;single-site-partition-function-from-cavity-method&#34;&gt;Single-site partition function from cavity method&lt;/h3&gt;
&lt;p&gt;The adaptive TAP equations can be derived using the cavity method, where a cavity field distribution is introduced to rewrite the marginal distributions of the spins. The cavity corresponds to the &amp;ldquo;hole&amp;rdquo; left by removing a single spin. By assuming a Gaussian cavity distribution in the large connectivity limit, one can show that the single-site partition function looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z_{0}^{(i)} = \int \mathrm{d} S \ \rho_{i}\left(S\right) \exp \left[ S \left( a_{i} + x_{i} \right) + \frac{V_{i} S^2}{2}  \right]
\end{equation}&lt;/p&gt;
&lt;p&gt;where the $a_i$ denote &lt;em&gt;cavity means&lt;/em&gt; and the $V_i$ &lt;em&gt;cavity variances&lt;/em&gt;. The single-site partition function can be integrated to yield an explicit expression after choosing well-behaved priors $\rho_{i}(S)$ for the spins. For binary spins $S=\pm 1$, we can pick $\rho_{i}(S)=\frac{1}{2}\left( \delta(S-1) + \delta(S+1) \right)$ to find&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z_{0}^{(i)} = \cosh \left( a_{i} + x_{i} \right). \label{eq:partfunbinaryspins}
\end{equation}&lt;/p&gt;
&lt;h3 id=&#34;cavity-means-and-onsager-correction-term&#34;&gt;Cavity means and Onsager correction term&lt;/h3&gt;
&lt;p&gt;The cavity means can be shown to be given by
\begin{equation}
a_{i} = \sum_{j} J_{ij} \langle S_{j} \rangle - V_{i} \langle S_{i} \rangle. \label{eq:cavitymean}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the last term is the &lt;em&gt;Onsager correction term&lt;/em&gt;, a self-correction term for every spin which depends on the cavity variances.&lt;/p&gt;
&lt;h3 id=&#34;cavity-variances-and-linear-response&#34;&gt;Cavity variances and linear response&lt;/h3&gt;
&lt;p&gt;The cavity variances are determined self-consistently, i.e. by calculating the same quantity in two different ways and demanding the obtained expressions to be equal. To do this, we introduce the matrix of susceptibilities&lt;/p&gt;
&lt;p&gt;\begin{equation}
\chi_{ij} = \langle S_{i} S_{j} \rangle - \langle S_{i} \rangle \langle S_{j} \rangle  = \frac{\partial^2}{\partial x_{i}\partial x_{j}} \log Z_{0}^{(i)}
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The susceptibility matrix $\chi_{ij}$ is a covariance matrix and should thus be positive semi-definite, which is criterion for the mean-field solution be consistent. As soon this property is lost, the fixed-point procedure will no longer be stable.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Its diagonal elements $\chi_{ii}$ can be obtained both from the explicit calculation of the spin variances from the partition function&lt;/p&gt;
&lt;p&gt;\begin{equation}
\chi_{ii} = \langle S_{i}^2 \rangle - \langle S_{i} \rangle^2 = \frac{\partial^2}{\partial x_{i}^2} \log Z_{0}^{(i)} \label{eq:chiii}
\end{equation}&lt;/p&gt;
&lt;p&gt;but also from a linear response calculation assuming fixed $V_i$,&lt;/p&gt;
&lt;p&gt;\begin{align}
\chi_{ij} = \frac{\partial \langle S_{i} \rangle}{\partial x_{j}} = \frac{\partial \langle S_{i} \rangle}{\partial x_{i}} \left( \delta_{ij} + \sum_{k} \left( J_{ik} - V_{k} \delta_{ik} \right) \chi_{kj} \right)  \label{eq:chiijlinrespexp}
\end{align}&lt;/p&gt;
&lt;p&gt;which can be solved for $\chi_{ij}$ to yield
\begin{equation}
\chi_{ij} =  \left[ \left( \boldsymbol{\Lambda} - \boldsymbol{J} \right)^{-1} \right]_{ij} \label{eq:chiijlinresp}
\end{equation}
where
\begin{align}
\boldsymbol{\Lambda} = \mathrm{diag} \left( \Lambda_1, \ldots, \Lambda_{N} \right),\\&lt;br&gt;
\Lambda_i = V_i + \left( \frac{\partial \langle S_{i} \rangle}{\partial x_{i}} \right)^{-1}.
\end{align}&lt;/p&gt;
&lt;p&gt;The cavity variances $V_i$ are then determined by equating \eqref{eq:chiii} to the diagonal elements of \eqref{eq:chiijlinresp} and solving the following consistency condition for $V_i$
\begin{equation}
\frac{1}{\Lambda_i - V_i} =  \left[ \left( \boldsymbol{\Lambda} - \boldsymbol{J} \right)^{-1} \right]_{ii}.  \label{eq:viselfcons}
\end{equation}&lt;/p&gt;
&lt;p&gt;Given updated values for the cavity means $a_i$ and the cavity variances $V_i$, spin means and spin variances can then be updated as follows:&lt;/p&gt;
&lt;p&gt;\begin{align}
\langle S_{i} \rangle &amp;amp;= \frac{\partial}{\partial x_{i}} \log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}),\\&lt;br&gt;
\langle S_{i}^2 \rangle - \langle S_{i} \rangle^2 &amp;amp;= \frac{\partial^2}{\partial x_{i}^2} \log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}),
\end{align}&lt;/p&gt;
&lt;p&gt;These equations reduce to explicit expressions given an explicit expression for $Z_{0}^{(i)}$. For the binary-spin partition function \eqref{eq:partfunbinaryspins} where $S=\pm 1$, we get a set of fixed-point equations for the spin means that look like&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle S_{i} \rangle = \tanh \left( \sum_{j} J_{ij} \langle S_{j} \rangle - V_{i} \langle S_{i} \rangle + x_{i} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;with spin variances $\chi_{ii} = 1 - \langle S_{i} \rangle^2$.&lt;/p&gt;
&lt;h1 id=&#34;3-attention-as-a-fixed-point-method&#34;&gt;3. Attention as a fixed-point method&lt;/h1&gt;
&lt;p&gt;In this section, we attempt to generalize the mean-field equations obtained in the previous section to random Ising-like models with vector spin degrees of freedom. We then recognize the physical system as an attention model and provide both a slow, explicit implementation and a faster, neural one.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;✨ Code: A reference PyTorch implementation of the models outlined below is available in the repository &lt;a href=&#34;https://github.com/mcbal/deep-implicit-attention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;deep-implicit-attention&lt;/code&gt;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;31-generalizing-spin-models-to-vector-degrees-of-freedom&#34;&gt;3.1. Generalizing spin models to vector degrees of freedom&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s return to our Ising model cartoon and replace the scalar spin degrees of freedom $S_i$ at every site with vectors $\boldsymbol{S}_i \in \mathbb{R}^d$, which we visualize using arrows below&lt;/p&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;Random Ising model configuration with vector spins&#34; width=&#34;250px&#34;/&gt;
&lt;p&gt;Let&amp;rsquo;s consider a system of $N$ $d$-dimensional spins and let&amp;rsquo;s label site indices with $i,j,\ldots$ and internal vector-space indices with Greek letters $\alpha,\beta,\ldots$. We let the coupling weight matrix become a tensor $\boldsymbol{J}_{ij} = J_{ij}^{\alpha\beta}$ (matrices coupling every pair of sites) and remove self-couplings by enforcing the couplings&#39; block-diagonal to be zero. Additionally, we can symmetrize both the internal dimension and the sites to end up with $N(N-1)/2$ times $d(d+1)/2$ effective free parameters for the couplings. If we also turn the external fields into vectors, we obtain a vector generalization of Eq. \eqref{eq:randomising}:&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = \sum_{i,j} \boldsymbol{S}_{i}^{T} \boldsymbol{J}_{ij} \boldsymbol{S}_{j} + \sum_{i} \boldsymbol{X}_{i} \cdot \boldsymbol{S}_{i}. \label{eq:vectrandomising}
\end{equation}&lt;/p&gt;
&lt;h2 id=&#34;32-deep-implicit-attention-attention-as-a-collective-response&#34;&gt;3.2. Deep implicit attention: attention as a collective response&lt;/h2&gt;
&lt;p&gt;Remember that our goal is to understand attention as the collective response of a statistical-mechanical system. Let&amp;rsquo;s now relate vector models like Eq. \eqref{eq:vectrandomising} to attention models by treating the external magnetic fields $\boldsymbol{X}_{i}$ as input data. Batches of sequences applied to every site act as probes for the system, pushing its behaviour into a certain direction. The system&amp;rsquo;s mean-field average magnetizations $\langle \boldsymbol{S}_{i} \rangle$ are an approximation of the collective response at every site: what is the expected value of this particular vector spin? We interpret solving mean-field equations for $\langle \boldsymbol{S}_{i} \rangle$ in the presence of input injections $\boldsymbol{X}_{i}$ as an attention operation. If the whole system is differentiable, we can tune the couplings $\boldsymbol{J}_{ij}$ in an outer-loop optimization to steer the system&amp;rsquo;s behaviour to better&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; respond to future incoming data.&lt;/p&gt;
&lt;h2 id=&#34;33-slow-and-explicit-solving-the-adaptive-tap-equations&#34;&gt;3.3. Slow and explicit: solving the adaptive TAP equations&lt;/h2&gt;
&lt;p&gt;What changes do we have to make to the adaptive TAP mean-field equations to turn them into a vector-based attention module and how can we implement them? Let&amp;rsquo;s explicitly enumerate the objects introduced in &lt;a href=&#34;#22-adaptive-thouless--anderson--palmer-mean-field-theory&#34;&gt;Section 2.2&lt;/a&gt; together with their (generalized) tensor shapes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Iteratively determined fixed-point variables&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spin means $\langle \boldsymbol{S}_{i} \rangle = \left[ \langle \boldsymbol{S}_{i} \rangle \right]^{\alpha}$ &lt;code&gt;(batch_size, N, d)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Cavity variances $\boldsymbol{V}_{i} = V_{i}^{\alpha\beta}$ &lt;code&gt;(N, d, d)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Other variables calculated during fixed-point iteration&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cavity means $\boldsymbol{a}_{i} = a_{i}^{\alpha}$ &lt;code&gt;(batch_size, N, d)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Spin variances $\langle \boldsymbol{S}_{i}^2 \rangle - \langle \boldsymbol{S}_{i} \rangle^2 = \boldsymbol{\chi}_{ii} = \chi_{ii}^{\alpha\beta}$ &lt;code&gt;(N, d, d)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For every site, the scalar spin and cavity variances have turned into $d \times d$ (inverse) covariance matrices on the level of the local dimension. Note that the &amp;ldquo;system properties&amp;rdquo; in the above list have no batch size: their values are identical across all examples and capture the properties of the system irrespective of the input injections $\boldsymbol{x}_i$.&lt;/p&gt;
&lt;p&gt;The vector translation of the single-site partition function looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z_{0}^{(i)} = \int \mathrm{d}^{d} \boldsymbol{S} \  \rho_{i}\left(\boldsymbol{S}\right) \exp \left[ \boldsymbol{S} \cdot \left( \boldsymbol{a}_{i} + \boldsymbol{X}_{i} \right) + \frac{1}{2} \boldsymbol{S}^T  \boldsymbol{V}_{i} \boldsymbol{S} \right]
\end{equation}&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{a}_{i} = \sum_{j} \boldsymbol{J}_{ij} \langle \boldsymbol{S}_{j} \rangle - \boldsymbol{V}_{i}\langle \boldsymbol{S}_{i} \rangle. \label{eq:veccavmeans}
\end{equation}&lt;/p&gt;
&lt;p&gt;Spin means and variances are then computed from&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i} \rangle = \frac{\partial}{\partial\boldsymbol{X}_{i}} \log Z_{0}^{(i)} (\boldsymbol{X}_{i}, \boldsymbol{a}_{i}, \boldsymbol{V}_{i})
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i}^2 \rangle - \langle \boldsymbol{S}_{i} \rangle^2 = \frac{\partial^2}{\partial\boldsymbol{X}_{i}^2} \log Z_{0}^{(i)} (\boldsymbol{X}_{i}, \boldsymbol{a}_{i}, \boldsymbol{V}_{i})
\end{equation}&lt;/p&gt;
&lt;p&gt;As a spin prior $\rho_{i}\left(\boldsymbol{S}\right)$, we pick a simple diagonal multivariate Gaussian $\mathcal{N} \left( \boldsymbol{\mu} = \boldsymbol{0}_{d}, \boldsymbol{\Sigma}= \boldsymbol{1}_{d \times d} \right)$ at every site, leading to the explicit equations:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i} \rangle = \left( \boldsymbol{\Sigma}^{-1} - \boldsymbol{V}_{i} \right)^{-1} \left( \boldsymbol{a}_{i} + \boldsymbol{X}_{i} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i}^2 \rangle - \langle \boldsymbol{S}_{i} \rangle^2 = \left( \boldsymbol{\Sigma}^{-1} - \boldsymbol{V}_{i} \right)^{-1}
\end{equation}&lt;/p&gt;
&lt;h3 id=&#34;generalizing-the-cavity-variance-calculation&#34;&gt;Generalizing the cavity variance calculation&lt;/h3&gt;
&lt;p&gt;The cavity variance computation can be done by generalizing Eqs. \eqref{eq:chiijlinrespexp}&amp;ndash;\eqref{eq:chiijlinresp} and solving the following system of equations for $\boldsymbol{\chi}_{ij}$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left( \delta_{ik} \otimes \boldsymbol{1}_{d} - \boldsymbol{\Sigma}_{i} \boldsymbol{J}_{ik} + \boldsymbol{\Sigma}_{i} \boldsymbol{V}_{i} \delta_{ik} \right)\boldsymbol{\chi}_{kj} = \boldsymbol{\Sigma}_{i} \delta_{ij}
\end{equation}&lt;/p&gt;
&lt;p&gt;The generalization of the self-consistency condition Eq \eqref{eq:viselfcons} is then obtained by solving $\boldsymbol{\chi}_{ii} \boldsymbol{V}_{i} = \boldsymbol{\chi}_{ii} \boldsymbol{\Lambda}_{i} - \boldsymbol{1}_{N \times d \times d}$ for $\boldsymbol{V}_{i}$, where $ \boldsymbol{\Lambda}_{i} = \boldsymbol{V}_{i} + \boldsymbol{\Sigma}^{-1}$ is computed using the current values of $\boldsymbol{V}_{i}$. The price to pay for this added complexity is a computational cost of $O(N^3d^3)$ and an excruciatingly slow backward pass. The algorithm works, but it ain&amp;rsquo;t pretty.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Implementation:&lt;/strong&gt; To avoid &lt;code&gt;torch.solve&lt;/code&gt; crashing on singular matrices during the fixed-point calculation, we found it crucial for stability and learning behaviour to initialize the couplings $J_{ij}^{\alpha\beta} \sim \mathcal{N}(0, \sigma^2)$ with small values $\sigma^2 = 1 / (N*d^2)$ to ensure $|J| \sim \mathcal{O}(1)$. It&amp;rsquo;s also beneficial if the sources satisfy $|\boldsymbol{X}_{i}| \sim \mathcal{O}(1)$ so that terms are balanced in the update step, all together adding up to $\mathcal{O}(1)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;34-fast-and-neural-parametrizing-the-onsager-self-correction-term&#34;&gt;3.4. Fast and neural: parametrizing the Onsager self-correction term&lt;/h2&gt;
&lt;p&gt;Can we somehow approximate the slow and explicit calculation of the cavity variances? Since $\boldsymbol{z}^{*} = \left( \langle \boldsymbol{S}_{i}^{*} \rangle, \boldsymbol{V}_{i}^{*} \right)$ at the fixed point, the Onsager self-correction term in Eq. \eqref{eq:veccavmeans} converges to a constant vector $\boldsymbol{V}_{i}^{*}\langle \boldsymbol{S}_{i}^{*} \rangle$ for every site. We propose to make a bold move by getting rid of the cavity variables altogether and reducing the equations for the fixed-point update step to&lt;/p&gt;
&lt;p&gt;\begin{equation}
\langle \boldsymbol{S}_{i} \rangle = \sum_{j} \boldsymbol{J}_{ij} \langle \boldsymbol{S}_{j} \rangle - f_{\theta} \left( \langle \boldsymbol{S}_{i} \rangle \right) + \boldsymbol{X}_{i}, \label{eq:diaupdate}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $f_{\theta}$ is a neural network parametrizing the action of the cavity variances on the spin means. Since the parameters $\theta$ stay fixed during the inner-loop fixed-point calculation, we have effectively lifted the optimization of the self-correction term to the outer-loop, which also optimizes the weights $\boldsymbol{J}_{ij}$.&lt;/p&gt;
&lt;p&gt;All of this starts to look an awful lot like a transformer module. Before discussing an explicit comparison in &lt;a href=&#34;#4-a-mean-field-theory-perspective-on-transformers&#34;&gt;Section 4&lt;/a&gt;, let&amp;rsquo;s finish this section with a simple example model.&lt;/p&gt;
&lt;h3 id=&#34;simple-example-mnist&#34;&gt;Simple example: MNIST&lt;/h3&gt;
&lt;p&gt;A simple image classification model for MNIST using a convolutional feature extractor and a deep implicit attention layer could look something like&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MNISTNet(nn.Module):
    def __init__(self, dim=10, dim_conv=32, num_spins=16):
        super(MNISTNet, self).__init__()

        self.to_patch_embedding = nn.Sequential(
            nn.Conv2d(1, dim_conv, kernel_size=3),  # -&amp;gt; 26 x 26
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2),  # -&amp;gt; 12 x 12
            nn.Conv2d(dim_conv, dim_conv, kernel_size=3),  # -&amp;gt; 10 x 10
            nn.ReLU(),
            nn.MaxPool2d(3, stride=2),  # -&amp;gt; 4 x 4
            Rearrange(
                &#39;b c h w -&amp;gt; b (h w) c&#39;
            ),
            nn.Linear(dim_conv, dim)
        )
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        self.deq_atn = nn.Sequential(
            DEQFixedPoint(
                DEQMeanFieldAttention(
                    num_spins=num_spins+1,
                    dim=dim,
                    weight_sym_internal=True,
                    weight_sym_sites=False,
                    lin_response=True,
                ),
                anderson,
                solver_fwd_max_iter=40,
                solver_fwd_tol=1e-4,
                solver_bwd_max_iter=40,
                solver_bwd_tol=1e-4,
            ),
        )
        self.final = nn.Linear(dim, 10)

    def forward(self, x):
        x = self.to_patch_embedding(x)
        cls_tokens = self.cls_token.repeat(x.shape[0], 1, 1)
        x = torch.cat((cls_tokens, x), dim=1)
        x = self.deq_atn(x)
        return self.final(x[:, 0, :])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ViT-style classification token is interpreted as an additional site in the system, which is probed with a learnable input injection that is shared across examples. The model uses the classification token&amp;rsquo;s output response to do the final classification. The system has to self-organize its behaviour so that the classification token gets all the information it needs.&lt;/p&gt;
&lt;img src=&#34;vit_mnist.gif&#34; alt=&#34;ViT-style model with deep implicit attention layer on MNIST&#34; width=&#34;500px&#34;/&gt;
&lt;p&gt;You can &lt;a href=&#34;https://github.com/mcbal/deep-implicit-attention/blob/549ef3c76ccd1a7b7df6af3eeebb540abb7f7f31/examples/mnist.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;train&lt;/a&gt; this small model (26k parameters) on MNIST to find a test set accuracy hovering around 99.1%. The animation above shows a graph reflecting the (directed) connection strengths between spins during training as measured by the Frobenius norms of the matrices $\boldsymbol{J}_{ij}$. Almost all major organization of connections is seen to happen in the first few iterations. One imagines the model getting frustrated at zeros which &lt;em&gt;really&lt;/em&gt; look like nines and just flat-out refusing to remember edge cases out of spite.&lt;/p&gt;
&lt;h1 id=&#34;4-a-mean-field-theory-perspective-on-transformers&#34;&gt;4. A mean-field theory perspective on transformers&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s conclude this post by applying the mean-field theory perspective on attention to the transformer architecture. Schematically, a vanilla transformer module looks like&lt;/p&gt;
&lt;img src=&#34;vanilla_transformer_module.png&#34; alt=&#34;Vanilla transformer module&#34; width=&#34;200px&#34;/&gt;
&lt;p&gt;which consists of an attention module acting on all vectors in the sequence input followed by a feed-forward layer acting &amp;ldquo;locally&amp;rdquo; across individual vectors in the sequence, mixed with some residual connections and layer normalizations.&lt;/p&gt;
&lt;h2 id=&#34;41-parametrizing-the-couplings-sparse-graph-structure-from-inputs&#34;&gt;4.1. Parametrizing the couplings: sparse graph structure from inputs&lt;/h2&gt;
&lt;p&gt;Transformers can be interpreted as fully-connected graph neural networks acting on sets of vectors. Inside an attention module, the row-stochastic attention matrix corresponds to a particular parametrization of the couplings&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_{ij} = \left[\mathrm{softmax}\left( \frac{\boldsymbol{X} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{X}^{T}}{\sqrt{d}} \right)\right]_{ij}. \label{eq:softmaxcouplings}
\end{equation}&lt;/p&gt;
&lt;p&gt;which swaps storing explicit coupling weights for parameters of linear query-key transformations. By dynamically determining the connectivity of the sites based on the inputs $\boldsymbol{X}$ according to Eq. \eqref{eq:softmaxcouplings}, the coupling weights are no longer completely free parameters. The introduction of queries and keys can be seen as a neural network approach to &amp;ldquo;amortizing&amp;rdquo; the coupling tensor while the softmax temperature promotes sparsity. Multiple attention heads correspond to imposing a block-diagonal structure in the hidden dimensions of the couplings: the dot product gets cut into disjoint pieces, one for each attention head.&lt;/p&gt;
&lt;h2 id=&#34;42-softmax-attention-does-a-single-naive-mean-field-update-step&#34;&gt;4.2. Softmax attention does a single, naive mean-field update step&lt;/h2&gt;
&lt;p&gt;Looking at the update step \eqref{eq:diaupdate} and the softmax couplings \eqref{eq:softmaxcouplings}, we observe that the softmax attention module does a single, naive mean-field update step without a self-correction term. Ignoring layer normalizations, the attention update step for every input vector looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{X}&#39;_{i} = \sum_{j} \left[ \mathrm{softmax} \left( \frac{\boldsymbol{X} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{X}^{T}}{\sqrt{d}} \right) \right]_{ij} \left[ \boldsymbol{X} \boldsymbol{W}_{\boldsymbol{V}} \right]_{j} + \boldsymbol{X}_{i}, \nonumber
\label{eq:vanilla-attention}
\end{equation}&lt;/p&gt;
&lt;p&gt;where, crucially, the residual connection is responsible for adding the source term to the update step. Without a residual connection, the applied magnetic field is effectively turned off and the signal would only be able to propagate via the coupling term.&lt;/p&gt;
&lt;h2 id=&#34;43-feed-forward-layer-corrects-naive-mean-field-update&#34;&gt;4.3. Feed-forward layer corrects naive mean-field update&lt;/h2&gt;
&lt;p&gt;Looking at the Onsager self-correction term $f_{\theta} \left( \langle \boldsymbol{S}_{i} \rangle \right)$ in the update step \eqref{eq:diaupdate}, we observe that the full transformer attention module emerges when we substitute $\langle \boldsymbol{S}_{i} \rangle$ for its naive mean-field value, leading to&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathrm{Attention}(\boldsymbol{X})_{i} = \boldsymbol{X}&#39;_{i} + \mathrm{FeedForward}\left( \boldsymbol{X}&#39;_{i} \right),
\end{equation}&lt;/p&gt;
&lt;p&gt;with $\boldsymbol{X}&#39;_{i}$ defined above. Again, the residual connection appears to be crucial for the structure of the mean-field theory equations to match the vanilla transformer module&amp;rsquo;s architecture. As previously discussed in &lt;a href=&#34;#34-fast-and-neural-parametrizing-the-onsager-self-correction-term&#34;&gt;Section 3.4&lt;/a&gt;, we hypothesize that feed-forward networks in transformer modules &amp;ldquo;amortize&amp;rdquo; the linear response self-corrections.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Real-world example:&lt;/strong&gt; Within the general mean-field structure outlined above, there is considerable freedom in parametrizing the interaction and self-correction terms. Most transformer papers parametrize the self-correction terms with a feed-forward layer, i.e. some variation of an MLP. In &lt;a href=&#34;https://arxiv.org/abs/2105.01601&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt; the authors went even further and dropped the softmax parametrization of the interaction term to approximate the full action of summing over couplings with an MLP as well. Interestingly, it seems like one can get away with such a crude approximation for images in the large-scale regime.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;5-conclusion-and-outlook&#34;&gt;5. Conclusion and outlook&lt;/h1&gt;
&lt;p&gt;We have shown how attention can be understood as the mean-field response of Ising-like spin systems being probed by data. By thinking of incoming data as applied magnetic fields and the output of attention modules as spin expectation values, attention can be interpreted as a fixed-point optimization process solving for a compromise between a system&amp;rsquo;s internal dynamics and the data it&amp;rsquo;s being exposed to. Since the whole system is differentiable, we can optimize the interaction weights in an outer loop to nudge the system&amp;rsquo;s behaviour.&lt;/p&gt;
&lt;p&gt;We have also seen how transformers fit into the mean-field theory framework. For scalability, transformers introduce two additional constraints/approximations on top of the mean-field approximation: (1) replacing explicit couplings with parametrized couplings that are dynamically computed from the input via linear transformations (softmax query-key-value attention), and (2) replacing the expensive self-consistent computation of Onsager self-correction terms with a neural network (feed-forward layer).&lt;/p&gt;
&lt;p&gt;Looking ahead, the methods introduced in this post could provide ways to implicitly train mean-field approximations of Boltzmann machines and have them serve as distributed attention modules in larger interconnected systems. To go beyond mean-field approaches, it could be interesting to look at tensor network approaches.&lt;/p&gt;
&lt;h1 id=&#34;6-related-work&#34;&gt;6. Related work&lt;/h1&gt;
&lt;p&gt;A non-exhaustive list of references and inspiration includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On deep equilibrium models: &lt;a href=&#34;https://arxiv.org/abs/1909.01377&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Equilibrium Models&lt;/a&gt; (2019) by Shaojie Bai, Zico Kolter, Vladlen Koltun and &lt;a href=&#34;https://implicit-layers-tutorial.org/deep_equilibrium_models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chapter 4: Deep Equilibrium Models&lt;/a&gt; of the &lt;a href=&#34;http://implicit-layers-tutorial.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Layers - Neural ODEs, Deep Equilibrium Models, and Beyond workshop (NeurIPS 2020)&lt;/a&gt; by Zico Kolter, David Duvenaud, and Matt Johnson&lt;/li&gt;
&lt;li&gt;On the adaptive Thouless-Anderson-Palmer (TAP) mean-field approach in disorder physics: &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.64.056131&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive and self-averaging Thouless-Anderson-Palmer mean-field theory for probabilistic modeling&lt;/a&gt; (2001) by Manfred Opper and Ole Winther&lt;/li&gt;
&lt;li&gt;On variational inference, iterative approximation algorithms, expectation propagation, mean-field methods and belief propagation: &lt;a href=&#34;https://arxiv.org/abs/1409.6179&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Expectation Propagation&lt;/a&gt; (2014) by Jack Raymond, Andre Manoel, Manfred Opper&lt;/li&gt;
&lt;li&gt;On Boltzmann machines and mean-field theory: &lt;a href=&#34;https://doi.org/10.1162/089976698300017386&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient Learning in Boltzmann Machines Using Linear Response Theory&lt;/a&gt; (1998) by H. J. Kappen and
F. B. Rodríguez and &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevE.58.2302&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mean-field theory of Boltzmann machine learning&lt;/a&gt; (1998) by Toshiyuki Tanaka&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/h1&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Whatever &amp;ldquo;better&amp;rdquo; means depends on the system&amp;rsquo;s (meta-)loss function, e.g. predicting corrupted tokens BERT-style or aligning representations to a teacher BYOL/DINO-style. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Physics and the Brain</title>
      <link>https://mcbal.github.io/post/physics-and-the-brain/</link>
      <pubDate>Wed, 07 Apr 2021 14:17:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/physics-and-the-brain/</guid>
      <description>&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-non-equilibrium-dynamics-embracing-chaos&#34;&gt;Non-equilibrium dynamics: embracing chaos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-there-is-no-avoiding-the-c-word&#34;&gt;There is no avoiding the C-word&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-attention-world-models-and-hallucinations&#34;&gt;Attention, world models, and hallucinations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-at-least-two-realities&#34;&gt;At least two realities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-the-world-as-an-external-memory&#34;&gt;The world as an external memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#7-attention-as-the-dynamical-response-of-non-equilibrium-systems&#34;&gt;Attention as the dynamical response of non-equilibrium systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#8-&#34;&gt;&amp;hellip;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I collect some open-ended thoughts on how a physicist with an inclination towards &lt;a href=&#34;https://plato.stanford.edu/entries/computational-mind/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;computationalism&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_physics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;statistical physics&lt;/a&gt; might look at neural information-processing systems and the idea of modeling human-like perception and intelligence.&lt;/p&gt;
&lt;p&gt;We play crackpot bingo and touch on physics, consciousness, awareness, attention, world models, hallucinations, the nature of reality, active inference, and how a physicist might like models of attention to behave.&lt;/p&gt;
&lt;h2 id=&#34;2-non-equilibrium-dynamics-embracing-chaos&#34;&gt;2. Non-equilibrium dynamics: embracing chaos&lt;/h2&gt;
&lt;p&gt;Ask physicists what they think about the brain as a physical system and they&amp;rsquo;ll likely tell you it&amp;rsquo;s a wet and messy non-equilibrium statistical-mechanical system. A vanishingly small minority will claim quantum effects are important. A large majority will claim these systems intrude on their honed and delicate sense of beauty as they&amp;rsquo;re too far removed from the world of spherical cows and sterile toy models. If the physicists in your sample happen to like computational metaphors, you might also hear something like &amp;ldquo;emergent collective computational abilities arising from many interacting degrees of freedom&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;But they might all agree that what survives beyond the small scale of a great many neuron excitations, inhibitions, and modulations are large-scale patterns. Different scales in our effective descriptions of physical reality tend to decouple: you don&amp;rsquo;t need quantum field theory to describe ocean currents. It&amp;rsquo;s what makes physics possible.&lt;/p&gt;
&lt;p&gt;If the brain is a non-equilibrium statistical-mechanical system, then sensory and internal inputs are time-dependent &amp;ldquo;probes&amp;rdquo; injecting energy into the system. Energy that needs to be dissipated somehow, encouraging dynamical responses across spatiotemporal scales that continuously nudge and alter large-scale behavior. The brain self-organizes by embracing stochasticity and by never being quiet: there are always fluctuations to respond to and there is always energy to dissipate.&lt;/p&gt;
&lt;h2 id=&#34;3-there-is-no-avoiding-the-c-word&#34;&gt;3. There is no avoiding the C-word&lt;/h2&gt;
&lt;p&gt;Most of the brain&amp;rsquo;s computations happen unconsciously, leading to ephemeral signals that wither away. This is the realm of fast perception and the current generation of deep neural networks. According to neuroscientists like Dehaene, only relatively long-lasting flashes of coherent activity appear in consciousness and constitute what we experience as &amp;ldquo;awareness&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Even though consciousness feels like the space where thoughts, feelings, and subjective experiences appear, it could be interpreted as the mere presence of whatever large-scale, semi-stable pattern happens to be dominating at any one instant, perpetually influenced by the relentless unconscious nudging going on.&lt;/p&gt;
&lt;p&gt;Large-scale patterns are useful because they can synchronize and remnants of their activity can be manipulated slowly on time scales of seconds, minutes, and days, up until biological death. We need slow, conscious processing of these remnants to come up with responses to probes that are hard to automate within the structure of our brains, which has been shaped by evolutionary baggage and contingincies.&lt;/p&gt;
&lt;h2 id=&#34;4-attention-world-models-and-hallucinations&#34;&gt;4. Attention, world models, and hallucinations&lt;/h2&gt;
&lt;p&gt;Consciousness seems like an adaptive control mechanism for attention, affording us to fix on certain aspects of sensory perception and high-level mental representations while varying others, enabling us to reflect on the relation of our attention to these concepts and on attention itself. By processing information to better grasp our environment, we can exert control over our bodily appendages and the likelihood of passing on our genes.&lt;/p&gt;
&lt;p&gt;Why do we have subjective experiences at all? Maybe it&amp;rsquo;s the result of evolutionary pressure in a social context: empathy, sympathy, perspective taking, and theory of mind all require to feel what other selves feel. That&amp;rsquo;s much easier to model if you are able to feel yourself, not only sharing the biological wetware but also &amp;ldquo;certain flavours of hallucinations&amp;rdquo;. Behavior resulting from large-scale patterns can then be recognized, encoded, and transferred as symbols to the minds of other beings, transcending space and time through language and culture.&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;feeling of being aware&amp;rdquo; is just another hallucination, your self-story just another part of the grand narrative constructed inside your brain&amp;rsquo;s world model. Even though there is nothing to &amp;ldquo;feel&amp;rdquo; outside our skulls, it is still helpful and beneficial for everyone to pretend that our subjective experiences align most of the time. And while we&amp;rsquo;re at it: free will is a spectrum of illusions and discussing it hardly matters at all; just don&amp;rsquo;t be a dick.&lt;/p&gt;
&lt;h2 id=&#34;5-at-least-two-realities&#34;&gt;5. At least two realities&lt;/h2&gt;
&lt;p&gt;Outside of individual subjective experiences, there does seem to exist a physical environment that is shared and which all propagating things exploit to interact with and use as a communication channel for sending and receiving information. Morally and pragmatically, there&amp;rsquo;s also a shared social reality and you would be rightly considered a solipsistic psychopath to deny its existence.&lt;/p&gt;
&lt;p&gt;The crucial distinction to make here is between that &amp;ldquo;shared physical reality which we can never truly experience but only probe, observe, and approach indirectly&amp;rdquo; on the one hand and the &amp;ldquo;subjective conceptual experience built on our particular human sense data and shared concepts whose meaning has been agreed upon by convention over the course of millennia of cultural evolution&amp;rdquo; on the other hand.&lt;/p&gt;
&lt;p&gt;We can use language and human concepts to make the statement that the sun has been a flaming ball of nuclear fusion in the middle of our solar system for billions of years. That it feels like warmth, light, and source of all life. These concepts did not arrive until we arrived, made &amp;ldquo;conscious observations&amp;rdquo;, and turned them into symbols that can be shared with the minds of others. Sensual, emotional, and scientific descriptions are all ways to grasp at the impenetrable and to doggedly hold on to order.&lt;/p&gt;
&lt;h2 id=&#34;6-the-world-as-an-external-memory&#34;&gt;6. The world as an external memory&lt;/h2&gt;
&lt;p&gt;The dominant trend in deep generative modeling nowadays is to reproduce data in a pixel-perfect way, e.g. photo-realistic GANs. These are the incentives and metrics used to measure progress and evaluate state-of-the-art performance. Distilling the space of natural images into a pixel-perfect model is a solid strategy for developing a product or selling cloud computing credits, but it is arguably a lousy one to approach human-like intelligence.&lt;/p&gt;
&lt;p&gt;Biological systems are embedded in a dynamic environment and have experienced early on that it&amp;rsquo;s a complete waste of energy to try to reconstruct accurate, low-level representations of incoming sense data when there is a world out there relentlessly bombarding you with evidence. In a very real sense, the environment is always right there, engulfing the system, and ready to be used as an external, probeable, and malleable memory for low-level details.&lt;/p&gt;
&lt;p&gt;Learning in the brain is induced by perpetual interactions with the environment, nudging the dynamical response behavior of the system and modulating large-scale behavior over time. You cannot overfit if the ground beneath your feet is shifting all the time and you are forced to jump.&lt;/p&gt;
&lt;h2 id=&#34;7-attention-as-the-dynamical-response-of-non-equilibrium-systems&#34;&gt;7. Attention as the dynamical response of non-equilibrium systems&lt;/h2&gt;
&lt;p&gt;If the goal is to build a system with self-organized emergent collective computational capabilities arising from many interacting degrees of freedom, we could look at systems where there a lot of metastable non-equilibrium states with rugged and interesting free energy landscapes and dynamic responses on a wide range of time scales.&lt;/p&gt;
&lt;p&gt;Examples of such systems are spin glasses and disordered random systems, which show internal dynamics determined by random couplings between spin degrees of freedom. A random Ising model (or Boltzmann machine) defined for some spins in an external field looks something like&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;spin_0.png&#34; alt=&#34;alt text&#34; title=&#34;Random Ising spin model&#34;&gt;&lt;/p&gt;
&lt;p&gt;with an energy function&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(t) = - \left( \sum_{i,j} J_{ij} S_{i} S_{j} + \sum_{i} \theta_{i}(t) S_{i} \right),
\end{equation}&lt;/p&gt;
&lt;p&gt;where the couplings $J_{ij}$ are drawn from some probability distribution and the external fields $\theta_{i}(t)$ specify a &amp;ldquo;preferred direction&amp;rdquo;. Modulating the external fields $\theta_{i}(t)$ pushes the model to try to align with an incoming &amp;ldquo;data stream&amp;rdquo; through relaxation. If we treat the couplings as free parameters (effectively making them sort of time-dependent as well), we expect structure and organization of the connection graph to emerge through learning. The goal for the system is to learn how to handle being driven by incoming data.&lt;/p&gt;
&lt;p&gt;Learning can happen in at least two ways: (1) for a fixed network structure, we can dump energy in the system and have the system figure out a way to dissipate the energy through relaxation, and (2) the structure of a network itself can be adjusted via the couplings through free energy minimization. The system should never stop learning since stopping learning means death, rigor mortis, and decay.&lt;/p&gt;
&lt;p&gt;A toy neural network agent in this framework would look something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;spin_1.png&#34; alt=&#34;alt text&#34; title=&#34;Random Ising spin model with inputs and outputs designated&#34;&gt;&lt;/p&gt;
&lt;p&gt;where we again have a graph of spins with two-body interactions but have designated a node as a probe point (e.g. sense inputs) and two other nodes as outputs (e.g. motor commands and muscle nerves). All nodes are part of the computation and can talk to each other but don&amp;rsquo;t have to. The probe point receives local time-varying &amp;ldquo;magnetic fields&amp;rdquo; $\theta_{i}(t)$ at every timestep which inject energy (and useful information if its content is low-entropy enough) into the system. We picture this system to operate at a sufficiently high level, i.e. the sense inputs could be feature vectors coming from a convolutional neural network.&lt;/p&gt;
&lt;p&gt;Without any explicit learning by tuning the couplings weights, relaxation of the system can lead to self-organization of the system&amp;rsquo;s response across time scales&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. Additionally minimizing free energy on top could (1) adjust the interaction weights to nudge the internal dynamics of the system, and (2) if the output nodes receive gradients, the output nodes could adapt to minimize free energy. This last concept is known as &lt;a href=&#34;https://en.wikipedia.org/wiki/Free_energy_principle&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;active inference&lt;/a&gt; where an agent&amp;rsquo;s actions are adjusted to improve its current world model. You perform saccadic eye movements because doing so will instantaneously improve your hallucinated understanding of the world.&lt;/p&gt;
&lt;h2 id=&#34;8-&#34;&gt;8. &amp;hellip;&lt;/h2&gt;
&lt;p&gt;&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;I would like to thank L.G.P. for inspiring discussions.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;For an interesting toy example of this kind of behavior, see &lt;a href=&#34;https://youtu.be/vSgHuErXuqk?t=2188&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this talk&lt;/a&gt; on &lt;em&gt;Low rattling: a principle for understanding driven many-body self-organization&lt;/em&gt;. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
