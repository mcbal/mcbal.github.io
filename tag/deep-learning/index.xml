<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | mcbal</title>
    <link>https://mcbal.github.io/tag/deep-learning/</link>
      <atom:link href="https://mcbal.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>Matthias Bal Â© 2020</copyright><lastBuildDate>Tue, 22 Dec 2020 10:03:17 +0100</lastBuildDate>
    <image>
      <url>https://mcbal.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Deep Learning</title>
      <link>https://mcbal.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Transformer Attention as an Implicit Mixture of Effective Energy-Based Models</title>
      <link>https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/</link>
      <pubDate>Tue, 22 Dec 2020 10:03:17 +0100</pubDate>
      <guid>https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/</guid>
      <description>&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-attention-from-effective-energy-based-models&#34;&gt;Attention from effective energy-based models&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#restricted-boltzmann-machines&#34;&gt;Restricted Boltzmann Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#integrating-out-hidden-units&#34;&gt;Integrating out hidden units&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#effective-energies-and-correlations&#34;&gt;Effective energies and correlations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modern-hopfield-networks-as-mixtures-of-effective-rbms&#34;&gt;Modern Hopfield networks as mixtures of effective RBMs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-attention-as-implicit-energy-minimization&#34;&gt;Attention as implicit energy minimization&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#bending-the-explicit-architecture&#34;&gt;Bending the explicit architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#from-explicit-architectures-to-implicit-energy-minimization&#34;&gt;From explicit architectures to implicit energy minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-implicit-layers-for-attention-dynamics&#34;&gt;Deep implicit layers for attention dynamics&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;!-- In this post, I will try to partly address the concerns of the following critic:

&gt; _In your [previous post](https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/), you introduced the energy function of modern Hopfield networks without explanation. Where does it come from? What&#39;s up with the logarithm? Is there actually any other interpretation then it being reverse-engineered from the Transformers&#39; attention step? Is this all a desperate attempt to make Hopfield networks cool again? Also, I cannot see the value of looking at attention from an energy-based perspective if it doesn&#39;t help me achieve SOTA. Weak reject._ --&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;In a &lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt;, I provided an overview of attention in Transformer models and summarized its connections to modern Hopfield networks. We saw that the energy-based model
\begin{equation}
E(\boldsymbol{\Xi}; \boldsymbol{X}) = \frac{1}{2} \boldsymbol{\Xi}^T \boldsymbol{\Xi} -\mathrm{logsumexp} \left( \boldsymbol{X}^T \boldsymbol{\Xi} \right).
\label{eq:mhnenergy}
\end{equation}
enables fast pattern storage and retrieval through its simple and robust dynamics, leading to rapid convergence
\begin{align}
\boldsymbol{\Xi}_{n+1}  = \boldsymbol{X} \ \mathrm{softmax} \left( \boldsymbol{X}^T \boldsymbol{\Xi}_{n}\right)
\label{eq:mhnupdate}
\end{align}
of input queries $\boldsymbol{\Xi}_{n}$ to updated queries $\boldsymbol{\Xi}_{n+1}$ lying in the convex hull of stored patterns $\boldsymbol{X}$. I also argued by means of handwaving that optimizing a Transformer looks like meta-learning from the point of view of its attention modules, sculpting energy landscapes to accommodate statistical patterns found in data.&lt;/p&gt;
&lt;p&gt;The main goal of this post is to build on these insights and highlight how an energy-based perspective can be a useful, complementary approach towards improving attention-based neural network modules. Parallel to scaling compute and making (self-)attention more efficient, it might be worthwhile to try to scale learning itself by experimenting with radically different attention mechanisms.&lt;/p&gt;
&lt;p&gt;To this end, we will first revisit ancient ideas at the boundary of statistical physics and machine learning and show how vanilla attention looks like a mixture of simple energy-based models. We will then argue how going beyond these simple models could benefit from thinking in terms of implicit instead of explicit attention modules, suggesting opportunities to put ideas from &lt;a href=&#34;https://implicit-layers-tutorial.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Layers&lt;/a&gt; to work.&lt;/p&gt;
&lt;h1 id=&#34;2-attention-from-effective-energy-based-models&#34;&gt;2. Attention from effective energy-based models&lt;/h1&gt;
&lt;p&gt;In this section, we will introduce &lt;a href=&#34;https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machines&lt;/a&gt; as a particular class of energy-based models, focusing on their capacity to capture effective correlations. After identifying classical discrete Hopfield networks and modern discrete Hopfield networks, we will demonstrate a naive way to fit modern continuous Hopfield networks into this framework. Throughout this section, we will rely heavily on the wonderful review &lt;a href=&#34;https://arxiv.org/abs/1803.08823&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A high-bias, low-variance introduction to machine learning for physicists&lt;/a&gt; by &lt;a href=&#34;https://arxiv.org/abs/1803.08823&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mehda et al.&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;restricted-boltzmann-machines&#34;&gt;Restricted Boltzmann Machines&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Restricted Boltzmann Machine&lt;/a&gt; (RBM) is an &lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/#energy-based-models-a-gentle-introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;energy-based model&lt;/a&gt; with a bipartite structure imposed on visible and hidden degrees of freedom: visible and hidden degrees of freedom interact with each other but do not interact among themselves (this is the &amp;ldquo;restriction&amp;rdquo;). The energy function looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
E \left( \boldsymbol{v}, \boldsymbol{h} \right) = - \sum_{i} a_{i} (v_{i}) - \sum_{\mu} b_{\mu} (h_{\mu}) - \sum_{i \mu} W_{i \mu} v_{i} h_{\mu},
\end{equation}&lt;/p&gt;
&lt;p&gt;where the matrix $W_{i \mu}$ encodes the coupling between hidden and visible units and where $a_{i} (\cdot)$ and $b_{\mu} (\cdot)$ are functions that can be chosen at will. Popular options are:&lt;/p&gt;
&lt;p&gt;\begin{align}
a_{i} (\cdot) =
\begin{cases}
a_{i} v_{i} &amp;amp; \text{if $v_{i} \in {0,1}$ is binary (Bernouilli)}\\&lt;br&gt;
\frac{v_{i}^2}{2\sigma_{i}^{2}} &amp;amp; \text{if $v_{i} \in \mathbb{R}$ is continuous (Gaussian)}\&lt;br&gt;
\end{cases} &lt;br&gt;
\end{align}&lt;/p&gt;
&lt;p&gt;and similar for $b_{\mu} (\cdot)$.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1803.08823&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;rbm.png&#34; alt=&#34;alt text&#34; title=&#34;Structure of a Restricted Boltzmann Machine&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;why-hidden-units&#34;&gt;Why hidden units?&lt;/h2&gt;
&lt;p&gt;Introducing hidden or latent variables is a powerful technique to encode interactions between visible units. Complex correlations between visible units can be captured at the cost of introducing new degrees of freedom and letting them interact with visible units in a simpler way. Since this trick often relies on exploiting &lt;a href=&#34;https://en.wikipedia.org/wiki/Common_integrals_in_quantum_field_theory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaussian integral identities&lt;/a&gt; and physicists like their Gaussians, it shows up in several places across physics, e.g. in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hubbard%E2%80%93Stratonovich_transformation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hubbard-Stratonovich transformation&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Renormalization group&lt;/strong&gt;: Rather than trying to fix the interactions in the &amp;ldquo;microscopic theory&amp;rdquo; like is done in the modeling scenario above, physicists are more familiar with the &amp;ldquo;reverse&amp;rdquo; procedure of deducing what effective theory emerges at large scales from a given microscopic theory. Indeed, integrating out degrees of freedom in physical theories can lead to complex, effective interactions between remaining degrees of freedom. This insight crystallized in the development of &lt;a href=&#34;https://en.wikipedia.org/wiki/Renormalization_group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;renormalization group&lt;/a&gt; theory in the early 1970s. By focusing on theories defined at different length scales, &lt;a href=&#34;https://en.wikipedia.org/wiki/Kenneth_G._Wilson&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kenneth G. Wilson&lt;/a&gt; and his contemporaries introduced and unified the notions of flows, fixed points, and universilty in theory space to understand the behavior of physical systems under a change of scale.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As we will see in the next sections, the bipartite structure of RBMs enables pairwise and higher-order correlations to emerge between visible units after integrating out hidden units. Additionally, the conditional independence of visible and hidden units enables tractable training methods like (block) Gibbs sampling and contrastive divergence&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. We will not consider explicitly training RBMs in this post but will instead reflect on the idea of implicitly training these models, which is what seems to be happening inside Transformers.&lt;/p&gt;
&lt;h2 id=&#34;effective-energies-and-correlations&#34;&gt;Effective energies and correlations&lt;/h2&gt;
&lt;p&gt;Let us now consider what kind of correlations between visible degrees of freedom are supported by RBMs. The distribution of the visible degrees of freedom can be obtained by marginalizing over the hidden degrees of freedom:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p \left( \boldsymbol{v} \right) = \int \mathrm{d} \boldsymbol{h} \  p \left( \boldsymbol{v}, \boldsymbol{h} \right) = \int \mathrm{d} \boldsymbol{h} \  \frac{\mathrm{e}^{- E \left( \boldsymbol{v}, \boldsymbol{h} \right)}}{Z}
\end{equation}&lt;/p&gt;
&lt;p&gt;We try to find an expression for the marginalized energy $E (\boldsymbol{v})$ by defining&lt;/p&gt;
&lt;p&gt;\begin{equation}
p \left( \boldsymbol{v} \right) = \frac{\mathrm{e}^{- E (\boldsymbol{v})}}{Z}
\end{equation}&lt;/p&gt;
&lt;p&gt;so that we can identify&lt;/p&gt;
&lt;p&gt;\begin{align}
E \left( \boldsymbol{v} \right) &amp;amp;= - \mathrm{log} \int \mathrm{d} \boldsymbol{h} \  \mathrm{e}^{- E \left( \boldsymbol{v}, \boldsymbol{h} \right)} \\&lt;br&gt;
&amp;amp;= - \sum_{i} a_{i} (v_{i}) - \sum_{\mu} \log \int \mathrm{d} h_{\mu}\ \mathrm{e}^{b_{\mu}(h_{\mu}) + \sum_{i} W_{i\mu} v_{i} h_{\mu}} \label{eq:effvisenergy}
\end{align}&lt;/p&gt;
&lt;p&gt;Following &lt;a href=&#34;https://arxiv.org/abs/1803.08823&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mehda et al.&lt;/a&gt;, we can try to better understand the correlations in $p(\boldsymbol{v})$ by introducing the (prior) distribution&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_{\mu} \left( h_{\mu} \right) = \frac{\mathrm{e}^{b_{\mu} (h_{\mu})}}{Z}
\end{equation}&lt;/p&gt;
&lt;p&gt;for the hidden units $h_{\mu}$, ignoring the interactions between $\boldsymbol{v}$ and $\boldsymbol{h}$. Additionally, we can introduce the hidden unit&amp;rsquo;s distribution&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulant generating function&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;\begin{align}
K_{\mu} (t) &amp;amp;= \mathrm{log}\ \mathbb{E} \left[ \mathrm{e}^{t h_{\mu}} \right] \\&lt;br&gt;
&amp;amp;= \mathrm{log} \int \mathrm{d} h_{\mu} \  q_{\mu} \left( h_{\mu} \right) \mathrm{e}^{t h_{\mu}}\\&lt;br&gt;
&amp;amp;= \sum_{n=1}^{\infty} \kappa_{\mu}^{(n)} \frac{t^{n}}{n!},
\end{align}&lt;/p&gt;
&lt;p&gt;which is defined such that the $n^{\mathrm{th}}$ cumulant $\kappa_{\mu}^{(n)}$ of $q_{\mu} \left( h_{\mu} \right)$ can be obtained by taking derivatives $\kappa_{\mu}^{(n)} = \partial_{t}^{n} K_{\mu} \rvert_{t=0}$.&lt;/p&gt;
&lt;p&gt;Looking back at the effective energy function \eqref{eq:effvisenergy} for the visible units, we find that the effective energy can be expressed in terms of cumulants:&lt;/p&gt;
&lt;p&gt;\begin{align}
E \left( \boldsymbol{v} \right) &amp;amp;= - \sum_{i} a_{i} \left(v_{i}\right) - \sum_{\mu} K_{\mu} \left( \sum_{i} W_{i\mu} v_{i} \right) \\&lt;br&gt;
&amp;amp;= - \sum_{i} a_{i} \left(v_{i}\right) - \sum_{\mu} \sum_{n=1}^{\infty} \kappa_{\mu}^{(n)} \frac{\left( \sum_{i} W_{i\mu} v_{i} \right)^{n}}{n!} \\&lt;br&gt;
&amp;amp;= - \sum_{i} a_{i} \left(v_{i}\right) - \sum_{i} \left( \sum_{\mu} \kappa_{\mu}^{(1)} W_{i\mu} \right) v_{i} \\&lt;br&gt;
&amp;amp;\ \ \ \ \ - \frac{1}{2} \sum_{ij} \left( \sum_{\mu} \kappa_{\mu}^{(2)} W_{i\mu} W_{j\mu} \right) v_{i} v_{j} + \ldots \label{eq:effectivenergy}
\end{align}&lt;/p&gt;
&lt;p&gt;We see that the auxiliary, hidden degrees of freedom induce effective pairwise and higher-order correlations among visible degrees of freedom. Each hidden unit $h_{\mu}$ can encode interactions of arbitrarily high order, with the $n$-th order cumulants of $q_{\mu} \left( h_{\mu} \right)$ weighting the $n$-th order interactions. By combining many hidden units and/or stacking layers, RBMs can in principle encode complex interactions at all orders and learn them from data.&lt;/p&gt;
&lt;p&gt;Let us now recover some known models by picking a suitable prior distribution for the hidden units:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classical discrete Hopfield networks&lt;/strong&gt;: Consider a Bernouilli distribution for the visible units and a standard Gaussian distribution for the hidden units. For a standard Gaussian, the mean $\kappa_{\mu}^{(1)} = 0$, the variance $\kappa_{\mu}^{(2)} = 1$, and $\kappa_{\mu}^{(n)} = 0$, $\forall n\geq 3$, leading to the quadratic energy function of Hopfield networks:
\begin{align}
E \left( \boldsymbol{v} \right) = - \sum_{i} a_{i} v_{i} - \frac{1}{2} \sum_{ij} \left( \sum_{\mu} W_{i\mu} W_{j\mu} \right) v_{i} v_{j}
\end{align}&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modern discrete Hopfield networks&lt;/strong&gt;: Consider a Bernouilli distribution for the visible units. Since it can be shown that the normal distribution is the only distribution whose cumulant generating function is a polynomial, i.e. the only distribution having a finite number of non-zero cumulants&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, it looks like we cannot model a finite amount of polynomial interactions in this framework. But we can model an exponential interaction by considering a Poisson distribution $\mathrm{Pois}(\lambda)$ with rate $\lambda=1$ for the hidden units, whose cumulants are all equal to the rate, i.e. $\kappa_{\mu}^{(n)} = 1$, $\forall n\geq 1$. Up to a constant, we then obtain an exponential interaction
\begin{align}
E \left( \boldsymbol{v} \right) = - \sum_{i} a_{i} v_{i} - \sum_{\mu} \exp \left( \sum_{i} W_{i\mu} v_{i} \right)
\end{align}&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other kinds of effective interactions can be obtained by substituting the cumulants of your favorite probability distribution. The &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution#Higher_moments_and_cumulants&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cumulants of hidden Bernouilli units&lt;/a&gt; induce interactions of all orders. Considering exponential or Laplacian distributions where $\kappa^{(n)} \sim (n-1)!$ seems to lead to funky logarithmic interactions.&lt;/p&gt;
&lt;h2 id=&#34;modern-hopfield-networks-as-mixtures-of-effective-rbms&#34;&gt;Modern Hopfield networks as mixtures of effective RBMs&lt;/h2&gt;
&lt;p&gt;Let us now turn to the energy function of modern Hopfield networks for a single query $\boldsymbol{\xi} \in \mathbb{R}^{d}$ and $N$ stored patterns encoded by $\boldsymbol{X} \in \mathbb{R}^{d \times N}$,
\begin{equation}
E(\boldsymbol{\xi}; \boldsymbol{X}) = \frac{1}{2} \boldsymbol{\xi}^T \boldsymbol{\xi} -\mathrm{logsumexp} \left( \boldsymbol{X}^T \boldsymbol{\xi} \right),
\end{equation}
which we can transform into the RBM notation of the previous section by changing the names of variables and transposing the stored pattern matrix,
\begin{equation}
E(\boldsymbol{v}; W) = \frac{1}{2} \sum_{i} v_{i}^{2} -\log \left( \sum_{\mu} \exp \left( \sum_{i} W_{\mu i} v_{i} \right) \right).
\end{equation}&lt;/p&gt;
&lt;p&gt;Is there a simple way to interpret this energy function in terms of (effective) RBMs? Let&amp;rsquo;s imagine this energy to be an effective energy $E(\boldsymbol{v})$ for the visible units with probability distribution
\begin{equation}
p(\boldsymbol{v}) = \frac{\mathrm{e}^{-E(\boldsymbol{v})}}{Z} = \frac{1}{Z} \sum_{\mu} \mathrm{e}^{-\frac{1}{2} \sum_{i} v_{i}^{2} + \sum_{i} W_{\mu i} v_{i}},
\end{equation}
where the partition function $Z$ follows from doing a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_integral#n-dimensional_with_linear_term&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gaussian integral&lt;/a&gt;
\begin{equation}
Z = (2\pi)^{n/2} \sum_{\mu} Z_{\mu} = (2\pi)^{n/2} \sum_{\mu} \mathrm{e}^{\frac{1}{2} \sum_{i} W_{\mu i} W_{i\mu}}
\end{equation}&lt;/p&gt;
&lt;p&gt;We can then identify the probability distribution $p(\boldsymbol{v})$ with a mixture of effective energy-based models&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
\begin{equation}
p(\boldsymbol{v}) = \sum_{\mu} w_{\mu} \frac{\mathrm{e}^{-\frac{1}{2} \sum_{i} v_{i}^{2} + \sum_{i} \mathbf{W}_{\mu i} v_{i}}}{Z_{\mu}} = \sum_{\mu} w_{\mu} \frac{ \mathrm{e}^{ -E_{\mu}(\boldsymbol{v}) }}{Z_{\mu}}
\end{equation}
where $w_{\mu} = Z_{\mu} / Z$ so that $\sum_{\mu} w_{\mu} = 1$. During training, the model can control prior weights $w_{\mu}$ by adjusting relative norms of patterns. If the difference in norms between the stored patterns is not too wild, $w_{\mu} \approx 1/N$.&lt;/p&gt;
&lt;p&gt;A single model in the mixture has an effective energy function derived from a joint energy function with just a single hidden unit,&lt;/p&gt;
&lt;p&gt;\begin{equation}
E_{\mu} \left( \boldsymbol{v}, h_{\mu} \right) = - \sum_{i} a_{i} (v_{i}) - b_{\mu} (h_{\mu}) - \sum_{i} W_{i \mu} v_{i} h_{\mu}
\end{equation}&lt;/p&gt;
&lt;p&gt;Looking back at \eqref{eq:effectivenergy}, we see that we can recover $E_{\mu}(\boldsymbol{v})$ by picking a hidden prior distribution that is a constant random variable so that $\kappa_{\mu}^{(1)}=1$ is the only non-zero cumulant. This frozen property of hidden units seems to agree with the fast dynamics of memory neurons in the dynamical systems model proposed in &lt;a href=&#34;https://arxiv.org/abs/2008.06996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Krotov and Hopfield (2020)&lt;/a&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;In conclusion, the energy-based model underlying vanilla Transformer attention is not terribly exciting.&lt;/p&gt;
&lt;h1 id=&#34;3-attention-as-implicit-energy-minimization&#34;&gt;3. Attention as implicit energy minimization&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s finish this post with some comments on how one could leverage the idea of implicit energy minimization to develop novel attention mechanisms.&lt;/p&gt;
&lt;h2 id=&#34;bending-the-explicit-architecture&#34;&gt;Bending the explicit architecture&lt;/h2&gt;
&lt;p&gt;A lot of work on post-vanilla Transformer architectures tries to improve &lt;code&gt;softmax&lt;/code&gt;-attention by making it more efficient through approximations and/or modifications at the level of the architecture. Kernel-based approaches like &lt;a href=&#34;https://arxiv.org/abs/2009.14794&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rethinking Attention with Performers&lt;/a&gt; have shown not only that &lt;code&gt;softmax&lt;/code&gt; attention can be efficiently approximated by a generalized attention mechanism but also that generalized &lt;code&gt;ReLU&lt;/code&gt;-based attention performed better in practice. Papers like &lt;a href=&#34;https://arxiv.org/abs/2005.09561&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Normalized Attention Without Probability Cage&lt;/a&gt; show how we can replace the &lt;code&gt;softmax&lt;/code&gt; non-linearity in \eqref{eq:mhnupdate} with pure normalization and still end up with a competitive algorithm, noting that the updated query being restricted to lie in the convex hull of the stored patterns is a bias we might want to question.&lt;/p&gt;
&lt;p&gt;From the above examples, it seems like at least a part of current research on attention is trying to break away from the confines of existing, explicit attention architectures but doesn&amp;rsquo;t quite know how to do so in a principled way. Does an energy-based perspective help to understand these developments?&lt;/p&gt;
&lt;h2 id=&#34;from-explicit-architectures-to-implicit-energy-minimization&#34;&gt;From explicit architectures to implicit energy minimization&lt;/h2&gt;
&lt;p&gt;We have seen in this post that the energy function behind the &lt;code&gt;softmax&lt;/code&gt; attention mechanism can be understood as a mixture of simple energy-based models. But what can we actually do with this information? Especially since we know from language modeling experiments that &amp;ldquo;just scaling&amp;rdquo; these simple models to billions of parameters enables them to store enough patterns to be useful. Despite huge progress, there however remain important challenges in terms of efficiency and generalizability. Considering slightly less trivial energy-based models might address both by adding interactions in such a way that attention modules are able to return a &lt;em&gt;collective response&lt;/em&gt; rather than a sum of decoupled contributions.&lt;/p&gt;
&lt;p&gt;To some extent, the additional linear transformations on the input patterns in the query-key-value formulation of Transformer self-attention already try to address this:
\begin{equation}
\mathrm{Attention}\left( \mathbf{Q}, \mathbf{K}, \mathbf{V} \right) = \mathrm{softmax} \left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d}} \right) \mathbf{V}
\label{eq:vanilla-attention}
\end{equation}
These linear transformations slightly generalize the &amp;ldquo;naked&amp;rdquo; explicit gradient step of \eqref{eq:mhnupdate} and can in principle learn to cluster and direct patterns to neighborhoods in the energy landscape, parametrizing the energy function. But why stop there?&lt;/p&gt;
&lt;h2 id=&#34;deep-implicit-layers-for-attention-dynamics&#34;&gt;Deep implicit layers for attention dynamics&lt;/h2&gt;
&lt;p&gt;An interesting way forward might be to integrate attention with &lt;em&gt;deep implicit layers&lt;/em&gt;. Funnily enough, the authors of the NeurIPS 2020 tutorial on &lt;a href=&#34;https://implicit-layers-tutorial.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Implicit Layers&lt;/a&gt; list self-attention as a prime example of an explicit layer in their &lt;a href=&#34;https://colab.research.google.com/drive/1OUVzeUh66wVOFI_Nc_rIAuO70gHimHH8?usp=sharing#scrollTo=vFlF3gTnzOpp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;introductory notebook&lt;/a&gt;. Approaches like &lt;a href=&#34;https://arxiv.org/abs/1909.01377&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Equilibrium Models&lt;/a&gt; implicitly train DEQ-Transformers but still consider the attention module itself an explicit function.&lt;/p&gt;
&lt;p&gt;Yet we have seen in a &lt;a href=&#34;https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous post&lt;/a&gt; that self-attention can &amp;mdash; and perhaps should &amp;mdash; actually be considered an implicit layer solving for a fixed point query. Because of the lack of dynamics of the current generation of attention mechanisms, this can be done in a single big gradient step, removing the need to iterate. Attention models with more complicated dynamics might benefit from a differentiable solver to find a fixed point and return the most appropriate result in a given context.&lt;/p&gt;
&lt;p&gt;Compared to modifying explicit architectures, the implicit-layer perspective seems to act on a different &amp;ldquo;conceptual level&amp;rdquo; of neural network architecture design. This raises a lot of questions. Which families of attention architectures can be expressed in terms of implicit energy functions like &lt;code&gt;softmax&lt;/code&gt;-attention? How many of these have efficient minimization properties with closed-form gradients? Beyond closed-form gradients, how far can we go in parametrizing more general energy-based attention models and still end up with an efficient algorithm? What does the trade-off look like between an attention model&amp;rsquo;s complexity and it still being implicitly trainable?&lt;/p&gt;
&lt;h1 id=&#34;4-conclusion&#34;&gt;4. Conclusion&lt;/h1&gt;
&lt;p&gt;Looking back and reversing causation, one could argue that the now-famous dot-product attention module introduced in &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; could only have been arrived at because of the properties of its implicit energy function \eqref{eq:mhnenergy}. Indeed, it is only because of the associative memory&amp;rsquo;s decoupled and rather crude way of storing patterns in isolated, high-dimensional valleys that expensive, implicit energy minimization steps can be traded for a cheap, explicit one-step gradient update like \eqref{eq:mhnupdate}.&lt;/p&gt;
&lt;p&gt;The obvious pitfall of continuing to hold on to the conceptual framework introduced by this shortcut is that a potentially far richer picture of (sparse) attention dynamics remains obscured. Rather than perpetually rethinking what is all you &lt;em&gt;really&lt;/em&gt; need within the confines of existing, explicit attention modules, why not opt for implicit modules built on top of an energy-based perspective to try to push things forward?&lt;/p&gt;
&lt;p&gt;In a next post, we might have a go at &lt;a href=&#34;https://github.com/mcbal/deep-implicit-attention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implementing&lt;/a&gt; a few implicit attention mechanisms ourselves. Most likely though, we will observe how they all perform worse than the current generation of simpler modules when scaled to industrial levels of compute&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h1 id=&#34;references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/h1&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, David J. Schwab, &lt;a href=&#34;https://arxiv.org/abs/1803.08823&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A high-bias, low-variance introduction to Machine Learning for physicists&lt;/a&gt; (2019)&lt;/em&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Proof by Marcinkiewicz (1935) according to &lt;a href=&#34;http://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf&#34;&gt;http://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf&lt;/a&gt;. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;We are aware that this identification might be tremendously trivial when considering prior work on &lt;a href=&#34;https://papers.nips.cc/paper/2008/hash/e820a45f1dfc7b95282d10b6087e11c0-Abstract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Implicit Mixtures of Restricted Boltzmann Machines&lt;/a&gt; or, more generally, mixture models in the context of &lt;a href=&#34;https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;expectation-minimization optimization&lt;/a&gt;. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Dmitry Krotov and John Hopfield, &lt;a href=&#34;https://arxiv.org/abs/2008.06996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large Associative Memory Problem in Neurobiology and Machine Learning&lt;/a&gt; (2020)&lt;/em&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt; (2017)&lt;/em&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Rich Sutton, &lt;a href=&#34;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bitter Lesson&lt;/a&gt; (2019)&lt;/em&gt; &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>An Energy-Based Perspective on Attention Mechanisms in Transformers</title>
      <link>https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/</link>
      <pubDate>Sat, 28 Nov 2020 10:54:21 +0100</pubDate>
      <guid>https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/</guid>
      <description>&lt;p align=&#34;center&#34;&gt;
  &lt;a href=&#34;https://xkcd.com/793/&#34;&gt;XKCD 793: A physicist encountering machine learning for the first time&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-a-growing-zoo-of-transformers&#34;&gt;A growing zoo of Transformers&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#vanilla-transformers&#34;&gt;Vanilla Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#beyond-vanilla-confronting-quadratic-scaling&#34;&gt;Beyond vanilla: confronting quadratic scaling&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-from-hopfield-networks-to-transformers&#34;&gt;From Hopfield networks to Transformers&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#classical-discrete-hopfield-networks&#34;&gt;Classical discrete Hopfield networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modern-discrete-hopfield-networks&#34;&gt;Modern discrete Hopfield networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modern-continuous-hopfield-networks&#34;&gt;Modern continuous Hopfield networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modern-continuous-hopfield-networks-as-energy-based-models&#34;&gt;Modern continuous Hopfield networks as energy-based models&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#energy-based-models-a-gentle-introduction&#34;&gt;Energy-based models: a gentle introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exactly-optimizing-modern-continuous-hopfield-networks&#34;&gt;Exactly optimizing modern continuous Hopfield networks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#transformers-store-and-retrieve-context-dependent-patterns&#34;&gt;Transformers store and retrieve context-dependent patterns&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#where-are-patterns-stored-in-a-transformer&#34;&gt;Where are patterns stored in a Transformer?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-training-transformers&#34;&gt;Training Transformers&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#pretraining-loss-functions&#34;&gt;Pretraining loss functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#stepping-through-the-transformer-implicit-energy-minimization&#34;&gt;Stepping through the Transformer: implicit energy minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#meta-learning-and-few-shot-inference&#34;&gt;Meta-learning and few-shot inference&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-beyond-dot-product-attention&#34;&gt;Beyond dot-product attention&lt;/a&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#attention-dynamics-embracing-collective-phenomena&#34;&gt;Attention dynamics: embracing collective phenomena&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-very-long-sequences-should-not-be-needed&#34;&gt;Why very long sequences should not be needed&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;
&lt;p&gt;In 2017, &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; demonstrated state-of-the-art performance in neural machine translation by stacking only (self-)attention layers. Compared to recurrent neural networks, Transformer models exhibit efficient parallel processing of tokens, leading to better modeling of long-range correlations and, most importantly, &lt;a href=&#34;https://arxiv.org/abs/2001.08361&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;favorable scaling in terms of data and compute&lt;/a&gt;. Since then, Transformers seem to have taken over natural language processing. Widespread adoption of attention-based architectures seems likely given recent work like &lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt; and the flurry of developments addressing the architecture&amp;rsquo;s quadratic scaling bottlenecks.&lt;/p&gt;
&lt;p&gt;Recently, the papers &lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need&lt;/a&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; and &lt;a href=&#34;https://arxiv.org/abs/2008.06996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large Associative Memory Problem in Neurobiology and Machine Learning&lt;/a&gt; &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; provided complementary post-facto explanations of some of the success of Transformers from the perspective of energy-based models. In this post, I provide a biased overview of (self-)attention in Transformers and summarize its connections to modern Hopfield networks. Along the way, I look for intuition from physics and indulge in hand-wavy arguments on how an energy-based perspective can shed light on training and improving Transformer models.&lt;/p&gt;
&lt;h1 id=&#34;2-a-growing-zoo-of-transformers&#34;&gt;2. A growing zoo of Transformers&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s start off with an overview of the components in a vanilla Transformer model. Since our focus is on (self-)attention, I am going to assume some prior knowledge&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; and skip comprehensive architecture descriptions and experimental results. In &lt;a href=&#34;#3-from-hopfield-networks-to-transformers&#34;&gt;Section 3&lt;/a&gt;, we will start from scratch and use Hopfield networks to build back up to the attention module described below.&lt;/p&gt;
&lt;h2 id=&#34;vanilla-transformers&#34;&gt;Vanilla Transformers&lt;/h2&gt;
&lt;p&gt;The proto-Transformer was introduced in an encoder-decoder context for machine translation in &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt;. The original motivation seems to have been mostly driven by engineering efforts to model long-range correlations in sequence data and the recent successes of attention mechanisms stacked on top of recurrent neural networks. The main contribution and selling point of the paper was making an attention-only approach to sequence modeling work.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;vanilla_transformer.png&#34; alt=&#34;alt text&#34; title=&#34;Vanilla Transformers encoder-decoder architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s focus on the encoder on the left and ignore the decoder on the right. Transformer models accept (batches of) sets of vectors, which covers most inputs people care about in machine learning. Text can be modelled as a sequence of embedded tokens. Images can be viewed as a snaky sequence of embedded pixels or embedded patches of pixels. Since sets have no notion of ordering, learned or fixed positional information needs to be explicitly added to the input vectors.&lt;/p&gt;
&lt;p&gt;The main module in the Transformer encoder block is the multi-head &lt;em&gt;self-attention&lt;/em&gt;, which is based on a (scaled) dot-product attention mechanism acting on a set of $d$-dimensional vectors:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathrm{Attention}\left( \mathbf{Q}, \mathbf{K}, \mathbf{V} \right) = \mathrm{softmax} \left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d}} \right) \mathbf{V}
\label{eq:vanilla-attention}
\end{equation}&lt;/p&gt;
&lt;p&gt;Here, queries $\mathbf{Q}$, keys $\mathbf{K}$, and values $\mathbf{V}$ are matrices obtained from acting with different linear transformations &amp;mdash; parametrized respectively by weights $\mathbf{W}_{\mathbf{Q}}$, $\mathbf{W}_{\mathbf{K}}$, and $\mathbf{W}_{\mathbf{V}}$ &amp;mdash; on the same set of $d$-dimensional inputs. &lt;em&gt;Cross-attention&lt;/em&gt; takes the inputs for its queries from a different source than for its keys and values, as can be glimpsed from the decoder part of the architecture on the right.&lt;/p&gt;
&lt;p&gt;For every input query, the updated output query of \eqref{eq:vanilla-attention} is a linear combination of values weighted by an attention vector quantifying the overlap of the input query with the keys corresponding to these values. Stacking input query attention vectors leads to an attention matrix. Since all objects are vectors and the attention mechanism is just a dot product between vectors, we can think of the attention module as matching query vectors to their &amp;ldquo;closest&amp;rdquo; key vectors in latent space and summing up contributions from value vectors, weighted by the &amp;ldquo;closeness&amp;rdquo; of their keys to the queries.&lt;/p&gt;
&lt;p&gt;The remaining components of the Transformer encoder block are needed to make the module work properly in practice:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;multi-headedness&lt;/em&gt; of the attention module refers to chunking up the dimension of the vector space and having multiple attention operations running in parallel in the same module, yet with each acting on a lower-dimensional segment of the full space. This is a trick to (1) get around the fact that every input vector only couples to one query at a time to calculate its attention coefficient, and (2) provide multiple starting points in the subspaces for the queries, which might help to avoid bad local minima in parameter space during optimization.&lt;/li&gt;
&lt;li&gt;A positional feed-forward network, made up of two linear layers with a non-linearity in between, is inserted at the end of the module. Folklore wisdom tells us that the feed-forward layer needs to blow up the dimension of the latent space by a factor of four for it to be able to &amp;ldquo;disentangle&amp;rdquo; the represention. More likely though, it&amp;rsquo;s a way to increase model capacity and warp latent spaces since the attention modules on their own are pretty much linear apart from the $\mathrm{softmax}$-operator used to obtain the normalized attention coefficients.&lt;/li&gt;
&lt;li&gt;Residual connections are added to control the flow of gradients.&lt;/li&gt;
&lt;li&gt;Layer normalisation is used to control learning dynamics and keep vector norms from exploding.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;beyond-vanilla-confronting-quadratic-scaling&#34;&gt;Beyond vanilla: confronting quadratic scaling&lt;/h2&gt;
&lt;p&gt;Most architectural variations of the vanilla Transformer are targeted at the attention module, which scales poorly with respect to the input sequence length $N$. Since the overlap of all queries with all keys is required, calculating a dense attention matrix scales like $\mathcal{O}(N^2)$ in time and space. Limits on the context window of the attention mechanism during training prevent the model from learning how to deal with long sequences and long-range correlations. The majority of post-vanilla Transformer species can be classified into one of the following buckets&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Low-rank approximations: truncate the matrix product $\mathbf{Q} \mathbf{K}^T$ since it&amp;rsquo;s likely not full rank for structured data&lt;/li&gt;
&lt;li&gt;Sparsification: reduce the attention calculation from all query-key pairs to a subset because not all of them feel the need to talk to each other&lt;/li&gt;
&lt;li&gt;Recurrence: keep track of a (compressed) history of context&lt;/li&gt;
&lt;li&gt;Kernels: approximate the attention operation with kernel methods&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the remainder of our discussion, we will focus on vanilla Transformers. One of the goals of this blog post is to explore how a different perspective on the &lt;em&gt;function&lt;/em&gt; of attention-based algorithms might lead to qualitatively different improvements beyond what is possible by relying on scaling and reducing computational complexity alone.&lt;/p&gt;
&lt;h1 id=&#34;3-from-hopfield-networks-to-transformers&#34;&gt;3. From Hopfield networks to Transformers&lt;/h1&gt;
&lt;p&gt;In this section, we provide a short history of Hopfield networks and gradually build up intuition until we can recognize the Transformer self-attention mechanism for what it really is. We refer to the &lt;a href=&#34;https://ml-jku.github.io/hopfield-layers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt; accompanying &lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need&lt;/a&gt; for more details and insightful visualizations of pattern storage and retrieval.&lt;/p&gt;
&lt;h2 id=&#34;classical-discrete-hopfield-networks&#34;&gt;Classical discrete Hopfield networks&lt;/h2&gt;
&lt;p&gt;A &lt;a href=&#34;https://en.wikipedia.org/wiki/Hopfield_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield network&lt;/a&gt; is a simple model for associative memory popularized by John Hopfield in his 1982 paper &lt;a href=&#34;https://www.pnas.org/content/pnas/79/8/2554.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Networks and Physical Systems with Emergent Collective Computational Abilities&lt;/a&gt;&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;. The task of an associative memory is to store and retrieve patterns, preferably in a way that allows one to recover stored patterns quickly with a low error rate.&lt;/p&gt;
&lt;p&gt;The basic idea of the Hopfield network &amp;mdash; and other energy-based models like &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boltzmann machines&lt;/a&gt; &amp;mdash; is to construct an &lt;em&gt;energy function&lt;/em&gt; which defines an &lt;em&gt;energy landscape&lt;/em&gt; containing basins of attraction around patterns we want to store. Starting at any pattern, we want to have an update rule pointing towards the closest stored pattern, guided by a scalar &amp;ldquo;closeness&amp;rdquo; score provided by the energy function.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Hopfield_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;img src=&#34;energy_landscape.png&#34; alt=&#34;alt text&#34; title=&#34;Toy energy landscape of a Hopfield Network&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s make this a bit more formal but not too formal. Consider trying to store a set of $N$ binary patterns $\{\boldsymbol{x}_{i}\}_{i=1}^{N}$ where each pattern $\boldsymbol{x}_{i}$ is a $d$-dimensional vector whose entries are either $-1$ or $1$. For example, in the case of storing black-and-white images, every image would correspond to a string of pixel values, a binary pattern $\boldsymbol{x}_{i}$.&lt;/p&gt;
&lt;p&gt;For any query $\boldsymbol{\xi} \in \mathbb{R}^{d}$, or &lt;em&gt;state pattern&lt;/em&gt;, we want to find a way to retrieve the closest &lt;em&gt;stored pattern&lt;/em&gt;. In his paper, Hopfield considered the energy function&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = - \frac{1}{2} \boldsymbol{\xi}^{T} \boldsymbol{W} \boldsymbol{\xi} + \boldsymbol{\xi}^{T} \boldsymbol{b} = - \frac{1}{2} \sum_{i=1}^{d} \sum_{j=1}^{d} w_{ij} \xi_{i} \xi_{j} + \sum_{i=1}^{d} b_{i} \xi_{i} ,
\label{eq:ising}
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\boldsymbol{b} \in \mathbb{R}^{d}$ denotes a bias vector and the weights $\boldsymbol{W} \in \mathbb{R}^{d \times d}$ are set to the sum of the outer products of the patterns we want to store&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{W} = \sum_{i=1}^{N} \boldsymbol{x}_{i} \otimes \boldsymbol{x}_{i}^{T}.
\end{equation}&lt;/p&gt;
&lt;p&gt;The state pattern update rule is given by the sign of the gradient of \eqref{eq:ising} with respect to $\boldsymbol{\xi}$ and can be done in one step (synchronously) or separately for every component of the vector (asynchronously):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{\xi}_{n+1} = \mathrm{sgn} \left( \boldsymbol{W}  \boldsymbol{\xi}_{n} - \boldsymbol{b} \right).
\end{equation}&lt;/p&gt;
&lt;p&gt;The storage capacity of this system for retrieval of patterns with a small amount of errors can be shown to be $C \cong 0.14 d$, scaling linearly with the dimension of the pattern vector.&lt;/p&gt;
&lt;h3 id=&#34;physical-intuition&#34;&gt;Physical intuition&lt;/h3&gt;
&lt;p&gt;Physicists immediately recognize the energy function \eqref{eq:ising} as an incarnation of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Ising_model#Application_to_neuroscience&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ising model&lt;/a&gt;. Spin degree of freedoms $\xi_{i}$ are grouped into patterns $\boldsymbol{\xi}$ that are equivalent to &lt;em&gt;spin configurations&lt;/em&gt; of $d$ spins. The weight matrix is a sum of stored-pattern spin configurations, serving as attractors for the state-pattern spin configuration. The couplings $w_{ij}$ can be regarded a sum of samples of an underlying pattern data distribution. They are not restricted to (nearest-)neighbors and their values are neither uniform like in exactly solvable models nor totally random like in spin glass models.&lt;/p&gt;
&lt;!-- After identifying relevant degrees of freedom, physicists combine appropriate conceptual structures with arguments based on locality, symmetry, and physical and mathematical intuition to write down a model description, usually after a lot of hard work and trial-and-error.--&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Neural networks and spin glasses&lt;/strong&gt;: There is some literature on connections between &lt;a href=&#34;https://en.wikipedia.org/wiki/Spin_glass&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spin glasses&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;neural networks&lt;/a&gt;. Spin glasses are phases of matter describing disordered magnetic systems exhibiting both &lt;a href=&#34;https://en.wikipedia.org/wiki/Order_and_disorder#Quenched_disorder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quenched disorder&lt;/a&gt; and frustratation. Spin glasses were a major inspiration for Hopfield networks, as beautifully explained by the condensed matter physicist &lt;a href=&#34;https://en.wikipedia.org/wiki/Philip_W._Anderson&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Philip W. Anderson&lt;/a&gt; in a &lt;a href=&#34;https://en.wikipedia.org/wiki/Spin_glass#cite_note-10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;column series for Physics Today&lt;/a&gt; (1988-1990). However, apart from &lt;a href=&#34;https://arxiv.org/abs/1910.01592&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient training of energy-based models via spin-glass control&lt;/a&gt; &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;, I could not find any recent papers that point to a productive research direction beyond qualitative statements like &amp;ldquo;here&amp;rsquo;s two hard problems where symmetry and order will not help you solve them&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;modern-discrete-hopfield-networks&#34;&gt;Modern discrete Hopfield networks&lt;/h2&gt;
&lt;p&gt;Modern discrete Hopfield networks (or &lt;em&gt;dense&lt;/em&gt; associative memories) introduced the following family of energy functions to improve pattern storage capacity and pattern separation capabilities &lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = - \sum_{i=1}^{N} F \left( \boldsymbol{x}_{i}^{T} \cdot \boldsymbol{\xi} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;Compared to the classical discrete Hopfield network energy function \eqref{eq:ising}, the explicit weight matrix is gone and the energy has been reduced to a sum of a function of dot products between the state pattern $\boldsymbol{\xi}$ and every stored pattern $\boldsymbol{x}_i$. For a polynomial interaction function $F(x) = x^{a}$, low-error storage capacity is $C \cong d^{a-1}$. The quadratic, classical discrete Hopfield network is recovered by setting $a=2$.&lt;/p&gt;
&lt;p&gt;Essentially, the role of $F(x)$ is to separate close patterns by blowing up differences in dot product values. Few things blow up better than exponentials, so
we can generalize the energy to&lt;/p&gt;
&lt;p&gt;\begin{equation}
E = - \sum_{i=1}^{N} \exp \left( \boldsymbol{x}_{i}^{T} \cdot \boldsymbol{\xi} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;with storage capacity $C \cong 2^{d/2}$. The corresponding update rules for modern discrete Hopfield networks can be shown to converge quickly with high probability&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;modern-continuous-hopfield-networks&#34;&gt;Modern continuous Hopfield networks&lt;/h2&gt;
&lt;p&gt;Most machine learning applications are tailored to work with continuous embeddings (vector representations) rather than discrete patterns. Is there a way to generalize modern Hopfield networks to continuous data? Recently, &lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need&lt;/a&gt; proposed the following energy function to deal with continuous $d$-dimensional patterns&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(\boldsymbol{\xi}; \boldsymbol{X}) = \frac{1}{2} \boldsymbol{\xi}^T \boldsymbol{\xi} -\mathrm{logsumexp} \left( \boldsymbol{X}^T \boldsymbol{\xi} \right),
\label{eq:energyfunc}
\end{equation}&lt;/p&gt;
&lt;p&gt;which we consider to be a function of the state pattern $\boldsymbol{\xi} \in \mathbb{R}^{d}$ and parametrized by $N$ stored patterns $\boldsymbol{X} = (\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}) \in \mathbb{R}^{d \times N}$. From the point of view of &lt;a href=&#34;https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;restricted Boltzmann machines&lt;/a&gt;, the stored patterns $\boldsymbol{X}^T$ can also be interpreted as weights mapping $\boldsymbol{\xi}$ to hidden units&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Smoothly taking a maximum&lt;/strong&gt;: The $\mathrm{logsumexp}$ operator is defined for vectors $\mathbf{x}$ as
\begin{equation}
\mathrm{logsumexp} \left( \mathbf{x} \right) = \log \left( \sum_{i=1}^{N} \mathrm{e}^{x_i} \right)
\end{equation}
while for matrix arguments (like a batch of vectors), the $\mathrm{sumexp}$ is understood to apply to just one dimension after which the $\log$ acts element-wise on the resulting vector.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;physical-intuition-1&#34;&gt;Physical intuition&lt;/h3&gt;
&lt;p&gt;We assume that the stored patterns equilibrate much quicker than those of the state pattern so that the former can effectively be considered &amp;ldquo;frozen&amp;rdquo;. The energy function \eqref{eq:energyfunc} looks deceptively simple: there is a single state pattern and there are no interactions among stored patterns. The first term takes care of making sure the norm of the input state pattern is finite, while the second term scores the query&amp;rsquo;s overlap based on its individual alignment with every stored pattern. The exponential function in the term&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathrm{logsumexp} \left( \boldsymbol{X}^T \boldsymbol{\xi} \right) = \log \left( \sum_{i=1}^{N} \mathrm{e}^{\mathbf{x}_i \cdot \boldsymbol{\xi}} \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;is used to pull apart close patterns by blowing up differences in the dot product between state pattern and stored patterns. From the perspective of the query, it is not so much an interaction term but rather a measure of the alignment of the query to external &amp;ldquo;magnetic fields&amp;rdquo; generated by the stored patterns.&lt;/p&gt;
&lt;h3 id=&#34;deriving-the-update-rule&#34;&gt;Deriving the update rule&lt;/h3&gt;
&lt;p&gt;In the spirit of hand-waving, let us refuse to resort to of the dynamical systems machinery used in the original references &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; and rather derive the update rule for the state pattern $\boldsymbol{\xi}$ by taking the derivative of the energy function \eqref{eq:energyfunc} with respect to $\boldsymbol{\xi}$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\nabla_{\boldsymbol{\xi}} E(\boldsymbol{\xi}; \boldsymbol{X}) = \boldsymbol{\xi} - \boldsymbol{X} \ \mathrm{softmax} \left( \boldsymbol{X}^T \boldsymbol{\xi} \right).
\end{equation}&lt;/p&gt;
&lt;p&gt;A gradient descent update with step size $\gamma$ looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{\xi}_{n+1} = \boldsymbol{\xi}_{n} - \gamma \left( \boldsymbol{\xi}_{n} - \boldsymbol{X} \ \mathrm{softmax} \left( \boldsymbol{X}^T \boldsymbol{\xi}_{n}\right) \right).
\label{eq:conthopfupdate}
\end{equation}&lt;/p&gt;
&lt;p&gt;We are very confident that the topography of the energy landscape allows us to take big steps and boldly set $\gamma = 1$ to recover the familiar update rule&lt;/p&gt;
&lt;p&gt;\begin{align}
\boldsymbol{\xi}_{n+1}  = \boldsymbol{X} \ \mathrm{softmax} \left( \boldsymbol{X}^T \boldsymbol{\xi}_{n}\right) .
\end{align}&lt;/p&gt;
&lt;p&gt;The updated vector is a linear combination of all stored patterns, weighted by an attention vector quantifying the overlap with the input pattern.&lt;/p&gt;
&lt;h2 id=&#34;modern-continuous-hopfield-networks-as-energy-based-models&#34;&gt;Modern continuous Hopfield Networks as energy-based models&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s now try to connect the system defined by the energy function \eqref{eq:energyfunc} to the statistical mechanics framework of energy-based models &lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;energy-based-models-a-gentle-introduction&#34;&gt;Energy-based models: a gentle introduction&lt;/h3&gt;
&lt;p&gt;Energy-based models learn a parametrized energy function $E_{\theta}$ which maps data points $\boldsymbol{x}$ to real, scalar energy values $E_{\theta}(\boldsymbol{x})$. The data distribution is modeled by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Boltzmann_distribution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Boltzmann distribution&lt;/a&gt;,
\begin{equation}
p_{\theta}(\boldsymbol{x}) = \frac{\mathrm{e}^{ - E_{\theta}(\boldsymbol{x}) }}{Z(\theta)},
\label{eq:boltzmann}
\end{equation}
where $Z(\theta) = \int \mathrm{d} \boldsymbol{x} \ \mathrm{e}^{-E(\boldsymbol{x})}$ denotes the system&amp;rsquo;s partition function. Configurations $\boldsymbol{x}$ with low energies $E_{\theta}(\boldsymbol{x})$ are considered more likely and their weight contributes more strongly to the partition function.&lt;/p&gt;
&lt;p&gt;To steer the model distribution $p_{\theta}$ towards a target data distribution $p_{\mathrm{data}}$, we can try to minimize the likelihood loss function&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mathcal{L}_{\mathrm{ML}} (\theta) = \mathbb{E}_{\boldsymbol{x} \sim p_{\mathrm{data}}} \left[ -\log p_{\theta} (\boldsymbol{x}) \right],
\label{eq:nll}
\end{equation}&lt;/p&gt;
&lt;p&gt;where the negative log-likelihood equals&lt;/p&gt;
&lt;p&gt;\begin{equation}
-\log p_{\theta} (\boldsymbol{x}) = E_{\theta} (\boldsymbol{x}) + \log Z (\theta).
\end{equation}&lt;/p&gt;
&lt;p&gt;This is a hard optimization problem because calculating $\log Z (\theta)$ is hard for the vast majority of high-dimensional data distributions we care about. In practice, people resort to approximations like contrastive divergence to push the energy down on &amp;ldquo;positive examples&amp;rdquo; drawn from the data distribution while pushing up on &amp;ldquo;negative examples&amp;rdquo; obtained from sampling the model distribution. Even though sampling from \eqref{eq:boltzmann} can be done with methods like Markov Chain Monte Carlo, it is computationally expensive to do so, especially as part of an inner-loop optimization step&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=&#34;exactly-optimizing-modern-continuous-hopfield-networks&#34;&gt;Exactly optimizing modern continuous Hopfield networks&lt;/h3&gt;
&lt;p&gt;So what about the system defined by the energy function \eqref{eq:energyfunc}? Let&amp;rsquo;s consider the stored patterns $\mathbf{X} \in \mathbb{R}^{d \times N}$ as the model parameters we want to optimise. The task for the model is then to try to memorise incoming state patterns $\boldsymbol{\xi} \in \mathbb{R}^{d}$ drawn from some data distribution $p_{\mathrm{data}}$ by deciding what kind of patterns to store. The partition function looks like&lt;/p&gt;
&lt;p&gt;\begin{equation}
Z = \int \mathrm{d} \boldsymbol{\xi} \ \mathrm{e}^{-E(\boldsymbol{\xi})} = \int \mathrm{d} \boldsymbol{\xi} \ \mathrm{e}^{-\frac{1}{2} \boldsymbol{\xi}^T \boldsymbol{\xi}} \left( \sum_{i=1}^{N} \mathrm{e}^{ \boldsymbol{x}^{T}_{i} \cdot \boldsymbol{\xi} } \right)
\label{eq:zforcontinuoushopfield}
\end{equation}&lt;/p&gt;
&lt;p&gt;which, because of the $\log$ in the &amp;ldquo;interaction term&amp;rdquo;, boils down to a sum of &lt;a href=&#34;https://en.wikipedia.org/wiki/Gaussian_integral#n-dimensional_with_linear_term&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;$n$-dimensional Gaussian integrals&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;\begin{aligned}
Z = (2\pi)^{n/2} \sum_{i=1}^{N} \mathrm{e}^{ \frac{1}{2} \boldsymbol{x}_{i}^{T} \cdot \boldsymbol{x}_{i} }
\end{aligned}&lt;/p&gt;
&lt;p&gt;After taking the logarithm, we end up with the $\mathrm{logsumexp}$ operator:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\log Z = \frac{n}{2} \log \left( 2\pi \right) + \mathrm{logsumexp} \left( \frac{1}{2} \mathrm{diag} \left( \boldsymbol{X}^{T} \boldsymbol{X} \right) \right)
\end{equation}&lt;/p&gt;
&lt;p&gt;where the $\mathrm{diag}$ operator is understood to turn the diagonal of its matrix argument into a vector. Plugging this expression into \eqref{eq:nll} leads to the following loss function for the matrix of stored patterns&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathcal{L}_{\mathrm{ML}} (\mathbf{X}) = &amp;amp; \mathbb{E}_{\boldsymbol{\xi} \sim p_{\mathrm{data}}} \left[ \frac{1}{2} \boldsymbol{\xi}^T \boldsymbol{\xi} -\mathrm{logsumexp} \left( \boldsymbol{X}^T \boldsymbol{\xi} \right) \right] \nonumber \\&lt;br&gt;
&amp;amp; + \mathrm{logsumexp} \left( \frac{1}{2} \mathrm{diag} \left( \boldsymbol{X}^{T} \boldsymbol{X} \right) \right) + \frac{n}{2} \log \left( 2\pi \right)
\end{align}&lt;/p&gt;
&lt;p&gt;and a gradient&lt;/p&gt;
&lt;p&gt;\begin{align}
\nabla_{\mathbf{X}} \mathcal{L}_{\mathrm{ML}} (\mathbf{X}) = &amp;amp; - \mathbb{E}_{\boldsymbol{\xi} \sim p_{\mathrm{data}}} \left[ \boldsymbol{\xi} \otimes \mathrm{softmax} \left( \boldsymbol{X}^T \boldsymbol{\xi} \right) \right] \nonumber \\&lt;br&gt;
&amp;amp; + \boldsymbol{X} \ \mathrm{softmax} \left( \frac{1}{2} \mathrm{diag} \left( \boldsymbol{X}^{T} \boldsymbol{X} \right) \right)
\end{align}&lt;/p&gt;
&lt;p&gt;and an update with step size $\gamma$&lt;/p&gt;
&lt;p&gt;\begin{align}
\mathbf{X}_{n+1} = \ \mathbf{X}_{n} &amp;amp;+ \gamma \ \mathbb{E}_{\boldsymbol{\xi} \sim p_{\mathrm{data}}} \left[ \boldsymbol{\xi} \otimes \mathrm{softmax} \left( \boldsymbol{X}^T_{n} \boldsymbol{\xi} \right) \right] \nonumber \\&lt;br&gt;
&amp;amp; - \gamma \ \mathbf{X}_{n} \ \mathrm{softmax} \left( \frac{1}{2} \mathrm{diag} \left( \boldsymbol{X}^{T}_{n} \boldsymbol{X}_{n} \right) \right)
\end{align}&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s try to guess what this means for a single input state pattern. The first gradient term pushes all stored patterns towards the sample but weighted by a dot-product attention vector quantifying their overlap with the input pattern, similar to \eqref{eq:conthopfupdate} but in the other direction. The second gradient term comes from the partition function and acts as a regularizer by keeping the norms of the stored patterns in check. Regularization keeps pattern values within a reasonable range and pushes the system towards regions in parameter space with non-trivial small dot-product values.&lt;/p&gt;
&lt;h2 id=&#34;transformers-store-and-retrieve-context-dependent-patterns&#34;&gt;Transformers store and retrieve context-dependent patterns&lt;/h2&gt;
&lt;p&gt;Making the leap from modern continous Hopfield networks to the vanilla Transformer (self-)attention mechanism we encountered in &lt;a href=&#34;#2-a-growing-zoo-of-transformers&#34;&gt;Section 2&lt;/a&gt; requires a few additional steps, as explained in detail in the &lt;a href=&#34;https://ml-jku.github.io/hopfield-layers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt; accompanying &lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We want to act on multipe $d$-dimensional state patterns at the same time in order to retrieve multiple updated patterns in parallel:
\begin{align}
\boldsymbol{\xi} \in \mathbb{R}^{d} \to \boldsymbol{\Xi} = (\boldsymbol{\xi}_{1}, \ldots, \boldsymbol{\xi}_{S}) \in \mathbb{R}^{d \times S}
\end{align}
so that
\begin{align}
\boldsymbol{\Xi}_{n+1}  = \boldsymbol{X} \ \mathrm{softmax} \left( \boldsymbol{X}^T \boldsymbol{\Xi}_{n}\right) .
\end{align}
In practice, the number of state patterns $S$ is often taken to be equal to the number of stored patterns $N$.&lt;/li&gt;
&lt;li&gt;We want to map stored patterns $\mathbf{X}$ and state patterns $\boldsymbol{\Xi}$ respectively to &lt;em&gt;keys&lt;/em&gt; $\mathbf{K} \in \mathbb{R}^{N \times d}$ and &lt;em&gt;queries&lt;/em&gt; $\mathbf{Q} \in \mathbb{R}^{S \times d}$ in a common feature space using linear transformations $\mathbf{W_{K}}$ and $\mathbf{W_{Q}}$.&lt;/li&gt;
&lt;li&gt;We want introduce another linear transformation $\mathbf{W_{V}}$ on stored patterns to transform them into &lt;em&gt;values&lt;/em&gt; $\mathbf{V} \in \mathbb{R}^{N \times d}$ appropriate for the keys&#39; content.&lt;/li&gt;
&lt;li&gt;We want to modify the learning dynamics by decreasing the inverse temperature to $\beta = 1 / \sqrt{d}$, effectively making the $\mathrm{softmax}$ softer by increasing the temperature of the system&lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;. Physically, this might correspond to warming up the system just enough to get out of the spin-glass phase while not introducing too much thermal noise&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result is the update rule we stated without explanation in &lt;a href=&#34;#2-a-growing-zoo-of-transformers&#34;&gt;Section 2&lt;/a&gt;:
\begin{equation}
\mathbf{Q}^{\mathrm{updated}} = \mathrm{Attention}\left( \mathbf{Q}, \mathbf{K}, \mathbf{V} \right) = \mathrm{softmax} \left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d}} \right) \mathbf{V},
\label{eq:transformerattnupdate}
\end{equation}
where the $\mathrm{softmax}$ acts row-wise. In practice, the vanilla Transformer module additionally wraps the above attention module in (1) residual connections to control the flow of gradients, (2) layer norms to control pattern normalisations and learning dynamics, and (3) a positional feed-forward network for additional model capacity.&lt;/p&gt;
&lt;h2 id=&#34;where-are-patterns-stored-in-a-transformer&#34;&gt;Where are patterns stored in a Transformer?&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s try to digest the implications of these quite substantial changes. It&amp;rsquo;s useful to think of Transformer (self-)attention modules as dynamic pattern storage and retrieval systems. In modern continuous Hopfield networks, stored patterns are considered a given. However, in the Transformer (self-)attenton module, patterns to be matched and retrieved are &lt;em&gt;dependent on inputs&lt;/em&gt; and &lt;em&gt;implicitly stored in the weights&lt;/em&gt; $\mathbf{W_{Q}}$, $\mathbf{W_{K}}$, and $\mathbf{W_{V}}$ of the linear transformations. In every layer, the module needs to learn how to map a set of inputs to patterns it wants to store (keys and values) as well as how to best retrieve them (queries). Within the same layer, dynamically generated queries are matched to keys within the same latent space. Between attention modules of neighboring layers, the non-linear activation function in the positional feed-forward network warps latent spaces.&lt;/p&gt;
&lt;!-- ## Transformer self-attention as energy-based models
For completeness, we can try to write down the energy function of the Transformer self-attention module. Starting from \eqref{eq:energyfunc}

Instead of stored patterns $X$ we considered fixed, the energy function 

that is implicitly being optimised for by making the necessary substitutions in Eq [] :

\begin{equation}
  E(\boldsymbol{\xi}; \boldsymbol{X}) = \frac{1}{2} \mathrm{diag} \left( \boldsymbol{\Xi}^T \boldsymbol{\Xi} \right) -\mathrm{logsumexp} \left( \boldsymbol{X}^T \boldsymbol{\Xi} \right),
  \label{eq:transformerenergy}
\end{equation}

\begin{equation}
  E(\boldsymbol{\xi}; \boldsymbol{X}) = \frac{1}{2} \mathrm{diag} \left( \boldsymbol{\Xi}^T \boldsymbol{\Xi} \right) -\mathrm{logsumexp} \left( \boldsymbol{X}^T \boldsymbol{\Xi} \right),
  \label{eq:transformerenergy}
\end{equation}

Checking whether this transformed energy function still leads to a tractable Gaussian partition function (possibly involving the determinant of a sum of products of linear transformation matrices), is left as an exercise for the reader. --&gt;
&lt;h1 id=&#34;4-training-transformers&#34;&gt;4. Training Transformers&lt;/h1&gt;
&lt;p&gt;Now that we are aware of an energy-based interpretation of dot-product (self-)attention, we can start hand-waving about what could be going on during the supervised training procedure of Transformer models and how energy-based models suggest a qualitatively different approach to improving attention mechanisms.&lt;/p&gt;
&lt;h2 id=&#34;pretraining-loss-functions&#34;&gt;Pretraining loss functions&lt;/h2&gt;
&lt;p&gt;The goal of pretraining loss functions is to induce &lt;em&gt;useful&lt;/em&gt; data-dependent pattern storage and retrieval behavior. Pretraining strategies for Transformer-based language models rely on loss functions derived from auxiliary tasks to learn statistical patterns in natural language. Starting from almost identical model architectures, autoregressive models like GPT-3 leverage all their parameters to predict the next token in a sequence given previous tokens while autoencoding models like BERT try to reconstruct corrupted tokens. In both cases, the loss function is a cross-entropy loss involving predictions in the space of the model&amp;rsquo;s token vocabulary.&lt;/p&gt;
&lt;h2 id=&#34;stepping-through-the-transformer-implicit-energy-minimization&#34;&gt;Stepping through the Transformer: implicit energy minimization&lt;/h2&gt;
&lt;p&gt;Although no energy function is &lt;em&gt;explicitly&lt;/em&gt; optimized during training&lt;sup id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;16&lt;/a&gt;&lt;/sup&gt;, let&amp;rsquo;s see how far we can push hand-wavy energy-based arguments by stepping through the forward and backward pass of a Transformer model. We have learned that the attention update \eqref{eq:transformerattnupdate} in every Transformer layer is actually a hidden gradient step. This trivial insight leads to a trio of trivial observations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trivial Observation #1:&lt;/strong&gt; &lt;em&gt;During training, the update step \eqref{eq:transformerattnupdate} of the attention mechanism in a Transformer layer acts as an inner-loop optimization step, minimizing an implicit energy function determined by the queries, keys, and values constructed from the output of the previous layer.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trivial Observation #2:&lt;/strong&gt; &lt;em&gt;During the forward pass of a deep Transformer model, a nested hierarchy of energy functions is minimized.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trivial Observation #3:&lt;/strong&gt; &lt;em&gt;During the backward pass of a deep Transformer model, the parameters of its attention modules get updated such that the inner-loop optimization steps conspire to pattern match queries to keys in such a way that the sequentially-updated final latent representations are useful for improving the loss.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;meta-learning-and-few-shot-inference&#34;&gt;Meta-learning and few-shot inference&lt;/h2&gt;
&lt;p&gt;Squinting our eyes, we can see traces of a &lt;em&gt;meta-learning&lt;/em&gt; problem: how to tune model parameters &amp;mdash; in particular the attention mechanisms&#39; linear transformation matrices &amp;mdash; such that applying a sequence of one-step attention updates to sets of input patterns converges to representations useful for minimizing the (meta-)loss function. Learnable modules of a differentiable program can of course often be considered part of a larger meta-learning setup. But what this point of view suggests is that confining the one-step inner-loop update to a simple associative memory pattern lookup might be quite restrictive.&lt;/p&gt;
&lt;p&gt;Yet even with with a simple dense associative memory, OpenAI&amp;rsquo;s paper &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language Models are Few-Shot Learners&lt;/a&gt; showed that large-capacity models like GPT-3 already exhibit quite impressive meta-learning capabilities. The energy-based perspective provides a naive yet attractive explanation for this phenomenon. At inference time, the few-shot demonstrations, which make up the initial part of a few-shot learning query, condition the sequential generation process by providing basins of attraction in the energy landscape for other energy minimization steps to be pulled towards. &lt;em&gt;The GPT-3 model is memorizing to the extent the demonstrations match patterns seen during training and generalizing within the possibilities of the rudimentary attention dynamics of the simple underlying energy functions.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;5-beyond-dot-product-attention&#34;&gt;5. Beyond dot-product attention&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s conclude this post with two related thoughts inspired by an energy-based perspective on current attention architectures: attention dynamics and modeling very long sequences.&lt;/p&gt;
&lt;h2 id=&#34;attention-dynamics-embracing-collective-phenomena&#34;&gt;Attention dynamics: embracing collective phenomena&lt;/h2&gt;
&lt;p&gt;We have seen that the energy function of a modern continuous Hopfield network \eqref{eq:energyfunc} is rather uninspiring from a physics perspective. Theoretically, the exponential storage and efficient retrieval of patterns is obtained by burning deep valleys into the energy landscape around stored patterns (keys) for neighbouring state patterns (queries) to quickly roll into. In practice, the authors of &lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need&lt;/a&gt; observed three kinds of fixed-point behavior in a pretrained BERT model: (1) global fixed points averaging over all stored patterns, (2) metastable states averaging over a subset of stored patterns, and (3) fixed points returning a single, well-separated stored pattern.&lt;/p&gt;
&lt;p&gt;What does this tell us? Assuming the attention updates converge faithfully during training, the linear maps turning input vectors into queries, keys, and values can become bottlenecks in terms of being able to separate patterns and organise the energy landscape. Additionally, the lack of interactions among patterns and the decoupled dot-product overlap between queries and keys puts considerable limits on how the network can process information. In practice, this is being partially addressed by using multiple attention heads (see &lt;a href=&#34;#2-a-growing-zoo-of-transformers&#34;&gt;Section 2&lt;/a&gt;), but this solution does not feel satisfactory.&lt;/p&gt;
&lt;h2 id=&#34;why-very-long-sequences-should-not-be-needed&#34;&gt;Why very long sequences should not be needed&lt;/h2&gt;
&lt;p&gt;Recurrent neural networks try to compress patterns in a single hidden state via sequential propagation but often fail to do so and forget stuff along the way. Transformers bake patterns into a hierarchical energy landscape but focus on a fixed-length context window to store and retrieve patterns. As we&amp;rsquo;ve seen in &lt;a href=&#34;#2-a-growing-zoo-of-transformers&#34;&gt;Section 2&lt;/a&gt;, a lot of research on improving Transformers focuses on alleviating the $\mathcal{O}(N^2)$ bottleneck of the attention computation with the implicit goal of scaling to longer sequences and enabling larger context windows.&lt;/p&gt;
&lt;p&gt;But very long sequences should not be needed if patterns are allowed to talk to each other. A model should not need all of the world as context if patterns and emergent concepts can be connected. It&amp;rsquo;s definitely worthwhile to try to reduce the computational complexity of current attention architectures, but it might be far more valuable to swap the simple energy-based model \eqref{eq:energyfunc} for more interesting energy-based models. Why not dust off the old unrestricted Boltzmann machine once again? Or experiment with any one of a century&amp;rsquo;s worth of physics models? Not to train them explicitly, but have them serve as implicit models underlying more intricate attention mechanisms, mediated by (local) interactions among patterns. Naturally, after so much hand-waving, our journey has to end here.&lt;/p&gt;
&lt;h1 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h1&gt;
&lt;p&gt;Even if attention turns out to &lt;em&gt;not&lt;/em&gt; be all we need, (self-)attention modules have established themselves as highly parallelizable neural network building blocks capable of dynamically routing information based on context. We have seen that dot-product attention modules in Transformer models work by encoding high-dimensional patterns into the landscapes of simple energy functions, enabling fast pattern storage and retrieval. During training, these landscapes are sculpted to accommodate statistical patterns found in data by hierarchically matching and combining latent pattern representations through a sequence of implicit energy function minimizations.&lt;/p&gt;
&lt;p&gt;We argued that an energy-based perspective on attention provides an intuitive explanation of meta-learning capabilities of large-capacity language models and encourages the exploration of qualitatively different attention mechanisms for pattern storage and retrievel. Rather than naively scaling the current generation of Transformers, it might be more rewarding to scale learning itself by exploring more powerful, expressive, and computationally efficient attention mechanisms, guided by energy-based models. Perhaps we should consider looking at neural networks again like John Hopfield already did in 1982: &lt;em&gt;physical systems with emergent collective computational abilities&lt;/em&gt;.&lt;/p&gt;
&lt;h1 id=&#34;references--footnotes&#34;&gt;References &amp;amp; footnotes&lt;/h1&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt; (2017)&lt;/em&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Hubert Ramsauer, Bernhard SchÃ¤fl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena PavloviÄ, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, GÃ¼nter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, &lt;a href=&#34;https://arxiv.org/abs/2008.02217&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hopfield Networks is All You Need&lt;/a&gt; (2020)&lt;/em&gt; &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Johannes Brandstetter, &lt;a href=&#34;https://ml-jku.github.io/hopfield-layers/&#34;&gt;https://ml-jku.github.io/hopfield-layers/&lt;/a&gt; (2020)&lt;/em&gt; &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Johannes Brandstetter and Hubert Ramsauer, &lt;a href=&#34;https://ml-jku.github.io/blog-post-performer/&#34;&gt;https://ml-jku.github.io/blog-post-performer/&lt;/a&gt; (2020)&lt;/em&gt; &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Dmitry Krotov and John Hopfield, &lt;a href=&#34;https://arxiv.org/abs/2008.06996&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Large Associative Memory Problem in Neurobiology and Machine Learning&lt;/a&gt; (2020)&lt;/em&gt; &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;If you have only just joined the attention revolution, there are a lot of great resources out there to get you started. Yannic Kilcher provides a great introduction in his &lt;a href=&#34;https://www.youtube.com/watch?v=iDulhoQ2pro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video on Attention is All You Need&lt;/a&gt;. The &lt;a href=&#34;http://gabrielilharco.com/publications/EMNLP_2020_Tutorial__High_Performance_NLP.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;High Performance NLP tutorial slides&lt;/a&gt; presented at &lt;a href=&#34;https://2020.emnlp.org/tutorials&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EMNLP 2020&lt;/a&gt; contain a thorough and visually appealing introduction to attention-based models. Because code is usually more to the point than papers that need to sell themselves, I highly recommend Phil Wang&amp;rsquo;s &lt;a href=&#34;https://github.com/lucidrains?tab=repositories&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excellent collection of self-contained repositories&lt;/a&gt; showcasing some of the latest models and techniques. &lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;John Hopfield, &lt;a href=&#34;https://www.pnas.org/content/pnas/79/8/2554.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Networks and Physical Systems with Emergent Collective Computational Abilities&lt;/a&gt; (1982)&lt;/em&gt; &lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Alejandro Pozas-Kerstjens, Gorka MuÃ±oz-Gil, Miguel Ãngel GarcÃ­a-March, Antonio AcÃ­n, Maciej Lewenstein, PrzemysÅaw R. Grzybowski, &lt;a href=&#34;https://arxiv.org/abs/1910.01592&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Efficient training of energy-based models via spin-glass control&lt;/a&gt; (2019)&lt;/em&gt; &lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Dmitry Krotov and John Hopfield, &lt;a href=&#34;https://arxiv.org/abs/1606.01164&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dense Associative Memory for Pattern Recognition&lt;/a&gt; (2016)&lt;/em&gt; &lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Mete Demircigil, Judith Heusel, Matthias LÃ¶we, Sven Upgang, and Franck Vermet, &lt;a href=&#34;https://arxiv.org/abs/1702.01929&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On a Model of Associative Memory with Huge Storage Capacity&lt;/a&gt; (2017)&lt;/em&gt; &lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;A physicist might consider these continuous patterns spin configurations of the degrees of freedom in a vector spin model where the internal dimension $D \sim 10^2-10^4$ is much bigger than familiar small-$D$ cases like the &lt;a href=&#34;https://en.wikipedia.org/wiki/Classical_XY_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;XY model&lt;/a&gt; or the &lt;a href=&#34;https://en.wikipedia.org/wiki/Classical_Heisenberg_model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Heisenberg model&lt;/a&gt; but much smaller than infinity. &lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Yann LeCun, Sumit Chopra, Raia Hadsell, Marc&amp;rsquo;Aurelio Ranzato, and Fu Jie Huang, &lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Tutorial on Energy-Based Learning&lt;/a&gt; (2006)&lt;/em&gt; and &lt;em&gt;Yann LeCun and Alfredo Canziani, &lt;a href=&#34;https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning DS-GA 1008 course&lt;/a&gt; (2020)&lt;/em&gt; &lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;em&gt;Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, and David J. Schwab, &lt;a href=&#34;https://arxiv.org/abs/1803.08823&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A high-bias, low-variance introduction to Machine Learning for physicists&lt;/a&gt; (2019)&lt;/em&gt; &lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The generator in a Generative Adverserial Network (GAN) setup can be considered a clever way to generate negative samples for the implicit energy function optimization taking place in the discriminator. &lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;As we have seen in &lt;a href=&#34;#2-a-growing-zoo-of-transformers&#34;&gt;Section 2&lt;/a&gt;, the naive interpretation of $\beta$ as &lt;em&gt;the&lt;/em&gt; effective inverse temperature is tenuous in practice given the influence of the surrounding layer normalisation modules. &lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:16&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;The implicitly defined energy functions in Tranformer layers are not optimized directly because they arguably do not provide a meaningful training signal on their own. Verifying whether this is true or not could make for an interesting experiment. &lt;a href=&#34;#fnref:16&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
