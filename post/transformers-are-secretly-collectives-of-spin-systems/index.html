<!DOCTYPE html><html lang="en-gb" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 4.8.0 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Matthias Bal">

  
  
  
    
  
  <meta name="description" content="Is the Ising model all you need?">

  
  <link rel="alternate" hreflang="en-gb" href="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Serif:ital,wght@0,300;0,400;1,300;1,400%7CIBM+Plex+Sans:ital,wght@0,300;0,400;0,700;1,400%7CFira+Code&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=G-H9Y98B2S2P"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-H9Y98B2S2P', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="mcbal">
  <meta property="og:url" content="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/">
  <meta property="og:title" content="Transformers Are Secretly Collectives of Spin Systems | mcbal">
  <meta property="og:description" content="Is the Ising model all you need?"><meta property="og:image" content="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/featured.jpg">
  <meta property="twitter:image" content="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/featured.jpg"><meta property="og:locale" content="en-gb">
  
    
      <meta property="article:published_time" content="2021-11-23T12:17:17&#43;01:00">
    
    <meta property="article:modified_time" content="2021-11-28T20:02:18&#43;01:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/"
  },
  "headline": "Transformers Are Secretly Collectives of Spin Systems",
  
  "image": [
    "https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/featured.jpg"
  ],
  
  "datePublished": "2021-11-23T12:17:17+01:00",
  "dateModified": "2021-11-28T20:02:18+01:00",
  
  "author": {
    "@type": "Person",
    "name": "Matthias Bal"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "mcbal",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mcbal.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Is the Ising model all you need?"
}
</script>

  

  


  


  





  <title>Transformers Are Secretly Collectives of Spin Systems | mcbal</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class=" ">

  
  
  
    <script>window.wcDarkLightEnabled = true;</script>
  
  
    <script>const isSiteThemeDark = false;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">mcbal</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">mcbal</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>



  <article class="article">

  




















  
  


<div class="article-container pt-3">
  <h1>Transformers Are Secretly Collectives of Spin Systems</h1>

  
  <p class="page-subtitle">Is the Ising model all you need?</p>
  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Nov 28, 2021
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 600px; max-height: 400px;">
  <div style="position: relative">
    <img src="/post/transformers-are-secretly-collectives-of-spin-systems/featured.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <hr>
<ol>
<li><a href="#1-introduction">Introduction</a></li>
<li><a href="#2-where-does-the-transformer-module-architecture-come-from">Where does the transformer module architecture come from?</a></li>
<li><a href="#3-deriving-attention-from-energy-functions-only-gets-you-so-far">Deriving attention from energy functions only gets you so far</a></li>
<li><a href="#4-back-to-the-roots-physical-spin-systems-and-vector-spin-models">Back to the roots: physical spin systems and vector-spin models</a></li>
<li><a href="#5-why-dont-we-just-probe-a-vector-spin-system-with-data">Why don&rsquo;t we just probe a vector-spin system with data?</a></li>
<li><a href="#6-a-slice-of-statistical-mechanics-magnetizations-and-free-energies">A slice of statistical mechanics: magnetizations and free energies</a></li>
<li><a href="#7-turning-a-differentiable-spin-system-into-a-neural-network">Turning a differentiable spin system into a neural network</a></li>
<li><a href="#8-an-exercise-in-squinting-recognizing-the-transformer-module">An exercise in squinting: recognizing the transformer module</a></li>
<li><a href="#9-training-transformer-modules-shapes-collective-behavior">Training transformer modules shapes collective behavior</a></li>
<li><a href="#10-training-deep-transformers-orchestrates-spin-system-collectives">Training deep transformers orchestrates spin-system collectives</a></li>
<li><a href="#11-conclusion">Conclusion</a></li>
</ol>
<hr>
<h3 id="1-introduction">1. Introduction</h3>
<p>In this post, we try to distill a unifying perspective out of ideas developed in a series of longer posts on understanding transformers as physical systems:</p>
<ul>
<li><a href="https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/" target="_blank" rel="noopener">Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</a></li>
<li><a href="https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/" target="_blank" rel="noopener">Transformers from Spin Models: Approximate Free Energy Minimization</a></li>
</ul>
<p>We argue that the neural-network architecture of the archetypical transformer module can be derived from the structure of physical spin systems familiar from classical statistical mechanics. More specifically, we claim that the forward pass of transformer modules maps onto computing magnetizations in vector-spin models in response to incoming data. We imagine transformers secretly being collectives of differentiable spin systems whose behavior can be shaped through training.</p>
<h3 id="2-where-does-the-transformer-module-architecture-come-from">2. Where does the transformer module architecture come from?</h3>
<p>Taking a bird&rsquo;s eye view of the evergrowing zoo of transformer architectures in natural language processing and computer vision suggests that the high-level design pattern introduced in <em><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is All You Need (Vaswani et al., 2017)</a></em><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> is still dominant. Almost all architectural variations of transformer modules published in the last four years have stuck to a successful combination of residual connections, an attention-like operation (token-mixing), normalization layers, and a feed-forward-like operation (channel-mixing).</p>
<p><a href="https://arxiv.org/abs/2111.11418" target=_blank><img src="metaformer.png" alt="MetaFormer architecture comparison" width="400px"/></a></p>
<p>Recent work like <em><a href="https://arxiv.org/abs/2111.11418" target="_blank" rel="noopener">MetaFormer is Actually What You Need for Vision (Yu et al., 2021)</a></em><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> appropriately shifts focus to the architecture of the full transformer module and argues that its full structure, rather than just the token-mixing attention operation, is essential for transformers to achieve competitive performance.</p>
<p>But where does this archetypical design pattern come from? Why does it seem to stick around? Is there any physical intuition behind it?</p>
<h3 id="3-deriving-attention-from-energy-functions-only-gets-you-so-far">3. Deriving attention from energy functions only gets you so far</h3>
<p>Recent papers like <em><a href="https://arxiv.org/abs/2008.02217" target="_blank" rel="noopener">Hopfield Networks is All You Need (Ramsauer et al., 2020)</a><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></em> and <em><a href="https://arxiv.org/abs/2008.06996" target="_blank" rel="noopener">Large Associative Memory Problem in Neurobiology and Machine Learning (Krotov and Hopfield, 2020)</a></em><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> have looked for physical intuition behind attention mechanisms using an <a href="https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/" target="_blank" rel="noopener">energy-based perspective</a> phrased in terms of modern continuous Hopfield networks. The main idea is to derive the softmax-attention update rule</p>
<p>\begin{equation}
\boldsymbol{Q}' = \text{softmax}\left( \frac{\boldsymbol{Q} \boldsymbol{K}^T}{\sqrt{d}} \right) \boldsymbol{K}
\end{equation}</p>
<p>by taking a large gradient descent update step using the derivative with respect to input queries $\boldsymbol{Q}$ of some judiciously chosen energy function</p>
<p>\begin{equation}
E = \frac{1}{2} \boldsymbol{Q} \boldsymbol{Q}^T -\mathrm{logsumexp} \left( \frac{\boldsymbol{Q} \boldsymbol{K}^T}{\sqrt{d}} \right). \label{eq:logsumexp}
\end{equation}</p>
<p>In this way, the vanilla softmax attention module can be recast as taking a <a href="https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/" target="_blank" rel="noopener">large gradient step according to some energy function</a>. The energy landscape defined by Eq. \eqref{eq:logsumexp} implements an associative memory system for storing and retrieving vector patterns where queries flow towards valleys associated with their nearest keys.</p>
<p><a href="https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/" target=_blank><img src="landscape.png" alt="Logsumexp energy function landscape" width="200px"/></a></p>
<p>But there is more to transformer modules than just attention. In practice, we know that residual connections, normalization layers, and feed-forward layers are all essential to obtain good empirical performance.</p>
<p>Can we generalize the physical intuition of taking derivatives with respect to an energy function to explain the full transformer module? Sure. But we have to take a step back from energy functions, forget about pecularities like softmax-attention, and focus on physical systems instead.</p>
<h3 id="4-back-to-the-roots-physical-spin-systems-and-vector-spin-models">4. Back to the roots: physical spin systems and vector-spin models</h3>
<p>Energy functions in classical statistical mechanics are succinct descriptions encoding interactions and constraints in physical systems. Spin systems are prototypical physical systems which often serve as toy models for all kinds of phenomena.</p>
<p>The Ising model is a famous toy model describing a classical binary spin system with local spin degrees of freedom at every site which point either up or down. The energy function of the binary random Ising model for $N$ spins in the presence of a site-dependent external magnetic field looks like</p>
<p>\begin{equation}
E = - \sum_{i,j=1}^{N} J_{ij} \sigma_{i} \sigma_{j} - \sum_{i=1}^{N} h_{i} \sigma_{i}, \label{eq:binaryrandomising}
\end{equation}</p>
<p>where the $J_{ij}$ encode coupling strengths between pairs of spins and the external magnetic fields $h_{i}$ act as biases by providing a preferential value of alignment at every site. The model defined by \eqref{eq:binaryrandomising} is also known as a <a href="https://en.wikipedia.org/wiki/Boltzmann_machine" target="_blank" rel="noopener">Boltzmann machine</a> or <a href="https://en.wikipedia.org/wiki/Hopfield_network" target="_blank" rel="noopener">Hopfield network</a>. A cartoon of this model looks like a graph of little arrows that are pairwise coupled:</p>
<img src="binary_ising.png" alt="Random Ising model configuration with binary spins" width="200px"/>
<p>At thermal equilibrium, the system&rsquo;s energy function determines what spin configurations (patterns of up/down spins) are preferred according to the Boltzmann probability distribution $p(\sigma) = e^{-\beta E\left( \sigma \right)} / Z$, where $Z = \sum_{\sigma} e^{-\beta E\left( \sigma \right)}$ denotes the partition function of the system and $\beta$ the inverse temperature. The partition function of a physical system is a magical object which relates the microscopic world to observable thermodynamic quantities via the free energy $F = - \beta^{-1} \log Z$. It is also shockingly hard to compute in most scenarios, but let&rsquo;s ignore that for now.</p>
<p>Even though binary spin models are nice, they rarely excite machine learning practitioners anymore nowadays. Since modern neural networks like transformers act on sets or sequences of vector data, it seems much more natural to consider <em>vector-spin models</em>. After replacing binary degrees of freedom with $d$-dimensional vector degrees of freedom, we can define an energy function</p>
<p>\begin{align}
E = - \sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} - \sum_{i=1}^{N} \boldsymbol{h}_{i} \cdot \boldsymbol{\sigma}_{i} \label{eq:vectorrandomising}
\end{align}</p>
<p>where the scalar products have turned into dot products.</p>
<img src="vector_ising.png" alt="Random Ising model configuration with vector spins" width="200px"/>
<p>Now how can we relate vector-spin systems like Eq. \eqref{eq:vectorrandomising} to neural networks?</p>
<h3 id="5-why-dont-we-just-probe-a-vector-spin-system-with-data">5. Why don’t we just probe a vector-spin system with data?</h3>
<p>Let&rsquo;s pursue an intuitive idea. Imagine we want to expose our vector-spin system Eq. \eqref{eq:vectorrandomising} to incoming data. We can do this by identifying data flowing into the system with the external magnetic field $(\boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N})$. We would then like to observe how the spin system responds to this particular environment of patterns. If all of the steps in the computation to arrive at the spin system&rsquo;s responses can be implemented in a differentiable way, then we should be able to engineer its collective behavior by optimizing the coupling parameters to better respond to future data. To implement this idea, we propose to observe spin-system responses in terms of <em>magnetizations computed from free energies</em>.</p>
<h3 id="6-a-slice-of-statistical-mechanics-magnetizations-and-free-energies">6. A slice of statistical mechanics: magnetizations and free energies</h3>
<p>For ease of notation, let us call the model parameters $\theta \equiv \{ J_{ij} \}$, the spins $\sigma \equiv \{ \boldsymbol{\sigma}_{i} \}$, and the external magnetic fields $h \equiv (\boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N})$. We can then schematically write our spin system&rsquo;s partition function as</p>
<p>\begin{align}
Z_{\theta} \left( h \right) = \int \mathrm{d} \sigma \ \mathrm{e}^{ - \beta E_{\theta}\left( \sigma, h \right) } \label{eq:partfun}
\end{align}</p>
<p>and the corresponding free energy as $F_{\theta} \left( h \right) = - \beta^{-1} \log Z_{\theta} \left( h \right)$.</p>
<p>Magnetizations are responses of our spin system to the external magnetic field imposed by $(\boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N})$. From standard thermodynamics, we know that we can calculate magnetizations from the free energy by differentiating with respect to the external field<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<p>\begin{align}
\boldsymbol{m}_{i} = - \frac{\mathrm{d} F_{\theta} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right)}{\mathrm{d} \boldsymbol{h}_{i}} = \langle \boldsymbol{\sigma}_{i} \rangle , \label{eq:sigma}
\end{align}</p>
<p>which, in this case, boils down to calculating spin expectation values. The magnetization for every site depends on the couplings and, through the couplings between spins, on the values of the external field at all sites. Magnetizations reveal how the spins will tend to align themselves when we place our spin system in an environment of patterns.</p>
<p>Before we move on, we will have to account for one more complication. If we want to draw a correspondence between transformers and spin systems, it looks like we will have to allow for couplings that depend on the external magnetic field. For example, the attention matrix in vanilla transformers looks something like</p>
<p>\begin{equation}
J_{ij} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right) = \left[\mathrm{softmax}\left( \frac{\boldsymbol{H} \boldsymbol{W}_{\boldsymbol{Q}} \boldsymbol{W}_{\boldsymbol{K}}^{T} \boldsymbol{H}^{T}}{\sqrt{d}} \right)\right]_{ij}, \label{eq:softmaxcouplings}
\end{equation}</p>
<p>where the matrix $\boldsymbol{H}$ denotes the stack of external magnetic field vectors and the interactions between spins are determined dynamically based on the inputs. From a physics perspective, this is very weird and highly unusual, but such is the transformer.</p>
<p>The potential dependency of the couplings on the external field changes the magnetization of Eq. \eqref{eq:sigma} to an expression of the form</p>
<p>\begin{align}
\boldsymbol{m}_{i} &amp;= - \frac{\mathrm{d} F_{\theta} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right)}{\mathrm{d} \boldsymbol{h}_{i}} \nonumber \\ &amp;= \langle \boldsymbol{\sigma}_{i} \rangle + \sum_{m,n} \langle \boldsymbol{\sigma}_{m} \cdot \boldsymbol{\sigma}_{n} \rangle \frac{\partial J_{mn} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right) }{ \partial \boldsymbol{h}_{i} } , \label{eq:sigmaweird}
\end{align}</p>
<p>which looks rather annoying. In practice, we can of course let an automatic differentiation framework keep track of dependencies so that we can get away with calculating</p>
<p>\begin{align}
\boldsymbol{m}_{i} = - \frac{\mathrm{d} F_{\theta} \left( \boldsymbol{h}_{1}, \boldsymbol{h}_{2}, \ldots, \boldsymbol{h}_{N} \right)}{\mathrm{d} \boldsymbol{h}_{i}}, \label{eq:magnetization}
\end{align}</p>
<p>assuming we have a differentiable expression for the (approximate) free energy available.</p>
<h3 id="7-turning-a-differentiable-spin-system-into-a-neural-network">7. Turning a differentiable spin system into a neural network</h3>
<p>Let&rsquo;s now use the ingredients defined above to construct a neural network module which wraps around a vector-spin system. Given the energy function Eq. \eqref{eq:vectorrandomising} and the free energy $F_{\theta} \left( h \right) = - \beta^{-1} \log \int \mathrm{d} \sigma \ \mathrm{e}^{ - \beta E_{\theta}\left( \sigma, h \right) }$, we let incoming data play the role of the external magnetic field and return magnetizations in response.</p>
<img src="spinmodule_new.png" alt="Spin system as a neural network" width="600px"/>
<p>But didn&rsquo;t we mention before that partition functions (and hence free energies and thus magnetizations) are shockingly hard to compute? Why introduce all these formal expressions if we cannot compute anything?</p>
<p>It turns out physicists have already developed several tricks and approximation methods that can be applied to vector-spin systems. Numerical evidence that the partition function approach outlined above <em>is</em> possible for vector-spin systems can be found in <a href="https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/" target="_blank" rel="noopener">Deep Implicit Attention</a> (below, left) and <a href="https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/" target="_blank" rel="noopener">Approximate Free Energy Minimization</a> (below, right). In these examples, approximations of the partition function Eq. \eqref{eq:partfun} were obtained following a mean-field theory and a steepest-descent approach, respectively. Our numerical implementations of both approaches rely internally on <a href="http://implicit-layers-tutorial.org/" target="_blank" rel="noopener">deep implicit layers</a> to ensure that fixed-point calculations and root-solving steps are efficiently differentiable.</p>
<img src="arch_dia_afem_new.png" alt="Deep implicit attention and approximate free-energy minimization" width="600px"/>
<h3 id="8-an-exercise-in-squinting-recognizing-the-transformer-module">8. An exercise in squinting: recognizing the transformer module</h3>
<p>In both <a href="https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/" target="_blank" rel="noopener">Deep Implicit Attention</a> and <a href="https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/" target="_blank" rel="noopener">Approximate Free Energy Minimization</a>, computing the magnetizations from (approximate) free energies according to Eq. \eqref{eq:magnetization} reveals a structure that is surprisingly familiar: a pattern of residual connections, token-mixing, normalizations, and channel-mixing.</p>
<p>Residual connections are proportional to the inputs and arise from the presence of the external magnetic field. Token-mixing contributions emerge from the coupling terms in the energy function and weigh and mix inputs without acting on the local vector-spin dimension. Normalization follows from normalizing external magnetic fields with respect to the vector dimension and requiring that the energy of the spin system remain linearly proportional to the number of lattice sites. Channel-mixing terms collect terms in the magnetization that can be applied locally, like Onsager self-correction terms in mean-field approaches or contributions coming from input-dependent couplings (see Eq. \eqref{eq:sigmaweird}).</p>
<p>These observations suggest that we can picture the forward pass of a transformer module as a wrapper around a vector-spin system: module inputs are routed to the external magnetic field (and, optionally, to a parametrized couplings function) and magnetizations are returned as outputs. The transformer module bears an uncanny resemblance to a differentiable physical system whose collective behavior we can control through training.</p>
<h3 id="9-training-transformer-modules-shapes-collective-behavior">9. Training transformer modules shapes collective behavior</h3>
<p>Now that we can picture transformer modules as physical spin systems responding to getting probed with data, let&rsquo;s imagine what training them looks like.</p>
<p>On the level of the energy function of our spin system Eq. \eqref{eq:vectorrandomising}, we can model the training process of a transformer module by introducing a (discrete) time dimension and making the external magnetic field time-dependent, leading to<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup></p>
<p>\begin{equation}
E(t) = - \sum_{i,j=1}^{N} J_{ij} \; \boldsymbol{\sigma}_{i} \cdot \boldsymbol{\sigma}_{j} - \sum_{i=1}^{N} \boldsymbol{h}_{i}(t) \cdot \boldsymbol{\sigma}_{i} \label{eq:sloppyenergy}
\end{equation}</p>
<p>At every training step $t$, a sequence of incoming data $\{ \boldsymbol{h}_{1}(t), \boldsymbol{h}_{2}(t), \ldots, \boldsymbol{h}_{N}(t) \}$ takes on the role of external magnetic field. During the forward pass, magnetizations $\boldsymbol{m}_{i}$ are computed in a differentiable way according to the current model parameters and in the presence of the current external magnetic field. During the backward pass, the module&rsquo;s coupling parameters $J_{ij}$ get updated, nudging the interactions in the spin system which in turn will influence its magnetization responses to similar data in future iterations.</p>
<img src="spinmoduletraining_new.png" alt="Training a spin system as a neural network" width="600px"/>
<p>We can think about this training process as gradually shaping the collective behavior of a differentiable vector-spin system that is driven by data. If the couplings depend on the inputs, like in Eq. \eqref{eq:softmaxcouplings}, we should make the couplings time-dependent as well in Eq. \eqref{eq:sloppyenergy}. In that case, the external magnetic fields as well as the parametrized couplings change instantaneously at every training step.</p>
<h3 id="10-training-deep-transformers-orchestrates-spin-system-collectives">10. Training deep transformers orchestrates spin-system collectives</h3>
<p>Training a deep transformer model corresponds to orchestrating a stack of transformer modules by building up a differentiable structure of correlations where the magnetizations of one spin system drive the next one. Wiggling billions of parameters during training then nudges the cascading response behavior of the collective of spin systems to better adapt to the collective&rsquo;s (meta-)tasks as specified by the data and the loss function.</p>
<img src="transformertraining_new.png" alt="Training a transformer" width="500px"/>
<h3 id="11-conclusion">11. Conclusion</h3>
<p>In this post, we have argued that the forward pass of a transformer module maps onto computing magnetizations in a vector-spin model responding to data. Generalizing previous work on understanding softmax attention modules in terms of modern continuous Hopfield networks by taking derivatives of a judiciously chosen <em>energy</em> function, we propose to take derivatives of the <em>free energy</em> of a general vector-spin system to get to the architecture of a full transformer module.</p>
<p>By zooming out and approaching transformers from a tangential, statistical-mechanical point of view, we arrived at a physical intuition of transformers that seems hard to obtain when restricting oneself to perturbing explicit neural network architectures. Recognizing transformer modules as spin models in disguise might not only unify architectural variations but also elucidate the high-level architectural convergence and empirical success of transformers in deep learning.</p>
<h2 id="references--footnotes">References &amp; footnotes</h2>
<p>If you happen to find this work useful, please consider citing it as:</p>
<pre><code>@article{bal2021isingisallyouneed,
  title   = {Transformers Are Secretly Collectives of Spin Systems},
  author  = {Bal, Matthias},
  year    = {2021},
  month   = {November},
  url     = {https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/}
}
</code></pre>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><em>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> (2017)</em>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><em>Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan, <a href="https://arxiv.org/abs/2111.11418" target="_blank" rel="noopener">MetaFormer is Actually What You Need for Vision</a> (2021)</em>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><em>Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, <a href="https://arxiv.org/abs/2008.02217" target="_blank" rel="noopener">Hopfield Networks is All You Need</a> (2020)</em>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p><em>Dmitry Krotov and John Hopfield, <a href="https://arxiv.org/abs/2008.06996" target="_blank" rel="noopener">Large Associative Memory Problem in Neurobiology and Machine Learning</a> (2020)</em>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>For example, see the content of Chapter 2 in the <a href="https://giamarchi.unige.ch/local/people/thierry.giamarchi/pdf/cours_sft.pdf" target="_blank" rel="noopener">lecture notes on statistical field theory</a> by Thierry Giamarchi.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6" role="doc-endnote">
<p>The time-dependence in Eq. \eqref{eq:sloppyenergy} smells of non-equilibrium statistical mechanics. Incoming data might be considered as time-dependent &ldquo;probes&rdquo; which inject energy (and useful information if its content is low-entropy enough) into a non-equilibrium system. By nudging its dynamical response behavior across spatiotemporal scales, the system could potentially learn how to deal with being driven by all kinds of patterns in incoming data. For an interesting toy example of such behavior, see <a href="https://youtu.be/vSgHuErXuqk?t=2188" target="_blank" rel="noopener">this talk</a> by Jeremy England on <em>Low rattling: a principle for understanding driven many-body self-organization</em>.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/artificial-intelligence/">Artificial Intelligence</a>
  
  <a class="badge badge-light" href="/tag/associative-memories/">Associative Memories</a>
  
  <a class="badge badge-light" href="/tag/attention/">Attention</a>
  
  <a class="badge badge-light" href="/tag/boltzmann-machine/">Boltzmann Machine</a>
  
  <a class="badge badge-light" href="/tag/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tag/emergent-collective-computational-capabilities/">Emergent Collective Computational Capabilities</a>
  
  <a class="badge badge-light" href="/tag/free-energy/">Free Energy</a>
  
  <a class="badge badge-light" href="/tag/hopfield-networks/">Hopfield Networks</a>
  
  <a class="badge badge-light" href="/tag/ising-models/">Ising Models</a>
  
  <a class="badge badge-light" href="/tag/neural-networks/">Neural Networks</a>
  
  <a class="badge badge-light" href="/tag/partition-function/">Partition Function</a>
  
  <a class="badge badge-light" href="/tag/statistical-physics/">Statistical Physics</a>
  
  <a class="badge badge-light" href="/tag/transformers/">Transformers</a>
  
  <a class="badge badge-light" href="/tag/vector-spin-models/">Vector-Spin Models</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&amp;text=Transformers%20Are%20Secretly%20Collectives%20of%20Spin%20Systems" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&amp;t=Transformers%20Are%20Secretly%20Collectives%20of%20Spin%20Systems" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Transformers%20Are%20Secretly%20Collectives%20of%20Spin%20Systems&amp;body=https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&amp;title=Transformers%20Are%20Secretly%20Collectives%20of%20Spin%20Systems" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Transformers%20Are%20Secretly%20Collectives%20of%20Spin%20Systems%20https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/&amp;title=Transformers%20Are%20Secretly%20Collectives%20of%20Spin%20Systems" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://mcbal.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/matthias-bal/avatar_huf6181012a34ccf45ad256514680767eb_86031_270x270_fill_q90_lanczos_center.jpg" alt="Matthias Bal"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://mcbal.github.io/">Matthias Bal</a></h5>
      
      <p class="card-text">Machine learning engineer with a background in physics.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatthiasBal" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matthiasbal/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.be/citations?user=vjYY0bMAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/mcbal" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>










<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/transformers-from-spin-models-approximate-free-energy-minimization/" rel="prev">Transformers from Spin Models: Approximate Free Energy Minimization</a>
  </div>
  
</div>

</div>





  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/transformers-from-spin-models-approximate-free-energy-minimization/">Transformers from Spin Models: Approximate Free Energy Minimization</a></li>
      
      <li><a href="/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/">Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</a></li>
      
      <li><a href="/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/">An Energy-Based Perspective on Attention Mechanisms in Transformers</a></li>
      
      <li><a href="/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/">Transformer Attention as an Implicit Mixture of Effective Energy-Based Models</a></li>
      
      <li><a href="/post/attention-as-energy-minimization-visualizing-energy-landscapes/">Attention as Energy Minimization: Visualizing Energy Landscapes</a></li>
      
    </ul>
  </div>
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/bash.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/wowchemy.min.434af0ebce9e15b273b954d65feb39c7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Matthias Bal © 2021
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
