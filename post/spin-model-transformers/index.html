<!DOCTYPE html><html lang="en-gb" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 4.8.0 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Matthias Bal">

  
  
  
    
  
  <meta name="description" content="Exploring connections between transformer neural networks, spin models, and (non-)equilibrium statistical mechanics">

  
  <link rel="alternate" hreflang="en-gb" href="https://mcbal.github.io/post/spin-model-transformers/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Serif:ital,wght@0,300;0,400;1,300;1,400%7CIBM+Plex+Sans:ital,wght@0,300;0,400;0,700;1,400%7CFira+Code&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=G-H9Y98B2S2P"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-H9Y98B2S2P', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://mcbal.github.io/post/spin-model-transformers/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="mcbal">
  <meta property="og:url" content="https://mcbal.github.io/post/spin-model-transformers/">
  <meta property="og:title" content="Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules | mcbal">
  <meta property="og:description" content="Exploring connections between transformer neural networks, spin models, and (non-)equilibrium statistical mechanics"><meta property="og:image" content="https://mcbal.github.io/post/spin-model-transformers/featured.jpg">
  <meta property="twitter:image" content="https://mcbal.github.io/post/spin-model-transformers/featured.jpg"><meta property="og:locale" content="en-gb">
  
    
      <meta property="article:published_time" content="2022-06-19T09:28:17&#43;01:00">
    
    <meta property="article:modified_time" content="2023-05-01T22:27:18&#43;01:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mcbal.github.io/post/spin-model-transformers/"
  },
  "headline": "Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules",
  
  "image": [
    "https://mcbal.github.io/post/spin-model-transformers/featured.jpg"
  ],
  
  "datePublished": "2022-06-19T09:28:17+01:00",
  "dateModified": "2023-05-01T22:27:18+01:00",
  
  "author": {
    "@type": "Person",
    "name": "Matthias Bal"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "mcbal",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mcbal.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Exploring connections between transformer neural networks, spin models, and (non-)equilibrium statistical mechanics"
}
</script>

  

  


  


  





  <title>Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules | mcbal</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class=" ">

  
  
  
    <script>window.wcDarkLightEnabled = true;</script>
  
  
    <script>const isSiteThemeDark = false;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">mcbal</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">mcbal</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>



  <article class="article">

  




















  
  


<div class="article-container pt-3">
  <h1>Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules</h1>

  
  <p class="page-subtitle">Exploring connections between transformer neural networks, spin models, and (non-)equilibrium statistical mechanics</p>
  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    May 1, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 600px; max-height: 400px;">
  <div style="position: relative">
    <img src="/post/spin-model-transformers/featured.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 id="1-introduction">1. Introduction</h1>
<blockquote>
<p><strong>✨ This is a work in progress. Implementations in JAX of the content outlined in this post should become available at <a href="https://github.com/mcbal/spin-model-transformers" target="_blank" rel="noopener"><code>mcbal/spin-model-transformers</code></a>. Come join the fun if you&rsquo;d like to see things moving.</strong></p>
</blockquote>
<p>In a series of previous <a href="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/" target="_blank" rel="noopener">blog posts</a>, we have tried to connect the forward pass of a transformer neural-network module to computing mean magnetizations in disordered Ising-like vector-spin models with parametrized couplings and external magnetic fields. In this picture, the forward pass of a transformer module computes statistical observables given a specific realization of quenched couplings and external magnetic fields while the backward pass nudges the parametrized couplings and external magnetic fields to better respond to the demands of the training loss.</p>
<p>Both the mean-field message-passing approach of <a href="https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/" target="_blank" rel="noopener">Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</a> and the saddle-point free-energy approach of <a href="https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/" target="_blank" rel="noopener">Transformers from Spin Models: Approximate Free Energy Minimization</a> rely on methods that are only well-defined for spin models with symmetric coupling matrices, whose stochastic dynamics obey detailed balance and converge to a steady-state equilibrium characterized by the Boltzmann distribution. The softmax attention operation in transformer modules is however famously asymmetric.</p>
<p>To capture spin models with asymmetric coupling matrices, we had better consider non-equilibrium systems, whose dynamics can be pretty wild yet sometimes lead to a relaxation to a non-equilibrium steady state. In the last few decades, dynamical mean-field approaches have been developed for the binary kinetic Ising model, which exhibits non-equilibrium behavior when couplings are asymmetric or when model parameters are subject to rapid changes. In this post, we try to generalize one of these approaches from binary spins to vector spins with the aim of comparing the resulting equations for the mean magnetizations to the forward pass of a transformer module.</p>
<h1 id="2-mean-field-theory-of-asymmetric-ising-models-with-binary-spins">2. Mean-field theory of asymmetric Ising models with binary spins</h1>
<p>In this section, we review known results on mean-field theory approaches to capturing the stochastic dynamics of binary kinetic Ising models. We primarily follow the discussion outlined in <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener"><em>A unifying framework for mean-field theories of asymmetric kinetic Ising systems</em> (Aguilera et al., 2021)</a>.</p>
<h2 id="21-setting-the-scene-the-kinetic-ising-model">2.1. Setting the scene: the kinetic Ising model</h2>
<img src="binary_spins.png" alt="Random Ising model configuration with binary spins" width="250px"/>
<p>We consider a kinetic Ising model describing a system made up of $N$ interacting binary spins $s_{i,t} \in \{-1, 1\}$ that evolve in discrete time steps $t$ according to synchronous dynamics, i.e. all spins get updated at the same time in parallel. Given a spin configuration $\mathbf{s}_{t-1} = \{ s_{1,t-1}, s_{2,t-1}, \ldots, s_{N,t-1} \}$ at time $t-1$, we consider the spins $\mathbf{s}_{t}$ at time $t$ to be conditionally independent random variables captured by a discrete-time Markov chain transition probability</p>
<p>\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{\sum_{s_{i,t}} \mathrm{e}^{s_{i,t} h_{i,t}}} = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{2 \cosh h_{i,t}}, \label{eq:pcond}
\end{equation}</p>
<p>where the effective external field is given by</p>
<p>\begin{equation}
h_{i,t} = x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1}.
\end{equation}</p>
<p>Here, the parameters $\mathbf{x}$ represent the (possibly time-dependent) local external fields at each site while the coupling parameters $\mathbf{J}$ are a specific realization of quenched disorder encoding the interactions between pairs of spins. Using the probability mass function of the previous state $P( \mathbf{s}_{t-1} )$ we can write the distribution of the current state as</p>
<p>\begin{equation}
P( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1}} P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P( \mathbf{s}_{t-1} ), \label{eq:marginal}
\end{equation}</p>
<p>which, when applied recursively, traces the evolution of the system starting from some initial distribution $P( \mathbf{s}_{0} )$. Unless we turn off the couplings by setting $\mathbf{J} = \mathbf{0}$, the marginal distribution $P( \mathbf{s}_{t} )$ is not factorized and tends to be quite complicated. Our goal is to compute statistical properties of the system, such as the mean magnetizations</p>
<p>\begin{equation}
m_{i,t} = \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t} ),
\end{equation}</p>
<p>as well as correlations</p>
<p>\begin{equation}
C_{ik,t} = \sum_{\mathbf{s}_{t}} s_{i,t} s_{k,t} P( \mathbf{s}_{t} ) - m_{i,t} m_{k,t},
\end{equation}</p>
<p>and delayed correlations</p>
<p>\begin{equation}
D_{il,t} = \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1}} s_{i,t} s_{l,t-1} P( \mathbf{s}_{t}, \mathbf{s}_{t-1} ) - m_{i,t} m_{l,t-1}.
\end{equation}</p>
<p>Since the above expressions involve summing over a large amount of possible spin configurations, they are not very useful in practice. So we will try to approximate the tricky marginal distribution $P( \mathbf{s}_{t} )$ defined in Eq. \eqref{eq:marginal} using a mean-field theory approach.</p>
<h2 id="22-mean-field-approximation">2.2. Mean-field approximation</h2>
<p>Mean-field theory tries to approximate a complicated object ${\color{red}P}$ by wiggling around the parameters of a simple, analytically tractable parametrized ansatz ${\color{green}Q_{\theta}}$ to get as close as possible to ${\color{red}P}$. At risk of inducing headaches in mathematicians by calling everything a manifold, we can picture what&rsquo;s going on geometrically as trying to approximate a target probability distribution $P( \mathbf{s}_{t} \vert \mathbf{x}, \mathbf{J})$ and its statistical properties $\mathbf{m}_{t}$, $\mathbf{C}_{t}$, and $\mathbf{D}_{t}$ by restricting ourselves to a submanifold of tractable probability distributions. A particularly convenient submanifold is that of factorized models, where each point on the submanifold corresponds to a distribution parametrized by a vector $\boldsymbol{\theta}_{t}$,</p>
<p>\begin{equation}
Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} \theta_{i,t}}}{2 \cosh \theta_{i,t}}, \label{eq:q}
\end{equation}</p>
<p>so that the mean magnetizations are simply given by</p>
<p>\begin{equation}
m_{i,t} = \tanh \theta_{i,t}
\end{equation}</p>
<p>as there are no couplings between spins. The factorized model $Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}^{*}_{t} )$ that minimizes the Kullback-Leibler (KL) divergence</p>
<p>\begin{equation}
D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) = \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log \frac{P( \mathbf{s}_{t})}{Q_{\theta}( \mathbf{s}_{t})} \label{eq:kl}
\end{equation}</p>
<p>has mean magnetizations $\mathbf{m}_{t}$ identical to those of the target distribution $P( \mathbf{s}_{t})$ since, for all spins $i=1,2,\ldots,N$, we find that</p>
<p>\begin{align}
\frac{\partial D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}} &amp;= - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \frac{\partial \log Q_{\theta}( \mathbf{s}_{t}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}}  \\
&amp;= - \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t}) + \tanh \theta^{*}_{i,t} \\
&amp;= -m^{{\color{red}P}}_{i,t} + m^{{\color{green}Q_{\theta^{*}}}}_{i,t} = 0, \label{eq:klm}
\end{align}</p>
<p>where $m^{{\color{red}P}}_{i,t}$ and $m^{{\color{green}Q_{\theta^{*}}}}_{i,t}$ respectively denote the expectation values of $s_{i,t}$ with respect to ${\color{red}P}$ and ${\color{green}Q_{\theta^{*}}}$. Indeed, minimizing $D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}})$ tries to cover the modes of ${\color{red}P}$ by moment matching since the expectation value in Eq. \eqref{eq:kl} is calculated with respect to ${\color{red}P}$.</p>
<h2 id="23-the-plefka-expansion">2.3. The Plefka expansion</h2>
<p>Great, but is it even possible to find the parameters</p>
<p>\begin{equation}
\DeclareMathOperator*{\argmin}{arg\,min}
\boldsymbol{\theta}^{*}_{t} = \argmin_{\boldsymbol{\theta}_{t}} \left( - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log Q( \mathbf{s}_{t}) \right)
\end{equation}</p>
<p>that minimize the KL divergence? Well, that&rsquo;s going to be hard, unless you already know the target distribution $P( \mathbf{s}_{t})$, or you have a clever way of approximately evaluating the expectation value of $\log {\color{green}Q_{\theta}}$ with respect to ${\color{red}P}$. So let&rsquo;s introduce some more distributions to get around this issue. To apply the Plefka expansion to our problem, we introduce the conditional distribution</p>
<p>\begin{equation}
P_{\alpha}( \mathbf{s}_{t}\vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N}  \frac{\mathrm{e}^{s_{i,t} h_{i,t}(\alpha) }}{2 \cosh h_{i,t}(\alpha)}, \label{eq:pcondalt}
\end{equation}</p>
<p>\begin{equation}
h_{i,t}(\alpha) = (1-\alpha) \theta_{i,t} + \alpha \left( x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1} \right), \label{eq:pcondalth}
\end{equation}</p>
<p>parametrized by a scalar $\alpha$ interpolating between $P_{\alpha=0}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} )$ (Eq. \eqref{eq:q}) and $P_{\alpha=1}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} )$ (Eq. \eqref{eq:pcond}). Using Eq. \eqref{eq:pcondalt}, we can construct an approximate marginal distribution $P_{\alpha}( \mathbf{s}_{t})$, leading to $\alpha$-dependent statistical properties $\mathbf{m}_{t}(\alpha)$, $\mathbf{C}_{t}(\alpha)$, and $\mathbf{D}_{t}(\alpha)$ for the approximate system. The Plefka expansion then boils down to writing these properties as Taylor series expansions around the factorized model $\alpha=0$. For the mean magnetizations, the expansion up to $n$-th order looks like</p>
<p>\begin{equation}
\mathbf{m}_{t}(\alpha) = \mathbf{m}_{t}(\alpha=0) + \sum_{k=1}^{n} \frac{\alpha^k}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}), \label{eq:mtaylor}
\end{equation}</p>
<p>where all coefficients in the expansion are functions of $\boldsymbol{\theta}_{t}$ via Eq. \eqref{eq:pcondalth}. The mean-field approximation is computed by setting $\alpha=1$ so that the original marginal distribution is recovered and Eq. \eqref{eq:klm} holds, which implies that $\mathbf{m}_{t}(\alpha=1) = \mathbf{m}_{t}(\alpha=0)$ and thus</p>
<p>\begin{equation}
\sum_{k=1}^{n} \frac{1}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}) = 0. \label{eq:mftheta}
\end{equation}</p>
<p>Finally, we solve Eq. \eqref{eq:mftheta} for $\boldsymbol{\theta}_{t}$ to find the mean-field values $\boldsymbol{\theta}^{*}_{t}$ of the parameters of the distribution Eq. \eqref{eq:q}. Physically, we are tuning the effective external magnetic fields of the factorized ansatz to $\boldsymbol{\theta}^{*}_{t}$ so that its approximate mean magnetizations get as close as possible to the real ones.</p>
<h2 id="24-recovering-naive-mean-field-and-thouless-anderson-palmer-approximations">2.4. Recovering naive mean-field and Thouless-Anderson-Palmer approximations</h2>
<p>We now consider first and second order approximations of the mean magnetizations Eq. \eqref{eq:mtaylor} to recover respectively the naive mean-field and Thouless-Anderson-Palmer (TAP) approximations for the binary kinetic Ising model. The starting point is a Plefka expansion around factorized models at times $t-1$ and $t$. From Eq. \eqref{eq:marginal} and Eq. \eqref{eq:pcondalt}, we construct a marginal probability distribution</p>
<p>\begin{equation}
P^{[t-1:t]}_{\alpha}( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ),
\end{equation}</p>
<p>interpolating between $P^{[t-1:t]}_{\alpha=0}( \mathbf{s}_{t} ) = Q( \mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\alpha=1}( \mathbf{s}_{t} ) = P( \mathbf{s}_{t} )$. The corresponding mean magnetizations are</p>
<p>\begin{align}
m_{i,t}(\alpha) &amp;= \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1},\mathbf{s}_{t-2}} s_{i,t} \, P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ) \\
&amp;= \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} \tanh h_{i,t}(\alpha) \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} )
\end{align}</p>
<p>Following Eq. \eqref{eq:mftheta}, the first-order approximation should satisfy</p>
<p>\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} = \left( 1-m^{2}_{i,t} \right) \left( -\theta_{i,t} + x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right) = 0,
\end{equation}</p>
<p>so that $\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1}$ and we end up with the naive mean-field equations</p>
<p>\begin{equation}
m_{i,t} \approx \tanh \theta^{*}_{i,t} = \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right). \label{eq:naivem}
\end{equation}</p>
<p>Again following Eq. \eqref{eq:mftheta}, the second-order approximation should satisfy</p>
<p>\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} + \frac{1}{2} \frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial^{2}\alpha} = 0,
\end{equation}</p>
<p>where the second-order derivative, neglecting terms higher than $\mathcal{O}(\alpha^2)$, is</p>
<p>\begin{equation}
\frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial^{2}\alpha} \approx -2 m_{i,t} \left( 1-m^{2}_{i,t} \right) \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}</p>
<p>so that</p>
<p>\begin{equation}
\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}</p>
<p>and we end up with the TAP mean-field equations</p>
<p>\begin{equation}
m_{i,t} \approx \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right) \right). \label{eq:tapm}
\end{equation}</p>
<p>We refer to <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">(Aguilera et al., 2020)</a> for full derivations and variations of the above mean-field results as well as variations based on different approximations of the marginal distribution $P( \mathbf{s}_{t} )$. The mean-field equations obtained above can also be elegantly derived using a Legendre transformation of the generating functional of the set of trajectories of the model, as outlined in e.g. <a href="https://arxiv.org/abs/1103.1044" target="_blank" rel="noopener"><em>Dynamical TAP equations for non-equilibrium Ising spin glasses (2011)</em></a>. It is also possible to derive second-order TAP approximations of the correlations</p>
<p>\begin{equation}
C_{ik,t} \approx \begin{cases}
1 - m^{2}_{i,t}  &amp; i = k \\
\left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{k,t} \right) \sum_{j} J_{ij} J_{kj} \left( 1-m^{2}_{j,t-1} \right) &amp; i \neq k \label{eq:tapc}
\end{cases}
\end{equation}</p>
<p>and delayed correlations</p>
<p>\begin{equation}
D_{il,t} \approx J_{il} \left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{l,t-1} \right) \left( 1 + 2 J_{il} m_{i,t} m_{l,t-1} \right). \label{eq:tapd}
\end{equation}</p>
<p>In summary, given the mean magnetizations $\mathbf{m}_{t-1}$ of the system at time $t-1$, we can use equations \eqref{eq:tapm} \eqref{eq:tapc} \eqref{eq:tapd} to compute a tuple $(\mathbf{m}_{t},\mathbf{C}_{t},\mathbf{D}_{t})$ of approximate statistical properties  of the system at time $t$. The time evolution of the system can thus be captured at the mean-field level by recursively computing $\mathbf{m}_{t}$ starting from an initial state $\mathbf{m}_{0}$, although approximation errors accumulate over the iterations.</p>
<h2 id="25-a-simple-jax-implementation">2.5. A simple JAX implementation</h2>
<p>To get more insight into what&rsquo;s going on, let&rsquo;s translate the mean-field update equations \eqref{eq:naivem} and \eqref{eq:tapm} for the mean magnetizations into code. But before we show a few examples, we need to know a bit more about what model we are actually simulating here. In <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">(Aguilera et al., 2020)</a>, the authors derive a solution of the asymmetric version of the kinetic <a href="https://en.wikipedia.org/wiki/Spin_glass#Sherrington%E2%80%93Kirkpatrick_model" target="_blank" rel="noopener">Sherrington-Kirkpatrick mean-field spin-glass model</a> using a generating functional or dynamical partition function approach capturing the distribution of the trajectories. They consider the same kinetic Ising model as in Eq. \eqref{eq:pcond} but with an inverse temperature parameter $\beta$ in the exponentials:</p>
<p>\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta s_{i,t} h_{i,t}}}{2 \cosh \beta h_{i,t}}.
\end{equation}</p>
<p>For Gaussian couplings $J_{ij} \sim \mathcal{N}\left( J_{\mu} / N, J^{2}_{\sigma} / N\right)$ and uniformly distributed external magnetic fields $x_{i} \sim \mathcal{U}(-X_{0}, X_{0})$, they show the existence of a ferromagnetic phase transition. In particular for $X_{0}=0.5$, $J_{\mu}=1.0$, and $J_{\sigma}=0.1$, the phase transition happens when tuning $\beta$ to a critical $\beta_{c} \approx 1.1108$.</p>
<h3 id="simulating-mean-field-dynamics">Simulating mean-field dynamics</h3>
<p>We now turn to a JAX implementation of the mean-field time evolution of the magnetizations according to the model described above. We use <code>lax.scan</code> to implement the update steps and <code>vmap</code> to parallelize trajectories starting from a batch of initial magnetization configurations $\mathbf{m}_{0}$.</p>
<pre><code class="language-python">from functools import partial

import jax
import jax.lax as lax
import jax.numpy as jnp

from jaxopt import AndersonAcceleration


def update_naive_mf(m0, _, x, J):
    m1 = jnp.tanh(x + jnp.einsum(&quot;... i j, ... j -&gt; ... i&quot;, J, m0))
    return m1, m0


def update_tap_mf(m0, _, x, J):
    def tap(m):
        return jnp.tanh(
            x
            + jnp.einsum(&quot;... i j, ... j -&gt; ... i&quot;, J, m0)
            - m * jnp.einsum(&quot;... i j, ... j -&gt; ... i&quot;, J**2, (1.0 - m0**2))
        )

    m1 = AndersonAcceleration(fixed_point_fun=tap, tol=1e-3, maxiter=10).run(m0).params
    return m1, m0


def time_evolution(m0, steps, update_fun):
    final_carry, stacked_outputs = lax.scan(update_fun, init=m0, xs=steps)
    return final_carry, stacked_outputs


def init_params(key, N, beta, X0, J_mu, J_sigma):
    x_key, J_key = jax.random.split(key)
    x = jax.random.uniform(x_key, shape=(N,), minval=-beta * X0, maxval=beta * X0)
    J = beta * J_mu * N**-1 + beta * J_sigma * N**-0.5 * jax.random.normal(
        J_key, shape=(N, N)
    )
    return x, J


def simulate(
    key, m0, steps, beta, X0=0.5, J_mu=1.0, J_sigma=0.1, update_fun=update_tap_mf
):
    x, J = init_params(key, m0.shape[-1], beta, X0, J_mu, J_sigma)
    wrapped_time_evolution = partial(
        time_evolution,
        steps=steps,
        update_fun=partial(update_fun, x=x, J=J),
    )
    final_carry, stacked_outputs = jax.vmap(wrapped_time_evolution)(m0)
    return final_carry, stacked_outputs
</code></pre>
<h3 id="naive-mean-field-update-vs-thouless-anderson-palmer-tap">Naive mean-field update vs. Thouless-Anderson-Palmer (TAP)</h3>
<p>We fix the seed for the randomly initialized model parameters $\mathbf{x}$ and $\mathbf{J}$ and simulate $N=512$ spins at the critical temperature $\beta_{c}$ for $t=128$ time steps starting from an all-ones intial state. We first consider the naive mean-field update step.</p>
<p><img src="binary_plot_1.png" alt=""></p>
<p>The left axis shows the individual magnetization trajectories for each spin plotted horizontally while the red line associated to the right axis describes the average of the magnetizations across all spins for each time step. We observe convergence to what looks like a steady state. Comparing the naive first-order mean-field update equations to the second-order Thouless-Anderson-Palmer (TAP) ones, we observe lower values for the mean magnetization across all spins, which <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">(Aguilera et al., 2020)</a> showed to be closer to ground truth values obtained via sampling and averaging spin configurations.</p>
<p><img src="binary_plot_2.png" alt=""></p>
<h3 id="sampling-trajectories">Sampling trajectories</h3>
<p>Let us now consider 100 randomly-initialized initial states with their associated trajectories in three different model regimes: below ($\beta=\beta_c / 2 $), near ($\beta=\beta_c$), and above ($\beta=2 \beta_c$) the critical point.</p>
<p><img src="binary_plot_3.png" alt=""></p>
<p>We observe that the trajectories of randomly-initialized initial states converge to identical final states in each regime. These states map to a simple Ising phase diagram, where a high-temperature disordered phase $\langle m_{i,t} \rangle \to 0$ is separated from a low-temperature locally-ordered phase $\langle m_{i,t} \rangle \to \pm 1$ by a critical point. The behavior around $\beta=\beta_{c}$ is pretty interesting though: the non-equilibrium steady state looks like a non-trivial configuration implicitly encoded in the dynamics of the model.</p>
<h3 id="sampling-model-parameters">Sampling model parameters</h3>
<p>Let&rsquo;s go back to considering just a single trajectory since we just saw that all trajectories seem to converge to the same states for fixed model parameters. To get a feel for the variation of the steady-state values across different realizations of model parameters, it would be interesting to plot the absolute value<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> $| \langle m_{i} \rangle |$ of the final steady-state magnetizations across say 100 samples of model parameters and a range of inverse temperatures. We&rsquo;re using JAX, so we can easily sample model parameters by <code>vmap</code>&lsquo;ing the random key fed into the <code>simulate</code> function followed by another <code>vmap</code> to sweep across $\beta$.</p>
<p><img src="binary_plot_4.png" alt=""></p>
<p>$\ldots$</p>
<h1 id="3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins">3. Mean-field theory of asymmetric Ising models with vector spins</h1>
<p>Let us now transpose the binary-spin results to a setting where the local spin degrees of freedom are $D$-dimensional vector spins restricted to wiggle around on $(D-1)$-dimensional spheres.</p>
<img src="vector_spins.png" alt="Random Ising model configuration with vector spins" width="250px"/>
<h2 id="31-">3.1. &hellip;</h2>
<p>We can think of the denominator as a single site partition function of a local energy function measuring dot-product alignment to an external magnetic field. The integral over the vector-spin degree of freedom smells of Bessel functions but let&rsquo;s see what pops out.</p>
<p>$\ldots$</p>
<h1 id="4-a-family-of-transformer-like-modules">4. A family of transformer-like modules</h1>
<p>$\ldots$</p>
<h2 id="41-fast--and-slow-moving-parameters">4.1. Fast- and slow-moving parameters</h2>
<p>$\ldots$</p>
<h1 id="5-conclusion">5. Conclusion</h1>
<p>$\ldots$</p>
<h1 id="related-work">Related work</h1>
<p>A non-exhaustive list of references and inspiration includes:</p>
<ul>
<li>
<p>M. Aguilera, S.A. Moosavi, and H. Shimazaki, A unifying framework for mean-field theories of asymmetric kinetic Ising systems, <em>Nat Commun</em> <strong>12</strong>, 1197 (2021). <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">https://arxiv.org/abs/2002.04309</a></p>
</li>
<li>
<p>H.C. Nguyen, R. Zecchina, and J. Berg, Inverse statistical problems: from the inverse Ising problem to data science, <em>Advances in Physics</em>, 66:3, 197-261 (2017). <a href="https://arxiv.org/abs/1702.01522" target="_blank" rel="noopener">https://arxiv.org/abs/1702.01522</a></p>
</li>
<li>
<p>B. Bravi, P. Sollich, and M. Opper, Extended Plefka Expansion for Stochastic Dynamics, <em>J. Phys. A: Math. Theor.</em> <strong>49</strong>, 194003 (2016). <a href="https://arxiv.org/abs/1509.07066" target="_blank" rel="noopener">https://arxiv.org/abs/1509.07066</a></p>
</li>
<li>
<p>Y. Roudi and J. Hertz, Dynamical TAP equations for non-equilibrium Ising spin glasses, <em>J. Stat. Mech.</em>, P03031 (2011) <a href="https://arxiv.org/abs/1103.1044" target="_blank" rel="noopener">https://arxiv.org/abs/1103.1044</a></p>
</li>
<li>
<p>H.J. Kappen and J.J. Spanjers, Mean field theory for asymmetric neural networks, <em>Phys. Rev. E</em> <strong>61</strong>, 5658 (2000) <a href="https://repository.ubn.ru.nl/handle/2066/112702" target="_blank" rel="noopener">https://repository.ubn.ru.nl/handle/2066/112702</a></p>
</li>
</ul>
<h1 id="appendices">Appendices</h1>
<h1 id="references--footnotes">References &amp; footnotes</h1>
<p>If you happen to find this work useful, please consider citing it as:</p>
<pre><code>@article{bal2023spinmodeltransformers,
  title   = {Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules},
  author  = {Bal, Matthias},
  year    = {2023},
  month   = {?},
  url     = {https://mcbal.github.io/post/spin-model-transformers}
}
</code></pre>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>We plot the absolute value to get rid of artificial &ldquo;jumps&rdquo; between the two branches. These occur because all models are simulated independently when sweeping across $\beta$ and the some combinations of initial state and model parameters might just happen to bounce to the other branch when $\beta$ changes in the $\beta &gt; \beta_c$ regime.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/artificial-intelligence/">Artificial Intelligence</a>
  
  <a class="badge badge-light" href="/tag/associative-memories/">Associative Memories</a>
  
  <a class="badge badge-light" href="/tag/attention/">Attention</a>
  
  <a class="badge badge-light" href="/tag/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tag/emergent-collective-computational-capabilities/">Emergent Collective Computational Capabilities</a>
  
  <a class="badge badge-light" href="/tag/ising-models/">Ising Models</a>
  
  <a class="badge badge-light" href="/tag/mean-field-theory/">Mean-Field Theory</a>
  
  <a class="badge badge-light" href="/tag/neural-networks/">Neural Networks</a>
  
  <a class="badge badge-light" href="/tag/non-equilibrium-dynamics/">Non-Equilibrium Dynamics</a>
  
  <a class="badge badge-light" href="/tag/partition-function/">Partition Function</a>
  
  <a class="badge badge-light" href="/tag/spin-glasses/">Spin Glasses</a>
  
  <a class="badge badge-light" href="/tag/spin-models/">Spin Models</a>
  
  <a class="badge badge-light" href="/tag/statistical-physics/">Statistical Physics</a>
  
  <a class="badge badge-light" href="/tag/transformers/">Transformers</a>
  
  <a class="badge badge-light" href="/tag/vector-spin-models/">Vector-Spin Models</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://mcbal.github.io/post/spin-model-transformers/&amp;text=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://mcbal.github.io/post/spin-model-transformers/&amp;t=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules&amp;body=https://mcbal.github.io/post/spin-model-transformers/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://mcbal.github.io/post/spin-model-transformers/&amp;title=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules%20https://mcbal.github.io/post/spin-model-transformers/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://mcbal.github.io/post/spin-model-transformers/&amp;title=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://mcbal.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/matthias-bal/avatar_huf6181012a34ccf45ad256514680767eb_86031_270x270_fill_q90_lanczos_center.jpg" alt="Matthias Bal"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://mcbal.github.io/">Matthias Bal</a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:matthiascbal%20at%20gmail%20dot%20com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatthiasBal" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.be/citations?user=vjYY0bMAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/mcbal" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>










<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/transformers-are-secretly-collectives-of-spin-systems/" rel="prev">Transformers Are Secretly Collectives of Spin Systems</a>
  </div>
  
</div>

</div>





  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/transformers-are-secretly-collectives-of-spin-systems/">Transformers Are Secretly Collectives of Spin Systems</a></li>
      
      <li><a href="/post/transformers-from-spin-models-approximate-free-energy-minimization/">Transformers from Spin Models: Approximate Free Energy Minimization</a></li>
      
      <li><a href="/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/">Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</a></li>
      
      <li><a href="/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/">Transformer Attention as an Implicit Mixture of Effective Energy-Based Models</a></li>
      
      <li><a href="/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/">An Energy-Based Perspective on Attention Mechanisms in Transformers</a></li>
      
    </ul>
  </div>
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/bash.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/wowchemy.min.434af0ebce9e15b273b954d65feb39c7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Matthias Bal © 2023
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
