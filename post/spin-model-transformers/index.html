<!DOCTYPE html><html lang="en-gb" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 4.8.0 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Matthias Bal">

  
  
  
    
  
  <meta name="description" content="A non-equilibrium statistical mechanics perspective on transformers">

  
  <link rel="alternate" hreflang="en-gb" href="https://mcbal.github.io/post/spin-model-transformers/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IBM+Plex+Serif:ital,wght@0,300;0,400;1,300;1,400%7CIBM+Plex+Sans:ital,wght@0,300;0,400;0,700;1,400%7CFira+Code&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=G-H9Y98B2S2P"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-H9Y98B2S2P', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://mcbal.github.io/post/spin-model-transformers/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="mcbal">
  <meta property="og:url" content="https://mcbal.github.io/post/spin-model-transformers/">
  <meta property="og:title" content="Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules | mcbal">
  <meta property="og:description" content="A non-equilibrium statistical mechanics perspective on transformers"><meta property="og:image" content="https://mcbal.github.io/post/spin-model-transformers/featured.jpg">
  <meta property="twitter:image" content="https://mcbal.github.io/post/spin-model-transformers/featured.jpg"><meta property="og:locale" content="en-gb">
  
    
      <meta property="article:published_time" content="2022-06-19T09:28:17&#43;01:00">
    
    <meta property="article:modified_time" content="2023-05-31T20:28:18&#43;01:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mcbal.github.io/post/spin-model-transformers/"
  },
  "headline": "Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules",
  
  "image": [
    "https://mcbal.github.io/post/spin-model-transformers/featured.jpg"
  ],
  
  "datePublished": "2022-06-19T09:28:17+01:00",
  "dateModified": "2023-05-31T20:28:18+01:00",
  
  "author": {
    "@type": "Person",
    "name": "Matthias Bal"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "mcbal",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mcbal.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "A non-equilibrium statistical mechanics perspective on transformers"
}
</script>

  

  


  


  





  <title>Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules | mcbal</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class=" ">

  
  
  
    <script>window.wcDarkLightEnabled = true;</script>
  
  
    <script>const isSiteThemeDark = false;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">mcbal</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">mcbal</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>About</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>



  <article class="article">

  




















  
  


<div class="article-container pt-3">
  <h1>Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules</h1>

  
  <p class="page-subtitle">A non-equilibrium statistical mechanics perspective on transformers</p>
  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    May 31, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    23 min read
  </span>
  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 600px; max-height: 400px;">
  <div style="position: relative">
    <img src="/post/spin-model-transformers/featured.jpg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <hr>
<ol>
<li><a href="#1-introduction">Introduction</a></li>
<li><a href="#2-mean-field-theory-of-asymmetric-ising-models-with-binary-spins">Mean-field theory of asymmetric Ising models with binary spins</a>
<ol>
<li><a href="#21-setting-the-scene-the-kinetic-ising-model">Setting the scene: the kinetic Ising model</a></li>
<li><a href="#22-mean-field-theory-and-kullback-leibler-divergence">Mean-field theory and Kullback-Leibler divergence</a></li>
<li><a href="#23-the-plefka-expansion-interpolating-distributions">The Plefka expansion: interpolating distributions</a></li>
<li><a href="#24-naive-mean-field-and-thouless-anderson-palmer-approximations">Naive mean-field and Thouless-Anderson-Palmer approximations</a></li>
<li><a href="#25-a-simple-jax-implementation">A simple JAX implementation</a></li>
</ol>
</li>
<li><a href="#3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins">Mean-field theory of asymmetric Ising models with vector spins</a>
<ol>
<li><a href="#31-vector-spins-distributions-on-hyperspheres">Vector spins: distributions on hyperspheres</a></li>
<li><a href="#32-naive-mean-field-and-thouless-anderson-palmer-approximations">Naive mean-field and Thouless-Anderson-Palmer approximations</a></li>
<li><a href="#33-a-simple-jax-implementation">A simple JAX implementation</a></li>
</ol>
</li>
<li><a href="#4-a-family-of-transformer-like-modules">A family of transformer-like modules</a>
<ol>
<li><a href="#41-fast--and-slow-moving-parameters">Fast- and slow-moving parameters</a></li>
</ol>
</li>
<li><a href="#5-conclusion">Conclusion</a></li>
<li><a href="#appendices">Appendices</a></li>
</ol>
<hr>
<h1 id="1-introduction">1. Introduction</h1>
<blockquote>
<p><strong>✨ This is a work in progress exploring connections between transformer neural networks and mean-field dynamics of asymmetric vector-spin models. JAX implementations of the content outlined in this post should eventually become available at <a href="https://github.com/mcbal/spin-model-transformers" target="_blank" rel="noopener"><code>mcbal/spin-model-transformers</code></a>.</strong></p>
</blockquote>
<p>In a series of previous <a href="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/" target="_blank" rel="noopener">blog posts</a>, we have tried to connect the forward pass of a transformer neural-network module to computing mean magnetizations in disordered Ising-like vector-spin models with parameterized couplings and external magnetic fields. In this framework, the forward pass of a transformer module computes statistical observables given a specific realization of quenched couplings and external magnetic fields while the backward pass nudges the parameterized couplings and external magnetic fields. Physically, the transformer module represents an interacting many-body system modulating its behavior by learning to respond to being driven in all kinds of funny ways.</p>
<p>However, both the mean-field message-passing approach of <a href="https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/" target="_blank" rel="noopener">Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</a> and the saddle-point free-energy approach of <a href="https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/" target="_blank" rel="noopener">Transformers from Spin Models: Approximate Free Energy Minimization</a> inherently rely on methods that are only well-defined for spin models with symmetric coupling matrices, whose stochastic dynamics obey detailed balance and converge to a steady-state equilibrium characterized by the Boltzmann distribution. Since the softmax attention operation in transformer modules is annoyingly and famously asymmetric, we need to come up with something better to establish a correspondence.</p>
<p>To capture spin models with asymmetric coupling matrices, we consider non-equilibrium kinetic Ising systems, whose dynamics can be pretty wild yet sometimes relax to a non-equilibrium steady state. In the last few decades, dynamical mean-field approaches have been developed for the binary kinetic Ising toy model, which exhibits non-equilibrium behavior when couplings are asymmetric or when model parameters are subject to rapid changes. In this post, we generalize one of these approaches from binary spins to vector spins with the aim of comparing the mean-field update equations for the magnetizations to the forward pass of a transformer module.</p>
<h1 id="2-mean-field-theory-of-asymmetric-ising-models-with-binary-spins">2. Mean-field theory of asymmetric Ising models with binary spins</h1>
<p>In this section, we review known results on mean-field theory approaches to capturing the stochastic dynamics of binary kinetic Ising models. We primarily follow the discussion outlined in <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener"><em>A unifying framework for mean-field theories of asymmetric kinetic Ising systems</em> (Aguilera et al., 2021)</a>. We implement the mean-field update equations for the mean magnetizations in JAX and run a few numerical experiments.</p>
<h2 id="21-setting-the-scene-the-kinetic-ising-model">2.1. Setting the scene: the kinetic Ising model</h2>
<img src="binary_spins.png" alt="Random Ising model configuration with binary spins" width="250px"/>
<p>We consider a kinetic Ising model describing a system made up of $N$ interacting binary spins $s_{i,t} \in \{-1, 1\}$ that evolve in discrete time steps $t$ according to synchronous dynamics, i.e. all spins get updated at the same time in parallel. Given a spin configuration $\mathbf{s}_{t-1} = \{ s_{1,t-1}, s_{2,t-1}, \ldots, s_{N,t-1} \}$ at time $t-1$, we consider the spins $\mathbf{s}_{t}$ at time $t$ to be conditionally independent random variables captured by a discrete-time Markov chain transition probability</p>
<p>\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{\sum_{s_{i,t}} \mathrm{e}^{s_{i,t} h_{i,t}}} = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} h_{i,t}}}{2 \cosh h_{i,t}}, \label{eq:pcond}
\end{equation}</p>
<p>where the effective external field is given by</p>
<p>\begin{equation}
h_{i,t} = x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1}.
\end{equation}</p>
<p>Here, the parameters $\mathbf{x}$ represent the (possibly time-dependent) local external fields at each site while the coupling parameters $\mathbf{J}$ are a specific realization of quenched disorder encoding the interactions between pairs of spins. Using the probability mass function of the previous state $P( \mathbf{s}_{t-1} )$ we can write the distribution of the current state as</p>
<p>\begin{equation}
P( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1}} P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P( \mathbf{s}_{t-1} ), \label{eq:marginal}
\end{equation}</p>
<p>which, when applied recursively, traces the evolution of the system starting from some initial distribution $P( \mathbf{s}_{0} )$. Unless we turn off the couplings by setting $\mathbf{J} = \mathbf{0}$, the marginal distribution $P( \mathbf{s}_{t} )$ is not factorized and tends to be quite complicated. Our goal is to compute statistical properties of the system, such as the mean magnetizations</p>
<p>\begin{equation}
m_{i,t} = \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t} ),
\end{equation}</p>
<p>as well as correlations</p>
<p>\begin{equation}
C_{ik,t} = \sum_{\mathbf{s}_{t}} s_{i,t} s_{k,t} P( \mathbf{s}_{t} ) - m_{i,t} m_{k,t},
\end{equation}</p>
<p>and delayed correlations</p>
<p>\begin{equation}
D_{il,t} = \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1}} s_{i,t} s_{l,t-1} P( \mathbf{s}_{t}, \mathbf{s}_{t-1} ) - m_{i,t} m_{l,t-1}.
\end{equation}</p>
<p>Since the above expressions involve summing over a large amount of possible spin configurations, they are not very useful in practice. So we will try to approximate the tricky marginal distribution $P( \mathbf{s}_{t} )$ defined in Eq. \eqref{eq:marginal} using a mean-field theory approach.</p>
<h2 id="22-mean-field-theory-and-kullback-leibler-divergence">2.2. Mean-field theory and Kullback-Leibler divergence</h2>
<p>Mean-field theory tries to approximate a complicated object ${\color{red}P}$ by wiggling around the parameters of a simple, analytically tractable parameterized ansatz ${\color{green}Q_{\theta}}$ to get as close as possible to ${\color{red}P}$. At risk of inducing headaches in mathematicians by calling everything a manifold, we can picture what is going on geometrically as trying to approximate a target probability distribution $P( \mathbf{s}_{t} \vert \mathbf{x}, \mathbf{J})$ and its statistical properties $\mathbf{m}_{t}$, $\mathbf{C}_{t}$, and $\mathbf{D}_{t}$ by restricting ourselves to a submanifold of tractable probability distributions. A particularly convenient submanifold is that of factorized models, where each point on the submanifold corresponds to a distribution parameterized by a vector $\boldsymbol{\theta}_{t}$,</p>
<p>\begin{equation}
Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{s_{i,t} \theta_{i,t}}}{2 \cosh \theta_{i,t}}, \label{eq:q}
\end{equation}</p>
<p>so that the mean magnetizations are simply given by</p>
<p>\begin{equation}
m_{i,t} = \tanh \theta_{i,t}  \label{eq:meanmagstanh}
\end{equation}</p>
<p>as there are no couplings between spins. The factorized model $Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}^{*}_{t} )$ that minimizes the Kullback-Leibler (KL) divergence</p>
<p>\begin{equation}
D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) = \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log \frac{P( \mathbf{s}_{t})}{Q_{\theta}( \mathbf{s}_{t})} \label{eq:kl}
\end{equation}</p>
<p>has mean magnetizations $\mathbf{m}_{t}$ identical to those of the target distribution $P( \mathbf{s}_{t})$ since, for all spins $i=1,2,\ldots,N$, we find that</p>
<p>\begin{align}
\frac{\partial D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}} &amp;= - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \frac{\partial \log Q_{\theta}( \mathbf{s}_{t}) }{\partial \theta_{i, t}} \Biggr\rvert_{\boldsymbol{\theta}_{t}=\boldsymbol{\theta}^{*}_{t}}  \\
&amp;= - \sum_{\mathbf{s}_{t}} s_{i,t} P( \mathbf{s}_{t}) + \tanh \theta^{*}_{i,t} \\
&amp;= -m^{{\color{red}P}}_{i,t} + m^{{\color{green}Q_{\theta^{*}}}}_{i,t} = 0, \label{eq:klm}
\end{align}</p>
<p>where $m^{{\color{red}P}}_{i,t}$ and $m^{{\color{green}Q_{\theta^{*}}}}_{i,t}$ respectively denote the expectation values of $s_{i,t}$ with respect to ${\color{red}P}$ and ${\color{green}Q_{\theta^{*}}}$. Indeed, minimizing $D_{\mathrm{KL}} ({\color{red}P}\vert\vert{\color{green}Q_{\theta}})$ tries to cover the modes of ${\color{red}P}$ by moment matching since the expectation value in Eq. \eqref{eq:kl} is calculated with respect to ${\color{red}P}$.</p>
<h2 id="23-the-plefka-expansion-interpolating-distributions">2.3. The Plefka expansion: interpolating distributions</h2>
<p>Great, but is it even possible to find the parameters</p>
<p>\begin{equation}
\DeclareMathOperator*{\argmin}{arg\,min}
\boldsymbol{\theta}^{*}_{t} = \argmin_{\boldsymbol{\theta}_{t}} \left( - \sum_{\mathbf{s}_{t}} P( \mathbf{s}_{t}) \log Q_{\theta}( \mathbf{s}_{t}) \right)
\end{equation}</p>
<p>that minimize the KL divergence? Well, that&rsquo;s going to be hard, unless you already know the target distribution $P( \mathbf{s}_{t})$, or you have a clever way of approximately evaluating the expectation value of $\log {\color{green}Q_{\theta}}$ with respect to ${\color{red}P}$. So let&rsquo;s introduce some more distributions to get around this issue. To apply the Plefka expansion to our problem, we introduce the conditional distribution</p>
<p>\begin{equation}
P_{\alpha}( \mathbf{s}_{t}\vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N}  \frac{\mathrm{e}^{s_{i,t} h_{i,t}(\alpha) }}{2 \cosh h_{i,t}(\alpha)}, \label{eq:pcondalt}
\end{equation}</p>
<p>\begin{equation}
h_{i,t}(\alpha) = (1-\alpha) \theta_{i,t} + \alpha \left( x_{i,t} + \sum_{j=1}^{N} J_{ij} s_{j,t-1} \right), \label{eq:pcondalth}
\end{equation}</p>
<p>parameterized by a scalar $\alpha$ interpolating between $P_{\alpha=0}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} )$ (Eq. \eqref{eq:q}) and $P_{\alpha=1}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} )$ (Eq. \eqref{eq:pcond}). Using Eq. \eqref{eq:pcondalt}, we can construct an approximate marginal distribution $P_{\alpha}( \mathbf{s}_{t})$, leading to $\alpha$-dependent statistical properties $\mathbf{m}_{t}(\alpha)$, $\mathbf{C}_{t}(\alpha)$, and $\mathbf{D}_{t}(\alpha)$ for the approximate system. The Plefka expansion then boils down to writing these properties as Taylor series expansions around the factorized model $\alpha=0$. For the mean magnetizations, the expansion up to $n$-th order looks like</p>
<p>\begin{equation}
\mathbf{m}_{t}(\alpha) = \mathbf{m}_{t}(\alpha=0) + \sum_{k=1}^{n} \frac{\alpha^k}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}), \label{eq:mtaylor}
\end{equation}</p>
<p>where all coefficients in the expansion are functions of $\boldsymbol{\theta}_{t}$ via Eq. \eqref{eq:pcondalth}. The mean-field approximation is computed by setting $\alpha=1$ so that the original marginal distribution is recovered and Eq. \eqref{eq:klm} holds, which implies that $\mathbf{m}_{t}(\alpha=1) = \mathbf{m}_{t}(\alpha=0)$ and thus</p>
<p>\begin{equation}
\sum_{k=1}^{n} \frac{1}{k!} \frac{\partial^{k} \mathbf{m}_{t}(\alpha=0)}{\partial \alpha^{k}} + \mathcal{O}(\alpha^{n+1}) = 0. \label{eq:mftheta}
\end{equation}</p>
<p>Finally, we solve Eq. \eqref{eq:mftheta} for $\boldsymbol{\theta}_{t}$ to find the mean-field values $\boldsymbol{\theta}^{*}_{t}$ of the parameters of the distribution Eq. \eqref{eq:q}. Physically, we are tuning the effective external magnetic fields of the factorized ansatz to $\boldsymbol{\theta}^{*}_{t}$ so that its approximate mean magnetizations get as close as possible to the real ones.</p>
<h2 id="24-naive-mean-field-and-thouless-anderson-palmer-approximations">2.4. Naive mean-field and Thouless-Anderson-Palmer approximations</h2>
<p>We now consider first and second order approximations of the mean magnetizations Eq. \eqref{eq:mtaylor} to recover respectively the naive mean-field and Thouless-Anderson-Palmer (TAP) approximations for the binary kinetic Ising model. The starting point is a Plefka expansion around factorized models at times $t-1$ and $t$. From Eq. \eqref{eq:marginal} and Eq. \eqref{eq:pcondalt}, we construct a marginal probability distribution</p>
<p>\begin{equation}
P^{[t-1:t]}_{\alpha}( \mathbf{s}_{t} ) = \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ),
\end{equation}</p>
<p>interpolating between $P^{[t-1:t]}_{\alpha=0}( \mathbf{s}_{t} ) = Q( \mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\alpha=1}( \mathbf{s}_{t} ) = P( \mathbf{s}_{t} )$. The corresponding mean magnetizations are</p>
<p>\begin{align}
m_{i,t}(\alpha) &amp;= \sum_{\mathbf{s}_{t},\mathbf{s}_{t-1},\mathbf{s}_{t-2}} s_{i,t} \, P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ) \\
&amp;= \sum_{\mathbf{s}_{t-1},\mathbf{s}_{t-2}} \tanh h_{i,t}(\alpha) \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} )
\end{align}</p>
<p>Following Eq. \eqref{eq:mftheta}, the first-order approximation should satisfy</p>
<p>\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} = \left( 1-m^{2}_{i,t} \right) \left( -\theta_{i,t} + x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right) = 0,
\end{equation}</p>
<p>so that $\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1}$ and we end up with the naive mean-field equations:</p>
<p>\begin{equation}
\boxed{m_{i,t} \approx \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} \right)} \label{eq:naivem}
\end{equation}</p>
<p>Again following Eq. \eqref{eq:mftheta}, the second-order approximation should satisfy</p>
<p>\begin{equation}
\frac{\partial m_{i,t}(\alpha=0)}{\partial\alpha} + \frac{1}{2} \frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial^{2}\alpha} = 0,
\end{equation}</p>
<p>where the second-order derivative, neglecting terms higher than $\mathcal{O}(\alpha^2)$, is</p>
<p>\begin{equation}
\frac{\partial^{2} m_{i,t}(\alpha=0)}{\partial^{2}\alpha} \approx -2 m_{i,t} \left( 1-m^{2}_{i,t} \right) \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}</p>
<p>so that</p>
<p>\begin{equation}
\theta^{*}_{i,t} = x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right)
\end{equation}</p>
<p>and we end up with the TAP mean-field equations:</p>
<p>\begin{equation}
\boxed{m_{i,t} \approx \tanh \left( x_{i,t} + \sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \sum_{j} J^{2}_{ij} \left( 1-m^{2}_{j,t-1} \right) \right)} \label{eq:tapm}
\end{equation}</p>
<p>The mean-field equations obtained above can also be elegantly derived using a Legendre transformation of the generating functional of the set of trajectories of the model, as outlined in e.g. <a href="https://arxiv.org/abs/1103.1044" target="_blank" rel="noopener"><em>Dynamical TAP equations for non-equilibrium Ising spin glasses (2011)</em></a>. We can also derive second-order TAP approximations of the correlations</p>
<p>\begin{equation}
C_{ik,t} \approx \begin{cases}
1 - m^{2}_{i,t}  &amp; i = k \\
\left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{k,t} \right) \sum_{j} J_{ij} J_{kj} \left( 1-m^{2}_{j,t-1} \right) &amp; i \neq k \label{eq:tapc}
\end{cases}
\end{equation}</p>
<p>and delayed correlations</p>
<p>\begin{equation}
D_{il,t} \approx J_{il} \left( 1-m^{2}_{i,t} \right) \left( 1-m^{2}_{l,t-1} \right) \left( 1 + 2 J_{il} m_{i,t} m_{l,t-1} \right). \label{eq:tapd}
\end{equation}</p>
<p>We refer to <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">(Aguilera et al., 2020)</a> for full derivations of the above mean-field results as well as variations based on different approximations of the marginal distribution $P( \mathbf{s}_{t} )$.</p>
<p>In summary, given the mean magnetizations $\mathbf{m}_{t-1}$ of the system at time $t-1$, we can use equations \eqref{eq:tapm} \eqref{eq:tapc} \eqref{eq:tapd} to compute a tuple $(\mathbf{m}_{t},\mathbf{C}_{t},\mathbf{D}_{t})$ of approximate statistical properties  of the system at time $t$. The time evolution of the system can thus be captured at the mean-field level by recursively computing $\mathbf{m}_{t}$ starting from an initial state $\mathbf{m}_{0}$, although approximation errors accumulate over the iterations.</p>
<h2 id="25-a-simple-jax-implementation">2.5. A simple JAX implementation</h2>
<p>To get more insight into what is going on, let us turn the mean-field update equations \eqref{eq:naivem} and \eqref{eq:tapm} for the mean magnetizations into code. But before we show a few plots, we need to know a bit more background about the model we are simulating. In <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">(Aguilera et al., 2020)</a>, the authors derive a solution of the asymmetric version of the kinetic <a href="https://en.wikipedia.org/wiki/Spin_glass#Sherrington%E2%80%93Kirkpatrick_model" target="_blank" rel="noopener">Sherrington-Kirkpatrick mean-field spin-glass model</a> using a generating functional or dynamical partition function approach to capture the distribution of trajectories. They consider the same kinetic Ising model as in Eq. \eqref{eq:pcond} but with an inverse temperature parameter $\beta$ in the exponentials:</p>
<p>\begin{equation}
P( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta s_{i,t} h_{i,t}}}{2 \cosh \beta h_{i,t}}. \label{eq:pcondwithbeta}
\end{equation}</p>
<p>For Gaussian couplings $J_{ij} \sim \mathcal{N}\left( J_{\mu} / N, J^{2}_{\sigma} / N\right)$ and uniformly distributed external magnetic fields $x_{i} \sim \mathcal{U}(-X_{0}, X_{0})$, they show the existence of a ferromagnetic phase transition. In particular for $X_{0}=0.5$, $J_{\mu}=1.0$, and $J_{\sigma}=0.1$, the phase transition happens when tuning $\beta$ to a critical value $\beta_{c} \approx 1.1108$.</p>
<h3 id="simulating-magnetization-trajectories">Simulating magnetization trajectories</h3>
<p>We now turn to a JAX implementation of the mean-field time evolution of the magnetizations according to the model described above. We use <code>lax.scan</code> to implement the update steps and <code>vmap</code> to parallelize trajectories starting from a batch of initial magnetization configurations $\mathbf{m}_{0}$. For the second-order TAP equations, Anderson acceleration is used to find the fixed point magnetization $\mathbf{m}_{t}$ given $\mathbf{m}_{t-1}$.</p>
<pre><code class="language-python">from functools import partial

import jax
import jax.numpy as jnp

from jaxopt import AndersonAcceleration


def update_naive_mf(m0, _, x, J):
    m1 = jnp.tanh(x + jnp.einsum(&quot;... i j, ... j -&gt; ... i&quot;, J, m0))
    return m1, m0


def update_tap_mf(m0, _, x, J):
    def tap(m):
        return jnp.tanh(
            x
            + jnp.einsum(&quot;... i j, ... j -&gt; ... i&quot;, J, m0)
            - m * jnp.einsum(&quot;... i j, ... j -&gt; ... i&quot;, J**2, (1.0 - m0**2))
        )

    m1 = AndersonAcceleration(fixed_point_fun=tap, tol=1e-3, maxiter=10).run(m0).params
    return m1, m0


def time_evolution(m0, steps, update_fun):
    final_carry, stacked_outputs = jax.lax.scan(update_fun, init=m0, xs=steps)
    return final_carry, stacked_outputs


def init_params(key, N, beta, X0, J_mu, J_sigma):
    x_key, J_key = jax.random.split(key)
    x = jax.random.uniform(x_key, shape=(N,), minval=-beta * X0, maxval=beta * X0)
    J = beta * J_mu * N**-1 + beta * J_sigma * N**-0.5 * jax.random.normal(
        J_key, shape=(N, N)
    )
    return x, J


def simulate(
    key, m0, steps, beta, X0=0.5, J_mu=1.0, J_sigma=0.1, update_fun=update_tap_mf
):
    x, J = init_params(key, m0.shape[-1], beta, X0, J_mu, J_sigma)
    wrapped_time_evolution = partial(
        time_evolution,
        steps=steps,
        update_fun=partial(update_fun, x=x, J=J),
    )
    final_carry, stacked_outputs = jax.vmap(wrapped_time_evolution)(m0)
    return final_carry, stacked_outputs
</code></pre>
<h3 id="naive-mean-field-vs-thouless-anderson-palmer-tap">Naive mean-field vs. Thouless-Anderson-Palmer (TAP)</h3>
<p>We fix the seed for the randomly initialized model parameters $\mathbf{x}$ and $\mathbf{J}$ and simulate $N=512$ spins at the critical temperature $\beta_{c}$ for $t=128$ time steps starting from an all-ones intial state. We first consider the naive mean-field update step.</p>
<p><img src="binary_plot_1.png" alt=""></p>
<p>The left axis shows the individual magnetization trajectories for each spin plotted horizontally while the red line associated to the right axis describes the average of the magnetizations across all spins for each time step. We observe convergence to what looks like a steady state.</p>
<p><img src="binary_plot_2.png" alt=""></p>
<p>Comparing the naive first-order mean-field update equations to the second-order Thouless-Anderson-Palmer (TAP) ones, we observe lower values for the mean magnetization across all spins, which <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">(Aguilera et al., 2020)</a> showed to be closer to ground truth values (not shown) obtained via sampling and averaging spin configurations.</p>
<h3 id="sampling-trajectories">Sampling trajectories</h3>
<p>Let&rsquo;s now consider 100 randomly-initialized initial states and simulate their associated trajectories in three different model regimes: far below the critical point ($\beta=\beta_c / 2 $), at the critical point ($\beta=\beta_c$), and far above the critical point ($\beta=2 \beta_c$).</p>
<p><img src="binary_plot_3.png" alt=""></p>
<p>We observe that the trajectories of randomly-initialized initial states converge to identical final states in each regime. These final states map to a simple ferromagnetic Ising phase diagram, where a high-temperature disordered phase $\langle m_{i,t} \rangle \to 0$ (left) is separated from a low-temperature locally-ordered phase $\langle m_{i,t} \rangle \to \pm 1$ (right) by a critical point (center). The behavior around $\beta=\beta_{c}$ is pretty interesting: <em>the non-trivial non-equilibrium steady state looks like an attractor implicitly encoded in the dynamics of the model</em>.</p>
<h3 id="sampling-model-parameters">Sampling model parameters</h3>
<p>Let&rsquo;s go back to considering just a single trajectory since we just saw that trajectories seem to converge to the same final steady-state magnetizations for fixed model parameters. To get a feel for the variation of these values across different realizations of model parameters, we plot the absolute value<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> $| \langle m_{i} \rangle |$ of the final steady-state magnetizations across 100 samples of model parameters and a range of inverse temperatures. We&rsquo;re using JAX, so we can easily sample model parameters by <code>vmap</code>&lsquo;ing the random key fed into the <code>simulate</code> function followed by another <code>vmap</code> to sweep across $\beta$.</p>
<p><img src="binary_plot_4.png" alt=""></p>
<p>Every curve in the above plot describes the final steady-state value of the &ldquo;order parameter&rdquo; $| \langle m_{i} \rangle |$ for a fixed set of model parameters sweeping across $\beta$. We observe a greater spread of values near the critical point. If we were to let the number of spins $N \to \infty$ and average over a large number of model parameter samples, the finite-size results above would probably transform into a sharp curve with zero magnetization below the critical point and a sudden non-zero magnetization emerging at the critical point.</p>
<h1 id="3-mean-field-theory-of-asymmetric-ising-models-with-vector-spins">3. Mean-field theory of asymmetric Ising models with vector spins</h1>
<p>We now transpose the binary-spin results of the previous section to a setting where the local spin degrees of freedom are $D$-dimensional vector spins restricted to wiggle around on $(D-1)$-dimensional spheres. We start by generalizing the conditional distribution Eq. \eqref{eq:pcondalt} to vector spins. Next, we derive mean-field equations for the mean magnetizations. We finish this section with a JAX implementation of the update equations and compare numerical results to the binary-spin case.</p>
<img src="vector_spins.png" alt="Random Ising model configuration with vector spins" width="250px"/>
<h2 id="31-vector-spins-distributions-on-hyperspheres">3.1. Vector spins: distributions on hyperspheres</h2>
<p>A vector-spin equivalent of Eq. \eqref{eq:pcondalt} looks like</p>
<p>\begin{equation}
P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \mathbf{h}_{i,t}(\alpha)}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s}_{i,t} \; \mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \mathbf{h}_{i,t}(\alpha)} }, \label{eq:pcondaltvector}
\end{equation}</p>
<p>where we immediately included an inverse temperature scaling $\beta$ like in Eq. \eqref{eq:pcondwithbeta}. A vector-spin equivalent of Eq. \eqref{eq:pcondalth} is</p>
<p>\begin{equation}
\mathbf{h}_{i,t}(\alpha) = (1-\alpha) \boldsymbol{\theta}_{i,t} + \alpha \left( \mathbf{x}_{i,t} + \sum_{j=1}^{N} J_{ij} \mathbf{s}_{j,t-1} \right) \equiv \boldsymbol{\theta}_{i,t} + \alpha \Delta \mathbf{h}_{i,t},  \label{eq:pcondalthvector}
\end{equation}</p>
<p>where $S_{D-1}(R) = \{ x \in \mathbb{R}^{D} : \lVert x \rVert = R \}$ denotes the $(D-1)$-dimensional sphere with radius $R$ embedded in $D$ dimensions. Let us focus on the distribution for a single site and drop all subscripts and dependencies for clarity:</p>
<p>\begin{equation}
\frac{\mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}}{\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} }. \label{eq:pcondsinglesitevector}
\end{equation}</p>
<p>The normalization constant in the denominator can be shown to be (see <a href="#a1-normalization-constant">Appendix A.1</a>)</p>
<p>\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } \equiv Z(\beta, R, \lVert \mathbf{h}\rVert) \label{eq:partfun}
\end{equation}</p>
<p>where $I_{\nu}(z)$ denotes the modified Bessel function of the first kind and $\lVert \mathbf{h}\rVert = \sqrt{\mathbf{h} \cdot \mathbf{h}}$. Physically, we can think of this single-site distribution as measuring dot-product alignment to an effective external magnetic field $\mathbf{h}$ at inverse temperature $\beta$. If we consider spins living on the unit sphere $R=1$ as well as unit vectors $\mathbf{h}$, the distribution looks like a <a href="https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution" target="_blank" rel="noopener">von Mises–Fisher distribution</a> with mean direction $\boldsymbol{\mu} \equiv \mathbf{h}$ and <a href="https://en.wikipedia.org/wiki/Concentration_parameter" target="_blank" rel="noopener">concentration parameter</a> $\kappa \equiv \beta$. This distribution is unimodal for $\kappa &gt; 0$ and can be derived from restricting an isotropic multivariate Gaussian to the unit hypersphere. The greater the value of $\kappa$ (the inverse temperature $\beta$), the higher the concentration of the distribution around the mean direction $\boldsymbol{\mu}$ (the more the spin tends to align to the effective external field $\mathbf{h}$). Instead of a fixed parameter $\boldsymbol{\mu}$, we have a very funky parameter Eq. \eqref{eq:pcondalthvector} that depends on all other spins to spice things up.</p>
<p>Before we derive the mean-field approximations for the mean magnetizations, we consider the decoupled $\alpha \to 0$ distribution of Eq. \eqref{eq:pcondaltvector},</p>
<p>\begin{equation}
Q( \mathbf{s}_{t} \vert \boldsymbol{\theta}_{t} ) = \prod_{i=1}^{N} \frac{\mathrm{e}^{\beta \, \mathbf{s}_{i,t} \cdot \boldsymbol{\theta}_{i,t}}}{Z_{i,t}\left(\beta, R, \lVert \boldsymbol{\theta}_{i,t} \rVert\right)},
\end{equation}</p>
<p>and find an expression for its mean magnetizations. For each decoupled site, the mean magnetization can be shown to be (see <a href="#a2-expected-value">Appendix A.2</a>)</p>
<p>\begin{equation}
\mathbf{m}_{i,t} = \frac{I_{D/2}(\beta R \lVert \boldsymbol{\theta}_{i,t} \rVert)}{I_{D/2 - 1}(\beta R \lVert \boldsymbol{\theta}_{i,t} \rVert)} \frac{R \boldsymbol{\theta}_{i,t}}{\lVert \boldsymbol{\theta}_{i,t} \rVert} \equiv \boldsymbol{\varphi} \left(\boldsymbol{\theta}_{i,t}\right), \label{eq:meanmagsbessels}
\end{equation}</p>
<p>which plays the role of $m_{i,t} = \tanh \theta_{i,t}$ in the binary case, see Eq. \eqref{eq:meanmagstanh}. Looking ahead at turning these equations into code, we note that there exist <a href="https://www.jstor.org/stable/2005830" target="_blank" rel="noopener">efficient algorithms</a> to compute the ratio of modified Bessel functions of the first kind. We implement a fast JAX version in <a href="#a3-ratio-of-modified-bessel-functions">Appendix A.3</a> for later use. Depending on the normalization conventions we settle on, we could also make use of <a href="https://link.springer.com/article/10.1007/BF02764812" target="_blank" rel="noopener">known asymptotic expansions</a>.</p>
<h2 id="32-naive-mean-field-and-thouless-anderson-palmer-approximations">3.2. Naive mean-field and Thouless-Anderson-Palmer approximations</h2>
<p>Closely mimicking the binary case, we start from the following approximated marginal probability distribution</p>
<p>\begin{equation}
P^{[t-1:t]}_{\alpha}( \mathbf{s}_{t} ) = \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ),
\end{equation}</p>
<p>interpolating between $P^{[t-1:t]}_{\alpha=0}( \mathbf{s}_{t} ) = Q( \mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\alpha=1}( \mathbf{s}_{t} ) = P( \mathbf{s}_{t} )$. Our lazy integral notation $\int \mathrm{d} \mathbf{s}_{t}$ should be understood as $\int \prod_{i=1}^{N} \mathrm{d}^{D} \mathbf{s}_{i, t}$, i.e. integrating over all the little spins at a fixed time $t$. The estimated mean magnetizations are</p>
<p>\begin{align}
\mathbf{m}_{i,t}(\alpha) &amp;= \int \mathrm{d} \mathbf{s}_{t} \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; \mathbf{s}_{i,t} P_{\alpha}( \mathbf{s}_{t} \vert \mathbf{s}_{t-1} ) P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} ) \nonumber\\
&amp;= \int \mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \; \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) P( \mathbf{s}_{t-2} )
\end{align}</p>
<p>The first-order derivative with respect to $\alpha$ is given by</p>
<p>\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha)}{\partial\alpha} = \int &amp;\mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \Biggl( \frac{\partial\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial\alpha} \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) \nonumber\\
&amp;+ \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, \frac{\partial P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial\alpha} \Biggr) P( \mathbf{s}_{t-2} ),
\end{align}</p>
<p>which is given by (see <a href="#a5-naive-mean-field-approximation">Appendix A.5</a>)</p>
<p>\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha)}{\partial\alpha} &amp;= \ldots .
\end{align}</p>
<p>We evaluate at $\alpha=0$ to find</p>
<p>\begin{align}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} &amp;= \ldots,
\end{align}</p>
<p>Following Eq. \eqref{eq:mftheta}, the first-order approximation should satisfy</p>
<p>\begin{equation}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} = \ldots = 0,
\end{equation}</p>
<p>so that $\boldsymbol{\theta}^{*}_{i,t} = \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1}$ and we end up with the naive mean-field equations:</p>
<p>\begin{equation}
\boxed{ \mathbf{m}_{i,t} \approx \frac{I_{D/2}(\beta R \lVert \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \rVert)} \frac{R \left(\mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1}\right)}{\lVert \mathbf{x}_{i,t} + \sum_{j} J_{ij} \mathbf{m}_{j,t-1} \rVert} } \label{eq:naivemvector}
\end{equation}</p>
<p>Again following Eq. \eqref{eq:mftheta}, the second-order approximation should satisfy</p>
<p>\begin{equation}
\frac{\partial \mathbf{m}_{i,t}(\alpha=0)}{\partial\alpha} + \frac{1}{2} \frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha} = 0,
\end{equation}</p>
<p>where the second-order derivative is</p>
<p>\begin{align}
\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha)}{\partial^{2}\alpha} = \int &amp;\mathrm{d} \mathbf{s}_{t-1} \int \mathrm{d} \mathbf{s}_{t-2} \Biggl( \frac{\partial^{2}\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial^{2}\alpha} \, P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} ) \nonumber\\
&amp;+ 2\frac{\partial\boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right)}{\partial\alpha} \, \frac{\partial P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial\alpha} \\
&amp;+ \boldsymbol{\varphi} \left(\mathbf{h}_{i,t}(\alpha)\right) \, \frac{\partial^{2} P_{\alpha}( \mathbf{s}_{t-1} \vert \mathbf{s}_{t-2} )}{\partial^{2}\alpha} \Biggr) P( \mathbf{s}_{t-2} ),
\end{align}</p>
<p>Neglecting terms higher than $\mathcal{O}(\alpha^2)$, we find (see <a href="#a6-thouless-anderson-palmer-approximation">Appendix A.6</a>)</p>
<p>\begin{equation}
\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha)}{\partial^{2}\alpha} \approx \ldots
\end{equation}</p>
<p>leading to</p>
<p>\begin{equation}
\frac{\partial^{2} \mathbf{m}_{i,t}(\alpha=0)}{\partial^{2}\alpha} \approx \ldots
\end{equation}</p>
<p>so that</p>
<p>\begin{equation}
\boldsymbol{\theta}^{*}_{i,t} = \ldots
\end{equation}</p>
<p>and we end up with the TAP mean-field equations:</p>
<p>\begin{equation}
\boxed{\mathbf{m}_{i,t} \approx \boldsymbol{\varphi} \left(\boldsymbol{\theta}^{*}_{i,t}\right) =  \ldots } \label{eq:tapmvector}
\end{equation}</p>
<p>$\ldots$</p>
<h2 id="33-a-simple-jax-implementation">3.3. A simple JAX implementation</h2>
<p>$\ldots$</p>
<h1 id="4-a-family-of-transformer-like-modules">4. A family of transformer-like modules</h1>
<p>$\ldots$</p>
<h2 id="41-fast--and-slow-moving-parameters">4.1. Fast- and slow-moving parameters</h2>
<p>$\ldots$</p>
<p>As proposed earlier in <a href="https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/" target="_blank" rel="noopener">Transformers Are Secretly Collectives of Spin Systems</a>, each example in a batch of sequential data can be thought of as probing the spin system in a particular way. Physically, the <em>fast-moving</em> parameterized couplings $\mathbf{J}(\mathbf{X})$ are determined by the <em>fast-moving</em> parameterized external fields $\mathbf{X}$, which in turn depend on the magnetizations of the previous layer and ultimately on the input data. The external fields act as an environment of patterns that gets transformed instantly into the values of the coupling matrix, effectively inducing something like a state of quenched disorder. The <em>slow-moving</em> parameters are those receiving gradient updates during training, e.g. the query-key matrices in the softmax couplings. Training can then be understood as <em>shaping the input-dependent distribution of coupling parameters</em> by amassing information from a huge amount of quenched disorder realizations, sculpting a spin glass with data.</p>
<p>$\ldots$</p>
<h1 id="5-conclusion">5. Conclusion</h1>
<p>$\ldots$</p>
<h1 id="appendices">Appendices</h1>
<h2 id="a1-normalization-constant">A.1. Normalization constant</h2>
<p>Let $Z(\beta, R, \mathbf{h})=\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}$ and switch to $D$-dimensional spherical coordinates to make our life easier. We use rotational symmetry to choose the polar axis parallel to $\mathbf{h}$,</p>
<p>\begin{equation}
Z(\beta, R, h) = R^{D-1} \int_{\Omega} \int_{0}^{\pi} \mathrm{d}^{D-2} \Omega \;\mathrm{d}\theta \; \mathrm{e}^{\beta R h \cos \theta } \sin^{D-2} \theta ,
\end{equation}</p>
<p>where $h=\lVert\mathbf{h}\rVert$ and where $\int_{\Omega} \mathrm{d}^{D-2} \Omega$ represents the integral over all other spherical angles, which coincides with the surface area of the unit sphere in $D-1$ dimensions,</p>
<p>\begin{equation}
S_{D-1} = \frac{2\pi^{\frac{D-1}{2}}}{\Gamma\left( \frac{D-1}{2} \right)},
\end{equation}</p>
<p>so that</p>
<p>\begin{equation}
Z(\beta, R, h) = \frac{2 \pi^{\frac{D-1}{2}} R^{D-1}}{\Gamma\left( \frac{D-1}{2} \right)} \int_{0}^{\pi} \mathrm{d}\theta \; \mathrm{e}^{\beta R h \cos \theta } \sin^{D-2} \theta .
\end{equation}</p>
<p>If we now let $u = \cos \theta$, then</p>
<p>\begin{equation}
Z(\beta, R, h) = \frac{2 \pi^{\frac{D-1}{2}} R^{D-1}}{\Gamma\left( \frac{D-1}{2} \right)} \int_{-1}^{1} \mathrm{d}u \; \mathrm{e}^{\beta R h u } \left(1 - u^2\right)^{(D-3)/2} .
\end{equation}</p>
<p>Recognizing <a href="https://dlmf.nist.gov/10.32#i" target="_blank" rel="noopener">an integral representation of the modified Bessel function of the first kind</a>,</p>
<p>\begin{equation}
I_{\nu}(z) = \frac{2^{-\nu}}{\sqrt{\pi}\, \Gamma\left(\nu+\frac{1}{2}\right)} z^{\nu} \int_{-1}^{1} \mathrm{d}t \; \mathrm{e}^{\pm zt} \left(1-t^2\right)^{\nu-\frac{1}{2}},
\end{equation}</p>
<p>we identify $\nu = D/2 - 1$ and $z = \beta R h$ to find</p>
<p>\begin{equation}
Z(\beta, R, h) = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R h) }{ \left(\beta h\right)^{D/2-1} }
\end{equation}</p>
<h2 id="a2-expected-value">A.2. Expected value</h2>
<p>Starting from the expression of the normalization constant Eq. \eqref{eq:partfun},</p>
<p>\begin{equation}
\int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} = \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert) }{ \left(\beta \lVert \mathbf{h}\rVert\right)^{D/2-1} } = Z(\beta, R, \lVert \mathbf{h}\rVert) ,
\end{equation}</p>
<p>we write the expected value as</p>
<p>\begin{equation}
\mathbf{m} = \frac{1}{Z} \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}} \, \mathbf{s} = \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \int_{S_{D-1}} \mathrm{d}^{D} \mathbf{s} \; \mathrm{e}^{\beta \, \mathbf{s} \cdot \mathbf{h}}
\end{equation}</p>
<p>so that</p>
<p>\begin{align}
\mathbf{m} = \frac{1}{\beta Z} \frac{ \partial }{ \partial \mathbf{h} } \left( \frac{ \left( 2 \pi R \right)^{D/2} I_{D/2 - 1}(\beta R \lVert\mathbf{h} \rVert) }{ \left(\beta \lVert\mathbf{h}\rVert \right)^{D/2-1} } \right)
\end{align}</p>
<p>which evaluates to</p>
<p>\begin{align}
\mathbf{m} = \left( \frac{I&rsquo;_{D/2 - 1}(\beta R \lVert \mathbf{h}\rVert)}{I_{D/2 - 1}(\beta R \lVert\mathbf{h}\rVert)} - \frac{ D/2-1 }{ \beta R \lVert\mathbf{h}\rVert} \right) \frac{R \mathbf{h}}{\lVert\mathbf{h}\rVert}.
\end{align}</p>
<p>Using the <a href="https://dlmf.nist.gov/10.29" target="_blank" rel="noopener">modified Bessel function recurrence relations</a>,</p>
<p>\begin{align}
I_{\nu-1}(z) - I_{\nu+1}(z) &amp;= \frac{2\nu}{z} I_{\nu}(z), \label{eq:irecurr}\\
I_{\nu-1}(z) + I_{\nu+1}(z) &amp;= 2 I'_{\nu}(z), \label{eq:irecurrderiv}
\end{align}</p>
<p>we end up with</p>
<p>\begin{align}
\mathbf{m} = \frac{I_{D/2}(\beta R \lVert \mathbf{h}\rVert)}{I_{D/2 - 1}(\beta R \lVert\mathbf{h}\rVert)} \frac{R \mathbf{h}}{\lVert\mathbf{h}\rVert}\equiv \boldsymbol{\varphi} (\mathbf{h}).
\end{align}</p>
<h2 id="a3-ratio-of-modified-bessel-functions">A.3. Ratio of modified Bessel functions</h2>
<p>To compute the ratio</p>
<p>\begin{align}
\frac{I_{\nu+1}(x)}{I_{\nu}(x)}
\end{align}</p>
<p>of modified Bessel functions of the first kind for $\nu \geq 0$ and $x \geq 0$, we implement a <a href="https://github.com/mcbal/spin-model-transformers/blob/main/spin_model_transformers/bessel.py" target="_blank" rel="noopener">JIT-compatible JAX version</a> of an algorithm described in <a href="https://www.jstor.org/stable/2005830" target="_blank" rel="noopener">(Amos, 1974)</a>. A pseudocode implementation can be found in <a href="https://isas.iar.kit.edu/pdf/ACC13_Kurz.pdf" target="_blank" rel="noopener">(Kurz et al., 2013)</a>. We compare our implementation against explicitly calculating the ratio using <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.ive.html#scipy.special.ive" target="_blank" rel="noopener"><code>scipy.special.ive</code></a> across a range of orders $\nu$ for several different values of $x$ to get a feel for its behavior.</p>
<p><img src="bessel_plot_1.png" alt=""></p>
<p>We observe a satisfying agreement between the two approaches. For $x=\sqrt{\nu}$, the ratio takes on very small values for large orders. For $x=\nu^2$, the oppositive happens and we see saturation. The case $x=\nu$ seems to sit in between, which suggests it might be opportune to fix the radius of our little spins to $R=\sqrt{D}$ so that with $\lVert\mathbf{h}\rVert \sim \mathcal{O}(\sqrt{D})$ we might maximize the &ldquo;sensitivity&rdquo; of the expected value.</p>
<h2 id="a4-mean-magnetization-derivatives">A.4. Mean-magnetization derivatives</h2>
<p>We are interested in computing the first-order and second-order derivative with respect to $\alpha$ of the function</p>
<p>\begin{equation}
\boldsymbol{\varphi}(\mathbf{h}(\alpha)) = \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \mathbf{h}(\alpha)}{\lVert \mathbf{h}(\alpha) \rVert},
\end{equation}</p>
<p>where $\mathbf{h}(\alpha) = \boldsymbol{\theta} + \alpha \Delta \mathbf{h}$. Using</p>
<p>\begin{equation}
\frac{\partial \lVert \mathbf{h}(\alpha) \rVert}{\partial\alpha} = \frac{\mathbf{h}(\alpha) \cdot \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert}
\end{equation}</p>
<p>and Eqs. \eqref{eq:irecurr}-\eqref{eq:irecurrderiv}, we find</p>
<p>\begin{align}
\frac{\partial \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial\alpha} = \beta &amp;\lambda (\lVert \mathbf{h}(\alpha) \rVert) \left( \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \cdot \Delta \mathbf{h} \right) \boldsymbol{\varphi}(\mathbf{h}(\alpha)) \nonumber \\
&amp;+ \frac{I_{D/2}(\beta R \lVert \mathbf{h}(\alpha) \rVert)}{I_{D/2 - 1}(\beta R \lVert \mathbf{h}(\alpha) \rVert)} \frac{R \Delta \mathbf{h}}{\lVert \mathbf{h}(\alpha) \rVert}
\end{align}</p>
<p>where</p>
<p>\begin{equation}
\lambda (x) = \frac{I^2_{D/2-1}(\beta R x)}{I^2_{D/2}(\beta R x)} - \frac{D}{\beta R x} \frac{I_{D/2-1}(\beta R x)}{I_{D/2}(\beta R x)} - 1.
\end{equation}</p>
<p>For the second-order derivative, we need to slog through even more tedious algebra,</p>
<p>\begin{equation}
\frac{\partial^2 \boldsymbol{\varphi}(\mathbf{h}(\alpha))}{\partial^2\alpha} = \ldots
\end{equation}</p>
<p>$\ldots$</p>
<h2 id="a5-naive-mean-field-approximation">A.5. Naive mean-field approximation</h2>
<p>$\ldots$</p>
<h2 id="a6-thouless-anderson-palmer-approximation">A.6. Thouless-Anderson-Palmer approximation</h2>
<p>$\ldots$</p>
<h2 id="references--footnotes">References &amp; footnotes</h2>
<p>A non-exhaustive list of references and inspiration includes:</p>
<ul>
<li>
<p>M. Aguilera, S.A. Moosavi, and H. Shimazaki, A unifying framework for mean-field theories of asymmetric kinetic Ising systems, <em>Nat Commun</em> <strong>12</strong>, 1197 (2021) <a href="https://arxiv.org/abs/2002.04309" target="_blank" rel="noopener">https://arxiv.org/abs/2002.04309</a></p>
</li>
<li>
<p>Y. Roudi and J. Hertz, Dynamical TAP equations for non-equilibrium Ising spin glasses, <em>J. Stat. Mech.</em>, P03031 (2011) <a href="https://arxiv.org/abs/1103.1044" target="_blank" rel="noopener">https://arxiv.org/abs/1103.1044</a></p>
</li>
<li>
<p>H.J. Kappen and J.J. Spanjers, Mean field theory for asymmetric neural networks, <em>Phys. Rev. E</em> <strong>61</strong>, 5658 (2000)</p>
</li>
<li>
<p>G. Parisi, Asymmetric neural networks and the process of learning, <em>J. Phys. A: Math. Gen.</em> <strong>19</strong> L675 (1986)</p>
</li>
</ul>
<hr>
<p>If you happen to find this work useful, please consider citing it as:</p>
<pre><code>@article{bal2023spinmodeltransformers,
  title   = {Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules},
  author  = {Bal, Matthias},
  year    = {2023},
  month   = {?},
  url     = {https://mcbal.github.io/post/spin-model-transformers}
}
</code></pre>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>We plot the absolute value to get rid of artificial &ldquo;jumps&rdquo; between the two branches. These occur because all models are simulated independently when sweeping across $\beta$ and the some combinations of initial state and model parameters might just happen to bounce to the other branch when $\beta$ changes in the $\beta &gt; \beta_c$ regime.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/artificial-intelligence/">Artificial Intelligence</a>
  
  <a class="badge badge-light" href="/tag/associative-memories/">Associative Memories</a>
  
  <a class="badge badge-light" href="/tag/attention/">Attention</a>
  
  <a class="badge badge-light" href="/tag/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tag/emergent-collective-computational-capabilities/">Emergent Collective Computational Capabilities</a>
  
  <a class="badge badge-light" href="/tag/ising-models/">Ising Models</a>
  
  <a class="badge badge-light" href="/tag/jax/">JAX</a>
  
  <a class="badge badge-light" href="/tag/many-body-systems/">Many-Body Systems</a>
  
  <a class="badge badge-light" href="/tag/mean-field-theory/">Mean-Field Theory</a>
  
  <a class="badge badge-light" href="/tag/neural-networks/">Neural Networks</a>
  
  <a class="badge badge-light" href="/tag/non-equilibrium-dynamics/">Non-Equilibrium Dynamics</a>
  
  <a class="badge badge-light" href="/tag/spin-glasses/">Spin Glasses</a>
  
  <a class="badge badge-light" href="/tag/spin-models/">Spin Models</a>
  
  <a class="badge badge-light" href="/tag/statistical-physics/">Statistical Physics</a>
  
  <a class="badge badge-light" href="/tag/transformers/">Transformers</a>
  
  <a class="badge badge-light" href="/tag/vector-spin-models/">Vector-Spin Models</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://mcbal.github.io/post/spin-model-transformers/&amp;text=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://mcbal.github.io/post/spin-model-transformers/&amp;t=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules&amp;body=https://mcbal.github.io/post/spin-model-transformers/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://mcbal.github.io/post/spin-model-transformers/&amp;title=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules%20https://mcbal.github.io/post/spin-model-transformers/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://mcbal.github.io/post/spin-model-transformers/&amp;title=Spin-Model%20Transformers:%20A%20Physics-Inspired%20Class%20of%20Transformer%20Modules" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://mcbal.github.io/"><img class="avatar mr-3 avatar-circle" src="/author/matthias-bal/avatar_hud6f6c722e4c9c5ebecd2fedd9826503b_202127_270x270_fill_q90_lanczos_center.jpg" alt="Matthias Bal"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://mcbal.github.io/">Matthias Bal</a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:matthiascbal%20at%20gmail%20dot%20com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MatthiasBal" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/matthiasbal/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.be/citations?user=vjYY0bMAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/mcbal" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>










<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/transformers-are-secretly-collectives-of-spin-systems/" rel="prev">Transformers Are Secretly Collectives of Spin Systems</a>
  </div>
  
</div>

</div>





  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/transformers-are-secretly-collectives-of-spin-systems/">Transformers Are Secretly Collectives of Spin Systems</a></li>
      
      <li><a href="/post/transformers-from-spin-models-approximate-free-energy-minimization/">Transformers from Spin Models: Approximate Free Energy Minimization</a></li>
      
      <li><a href="/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/">Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms</a></li>
      
      <li><a href="/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/">Transformer Attention as an Implicit Mixture of Effective Energy-Based Models</a></li>
      
      <li><a href="/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/">An Energy-Based Perspective on Attention Mechanisms in Transformers</a></li>
      
    </ul>
  </div>
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/bash.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/wowchemy.min.434af0ebce9e15b273b954d65feb39c7.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Matthias Bal © 2023
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
