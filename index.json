[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mcbal.github.io/author/matthias-bal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matthias-bal/","section":"authors","summary":"","tags":null,"title":"Matthias Bal","type":"authors"},{"authors":null,"categories":null,"content":"Content\u0026hellip; So you want to learn something new? Good.\nThe eventual goal of the courses listed below is for them to become centralized repositories with scattered pointers to the good stuff: useful blogposts, nuanced reviews, elegant papers, elucidating visualizations, comprehensive books, and intuitive explanations. The kind of stuff you\u0026rsquo;re often only able to appreciate after getting acquainted with a subject first but still wished you\u0026rsquo;d found out about earlier. Feel free to reach out if you have any more recommendations.\n","date":1605225600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1605225600,"objectID":"a97598ba3008e58e8209a63e3a48317e","permalink":"https://mcbal.github.io/learning/","publishdate":"2020-11-13T00:00:00Z","relpermalink":"/learning/","section":"learning","summary":"Content\u0026hellip; So you want to learn something new? Good.\nThe eventual goal of the courses listed below is for them to become centralized repositories with scattered pointers to the good stuff: useful blogposts, nuanced reviews, elegant papers, elucidating visualizations, comprehensive books, and intuitive explanations.","tags":null,"title":"Machine Learning","type":"book"},{"authors":[],"categories":[],"content":"XKCD 793: A physicist encountering machine learning for the first time   Introduction A growing zoo of Transformers  Vanilla Transformers Beyond vanilla: confronting quadratic scaling   From Hopfield networks to Transformers  Classical discrete Hopfield networks Modern discrete Hopfield networks Modern continuous Hopfield networks Modern continuous Hopfield networks as energy-based models  Energy-based models: a gentle introduction Exactly optimizing modern continuous Hopfield networks   Transformers store and retrieve context-dependent patterns Where are patterns stored in a Transformer?   Training Transformers  Pretraining loss functions Stepping through the Transformer: implicit energy minimization Meta-learning and few-shot inference   Beyond dot-product attention  Attention dynamics: embracing collective phenomena Why very long sequences should not be needed   Conclusion References \u0026amp; footnotes   1. Introduction In 2017, Attention Is All You Need 1 demonstrated state-of-the-art performance in neural machine translation by stacking only (self-)attention layers. Compared to recurrent neural networks, Transformer models exhibit efficient parallel processing of tokens, leading to better modeling of long-range correlations and, most importantly, favorable scaling in terms of data and compute. Since then, Transformers seem to have taken over natural language processing. Widespread adoption of attention-based architectures seems likely given recent work like An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale and the flurry of developments addressing the architecture\u0026rsquo;s quadratic scaling bottlenecks.\nRecently, the papers Hopfield Networks is All You Need 2 3 4 and Large Associative Memory Problem in Neurobiology and Machine Learning 5 provided complementary post-facto explanations of some of the success of Transformers from the perspective of energy-based models. In this post, I provide a biased overview of (self-)attention in Transformers and summarize its connections to modern Hopfield networks. Along the way, I look for intuition from physics and indulge in hand-wavy arguments on how an energy-based perspective can shed light on training and improving Transformer models.\n2. A growing zoo of Transformers Let\u0026rsquo;s start off with an overview of the components in a vanilla Transformer model. Since our focus is on (self-)attention, I am going to assume some prior knowledge6 and skip comprehensive architecture descriptions and experimental results. In Section 3, we will start from scratch and use Hopfield networks to build back up to the attention module described below.\nVanilla Transformers The proto-Transformer was introduced in an encoder-decoder context for machine translation in Attention Is All You Need. The original motivation seems to have been mostly driven by engineering efforts to model long-range correlations in sequence data and the recent successes of attention mechanisms stacked on top of recurrent neural networks. The main contribution and selling point of the paper was making an attention-only approach to sequence modeling work.\nLet\u0026rsquo;s focus on the encoder on the left and ignore the decoder on the right. Transformer models accept (batches of) sets of vectors, which covers most inputs people care about in machine learning. Text can be modelled as a sequence of embedded tokens. Images can be viewed as a snaky sequence of embedded pixels or embedded patches of pixels. Since sets have no notion of ordering, learned or fixed positional information needs to be explicitly added to the input vectors.\nThe main module in the Transformer encoder block is the multi-head self-attention, which is based on a (scaled) dot-product attention mechanism acting on a set of $d$-dimensional vectors:\n\\begin{equation} \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V} \\label{eq:vanilla-attention} \\end{equation}\nHere, queries $\\mathbf{Q}$, keys $\\mathbf{K}$, and values $\\mathbf{V}$ are matrices obtained from acting with different linear transformations \u0026mdash; parametrized respectively by weights $\\mathbf{W}_{\\mathbf{Q}}$, $\\mathbf{W}_{\\mathbf{K}}$, and $\\mathbf{W}_{\\mathbf{V}}$ \u0026mdash; on the same set of $d$-dimensional inputs. Cross-attention takes the inputs for its queries from a different source than for its keys and values, as can be glimpsed from the decoder part of the architecture on the right.\nFor every input query, the updated output query of \\eqref{eq:vanilla-attention} is a linear combination of values weighted by an attention matrix quantifying the overlap of the input query with the keys corresponding to these values. Since all objects are vectors and the attention mechanism is just a dot product between vectors, we can think of the attention module as matching query vectors to their \u0026ldquo;closest\u0026rdquo; key vectors in latent space and summing up contributions from value vectors, weighted by the \u0026ldquo;closeness\u0026rdquo; of their keys to the queries.\nThe remaining components of the Transformer encoder block are needed to make the module work properly in practice:\n The multi-headedness of the attention module refers to chunking up the dimension of the vector space and having multiple attention operations running in parallel in the same module, yet with each acting on a lower-dimensional segment of the full space. This is a trick to (1) get around the fact that every input vector only couples to one query at a time to calculate its attention coefficient, and (2) provide multiple starting points in the subspaces for the queries, which might help to avoid bad local minima in parameter space during optimization. A positional feed-forward network, made up of two linear layers with a non-linearity in between, is inserted at the end of the module. Folklore wisdom tells us that the feed-forward layer needs to blow up the dimension of the latent space by a factor of four for it to be able to \u0026ldquo;disentangle\u0026rdquo; the represention. More likely though, it\u0026rsquo;s a way to increase model capacity and warp latent spaces since the attention modules on their own are pretty much linear apart from the $\\mathrm{softmax}$-operator used to obtain the normalized attention coefficients. Residual connections are added to control the flow of gradients. Layer normalisation is used to control learning dynamics and keep vector norms from exploding.  Beyond vanilla: confronting quadratic scaling Most architectural variations of the vanilla Transformer are targeted at the attention module, which scales poorly with respect to the input sequence length $N$. Since the overlap of all queries with all keys is required, calculating a dense attention matrix scales like $\\mathcal{O}(N^2)$ in time and space. Limits on the context window of the attention mechanism during training prevent the model from learning how to deal with long sequences and long-range correlations. The majority of post-vanilla Transformer species can be classified into one of the following buckets6:\n Low-rank approximations: truncate the matrix product $\\mathbf{Q} \\mathbf{K}^T$ since it\u0026rsquo;s likely not full rank for structured data Sparsification: reduce the attention calculation from all query-key pairs to a subset because not all of them feel the need to talk to each other Recurrence: keep track of a (compressed) history of context Kernels: approximate the attention operation with kernel methods  For the remainder of our discussion, we will focus on vanilla Transformers. One of the goals of this blog post is to explore how a different perspective on the function of attention-based algorithms might lead to qualitatively different improvements beyond what is possible by relying on scaling and reducing computational complexity alone.\n3. From Hopfield networks to Transformers In this section, we provide a short history of Hopfield networks and gradually build up intuition until we can recognize the Transformer self-attention mechanism for what it really is. We refer to the blog post accompanying Hopfield Networks is All You Need for more details and insightful visualizations of pattern storage and retrieval.\nClassical discrete Hopfield networks A Hopfield network is a simple model for associative memory popularized by John Hopfield in his 1982 paper Neural Networks and Physical Systems with Emergent Collective Computational Abilities7. The task of an associative memory is to store and retrieve patterns, preferably in a way that allows one to recover stored patterns quickly with a low error rate.\nThe basic idea of the Hopfield network \u0026mdash; and other energy-based models like Boltzmann machines \u0026mdash; is to construct an energy function which defines an energy landscape containing basins of attraction around patterns we want to store. Starting at any pattern, we want to have an update rule pointing towards the closest stored pattern, guided by a scalar \u0026ldquo;closeness\u0026rdquo; score provided by the energy function.\n\nLet\u0026rsquo;s make this a bit more formal but not too formal. Consider trying to store a set of $N$ binary patterns $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$ where each pattern $\\boldsymbol{x}_{i}$ is a $d$-dimensional vector whose entries are either $-1$ or $1$. For example, in the case of storing black-and-white images, every image would correspond to a string of pixel values, a binary pattern $\\boldsymbol{x}_{i}$.\nFor any query $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$, or state pattern, we want to find a way to retrieve the closest stored pattern. In his paper, Hopfield considered the energy function\n\\begin{equation} E = - \\frac{1}{2} \\boldsymbol{\\xi}^{T} \\boldsymbol{W} \\boldsymbol{\\xi} + \\boldsymbol{\\xi}^{T} \\boldsymbol{b} = - \\frac{1}{2} \\sum_{i=1}^{d} \\sum_{j=1}^{d} w_{ij} \\xi_{i} \\xi_{j} + \\sum_{i=1}^{d} b_{i} \\xi_{i} , \\label{eq:ising} \\end{equation}\nwhere $\\boldsymbol{b} \\in \\mathbb{R}^{d}$ denotes a bias vector and the weights $\\boldsymbol{W} \\in \\mathbb{R}^{d \\times d}$ are set to the sum of the outer products of the patterns we want to store\n\\begin{equation} \\boldsymbol{W} = \\sum_{i=1}^{N} \\boldsymbol{x}_{i} \\otimes \\boldsymbol{x}_{i}^{T}. \\end{equation}\nThe state pattern update rule is given by the sign of the gradient of \\eqref{eq:ising} with respect to $\\boldsymbol{\\xi}$ and can be done in one step (synchronously) or separately for every component of the vector (asynchronously):\n\\begin{equation} \\boldsymbol{\\xi}_{n+1} = \\mathrm{sgn} \\left( \\boldsymbol{W} \\boldsymbol{\\xi}_{n} - \\boldsymbol{b} \\right). \\end{equation}\nThe storage capacity of this system for retrieval of patterns with a small amount of errors can be shown to be $C \\cong 0.14 d$, scaling linearly with the dimension of the pattern vector.\nPhysical intuition Physicists immediately recognize the energy function \\eqref{eq:ising} as an incarnation of the Ising model. Spin degree of freedoms $\\xi_{i}$ are grouped into patterns $\\boldsymbol{\\xi}$ that are equivalent to spin configurations of $d$ spins. The weight matrix is a sum of stored-pattern spin configurations, serving as attractors for the state-pattern spin configuration. The couplings $w_{ij}$ can be regarded a sum of samples of an underlying pattern data distribution. They are not restricted to (nearest-)neighbors and their values are neither uniform like in exactly solvable models nor totally random like in spin glass models.\n Neural networks and spin glasses: There is some literature on connections between spin glasses and neural networks. Spin glasses are phases of matter describing disordered magnetic systems exhibiting both quenched disorder and frustratation. Spin glasses were a major inspiration for Hopfield networks, as beautifully explained by the condensed matter physicist Philip W. Anderson in a column series for Physics Today (1988-1990). However, I could not find any recent papers that point to a productive research direction beyond qualitative statements like \u0026ldquo;here\u0026rsquo;s two hard problems where symmetry and order will not help you solve them\u0026rdquo;.\n Modern discrete Hopfield networks Modern discrete Hopfield networks (or dense associative memories) introduced the following family of energy functions to improve pattern storage capacity and pattern separation capabilities 8 9\n\\begin{equation} E = - \\sum_{i=1}^{N} F \\left( \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{\\xi} \\right) \\end{equation}\nCompared to the classical discrete Hopfield network energy function \\eqref{eq:ising}, the explicit weight matrix is gone and the energy has been reduced to a sum of a function of dot products between the state pattern $\\boldsymbol{\\xi}$ and every stored pattern $\\boldsymbol{x}_i$. For a polynomial interaction function $F(x) = x^{a}$, low-error storage capacity is $C \\cong d^{a-1}$. The quadratic, classical discrete Hopfield network is recovered by setting $a=2$.\nEssentially, the role of $F(x)$ is to separate close patterns by blowing up differences in dot product values. Few things blow up better than exponentials, so we can generalize the energy to\n\\begin{equation} E = - \\sum_{i=1}^{N} \\exp \\left( \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{\\xi} \\right) \\end{equation}\nwith storage capacity $C \\cong 2^{d/2}$. The corresponding update rules for modern discrete Hopfield networks can be shown to converge quickly with high probability9.\nModern continuous Hopfield networks Most machine learning applications are tailored to work with continuous embeddings (vector representations) rather than discrete patterns. Is there a way to generalize modern Hopfield networks to continuous data? Recently, Hopfield Networks is All You Need proposed the following energy function to deal with continuous $d$-dimensional patterns10:\n\\begin{equation} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right), \\label{eq:energyfunc} \\end{equation}\nwhich we consider to be a function of the state pattern $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ and parametrized by $N$ stored patterns $\\boldsymbol{X} = (\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{N}) \\in \\mathbb{R}^{d \\times N}$. From the point of view of restricted Boltzmann machines, the stored patterns $\\boldsymbol{X}^T$ can also be interpreted as weights mapping $\\boldsymbol{\\xi}$ to hidden units5.\n Smoothly taking a maximum: The $\\mathrm{logsumexp}$ operator is defined for vectors $\\mathbf{x}$ as \\begin{equation} \\mathrm{logsumexp} \\left( \\mathbf{x} \\right) = \\log \\left( \\sum_{i=1}^{N} \\mathrm{e}^{x_i} \\right) \\end{equation} while for matrix arguments (like a batch of vectors), the $\\mathrm{sumexp}$ is understood to apply to just one dimension after which the $\\log$ acts element-wise on the resulting vector.\n Physical intuition We assume that the stored patterns equilibrate much quicker than those of the state pattern so that the former can effectively be considered \u0026ldquo;frozen\u0026rdquo;. The energy function \\eqref{eq:energyfunc} looks deceptively simple: there is a single state pattern and there are no interactions among stored patterns. The first term takes care of making sure the norm of the input state pattern is finite, while the second term scores the query\u0026rsquo;s overlap based on its individual alignment with every stored pattern. The exponential function in the term\n\\begin{equation} \\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) = \\log \\left( \\sum_{i=1}^{N} \\mathrm{e}^{\\mathbf{x}_i \\cdot \\boldsymbol{\\xi}} \\right) \\end{equation}\nis used to pull apart close patterns by blowing up differences in the dot product between state pattern and stored patterns. From the perspective of the query, it is not so much an interaction term but rather a measure of the alignment of the query to external \u0026ldquo;magnetic fields\u0026rdquo; generated by the stored patterns.\nDeriving the update rule In the spirit of hand-waving, let us refuse to resort to of the dynamical systems machinery used in the original references 2 5 and rather derive the update rule for the state pattern $\\boldsymbol{\\xi}$ by taking the derivative of the energy function \\eqref{eq:energyfunc} with respect to $\\boldsymbol{\\xi}$\n\\begin{equation} \\nabla_{\\boldsymbol{\\xi}} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\boldsymbol{\\xi} - \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right). \\end{equation}\nA gradient descent update with step size $\\gamma$ looks like\n\\begin{equation} \\boldsymbol{\\xi}_{n+1} = \\boldsymbol{\\xi}_{n} - \\gamma \\left( \\boldsymbol{\\xi}_{n} - \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi}_{n}\\right) \\right). \\label{eq:conthopfupdate} \\end{equation}\nWe are very confident that the topography of the energy landscape allows us to take big steps and boldly set $\\gamma = 1$ to recover the familiar update rule\n\\begin{align} \\boldsymbol{\\xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi}_{n}\\right) . \\end{align}\nThe updated vector is a linear combination of all stored patterns, weighted by an attention matrix quantifying the overlap with the input pattern.\nModern continuous Hopfield Networks as energy-based models Let\u0026rsquo;s now try to connect the system defined by the energy function \\eqref{eq:energyfunc} to the statistical mechanics framework of energy-based models11.\nEnergy-based models: a gentle introduction Energy-based models learn a parametrized energy function $E_{\\theta}$ which maps data points $\\boldsymbol{x}$ to real, scalar energy values $E_{\\theta}(\\boldsymbol{x})$. The data distribution is modeled by the Boltzmann distribution, \\begin{equation} p_{\\theta}(\\boldsymbol{x}) = \\frac{\\mathrm{e}^{ - E_{\\theta}(\\boldsymbol{x}) }}{Z(\\theta)}, \\label{eq:boltzmann} \\end{equation} where $Z(\\theta) = \\int \\mathrm{d} \\boldsymbol{x} \\ \\mathrm{e}^{-E(\\boldsymbol{x})}$ denotes the system\u0026rsquo;s partition function. Configurations $\\boldsymbol{x}$ with low energies $E_{\\theta}(\\boldsymbol{x})$ are considered more likely and their weight contributes more strongly to the partition function.\nTo steer the model distribution $p_{\\theta}$ towards a target data distribution $p_{\\mathrm{data}}$, we can try to minimize the likelihood loss function\n\\begin{equation} \\mathcal{L}_{\\mathrm{ML}} (\\theta) = \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\mathrm{data}}} \\left[ -\\log p_{\\theta} (\\boldsymbol{x}) \\right], \\label{eq:nll} \\end{equation}\nwhere the negative log-likelihood equals\n\\begin{equation} -\\log p_{\\theta} (\\boldsymbol{x}) = E_{\\theta} (\\boldsymbol{x}) + \\log Z (\\theta). \\end{equation}\nThis is a hard optimization problem because calculating $\\log Z (\\theta)$ is hard for the vast majority of high-dimensional data distributions we care about. In practice, people resort to approximations like contrastive divergence to push the energy down on \u0026ldquo;positive examples\u0026rdquo; drawn from the data distribution while pushing up on \u0026ldquo;negative examples\u0026rdquo; obtained from sampling the model distribution. Even though sampling from \\eqref{eq:boltzmann} can be done with methods like Markov Chain Monte Carlo, it is computationally expensive to do so, especially as part of an inner-loop optimization step12.\nExactly optimizing modern continuous Hopfield networks So what about the system defined by the energy function \\eqref{eq:energyfunc}? Let\u0026rsquo;s consider the stored patterns $\\mathbf{X} \\in \\mathbb{R}^{d \\times N}$ as the model parameters we want to optimise. The task for the model is then to try to memorise incoming state patterns $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ drawn from some data distribution $p_{\\mathrm{data}}$ by deciding what kind of patterns to store. The partition function looks like\n\\begin{equation} Z = \\int \\mathrm{d} \\boldsymbol{\\xi} \\ \\mathrm{e}^{-E(\\boldsymbol{\\xi})} = \\int \\mathrm{d} \\boldsymbol{\\xi} \\ \\mathrm{e}^{-\\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi}} \\left( \\sum_{i=1}^{N} \\mathrm{e}^{ \\boldsymbol{x}^{T}_{i} \\cdot \\boldsymbol{\\xi} } \\right) \\label{eq:zforcontinuoushopfield} \\end{equation}\nwhich, because of the $\\log$ in the \u0026ldquo;interaction term\u0026rdquo;, boils down to a sum of $n$-dimensional Gaussian integrals\n\\begin{aligned} Z = (2\\pi)^{n/2} \\sum_{i=1}^{N} \\mathrm{e}^{ \\frac{1}{2} \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{x}_{i} } \\end{aligned}\nAfter taking the logarithm, we end up with the $\\mathrm{logsumexp}$ operator:\n\\begin{equation} \\log Z = \\frac{n}{2} \\log \\left( 2\\pi \\right) + \\mathrm{logsumexp} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) \\end{equation}\nwhere the $\\mathrm{diag}$ operator is understood to turn the diagonal of its matrix argument into a vector. Plugging this expression into \\eqref{eq:nll} leads to the following loss function for the matrix of stored patterns\n\\begin{align} \\mathcal{L}_{\\mathrm{ML}} (\\mathbf{X}) = \u0026amp; \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\\n\u0026amp; + \\mathrm{logsumexp} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) + \\frac{n}{2} \\log \\left( 2\\pi \\right) \\end{align}\nand a gradient\n\\begin{align} \\nabla_{\\mathbf{X}} \\mathcal{L}_{\\mathrm{ML}} (\\mathbf{X}) = \u0026amp; - \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\boldsymbol{\\xi} \\otimes \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\\n\u0026amp; + \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) \\end{align}\nand an update with step size $\\gamma$\n\\begin{align} \\mathbf{X}_{n+1} = \\ \\mathbf{X}_{n} \u0026amp;+ \\gamma \\ \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\boldsymbol{\\xi} \\otimes \\mathrm{softmax} \\left( \\boldsymbol{X}^T_{n} \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\\n\u0026amp; - \\gamma \\ \\mathbf{X}_{n} \\ \\mathrm{softmax} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T}_{n} \\boldsymbol{X}_{n} \\right) \\right) \\end{align}\nLet\u0026rsquo;s try to guess what this means for a single input state pattern. The first gradient term pushes all stored patterns towards the sample but weighted by an dot-product attention matrix quantifying their overlap with the input pattern, similar to \\eqref{eq:conthopfupdate} but in the other direction. The second gradient term comes from the partition function and acts as a regularizer by keeping the norms of the stored patterns in check. Regularization keeps pattern values within a reasonable range and pushes the system towards regions in parameter space with non-trivial small dot-product values.\nTransformers store and retrieve context-dependent patterns Making the leap from modern continous Hopfield networks to the vanilla Transformer (self-)attention mechanism we encountered in Section 2 requires a few additional steps, as explained in detail in the blog post accompanying Hopfield Networks is All You Need.\n We want to act on multipe $d$-dimensional state patterns at the same time in order to retrieve multiple updated patterns in parallel: \\begin{align} \\boldsymbol{\\xi} \\in \\mathbb{R}^{d} \\to \\boldsymbol{\\Xi} = (\\boldsymbol{\\xi}_{1}, \\ldots, \\boldsymbol{\\xi}_{S}) \\in \\mathbb{R}^{d \\times S} \\end{align} so that \\begin{align} \\boldsymbol{\\Xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi}_{n}\\right) . \\end{align} In practice, the number of state patterns $S$ is often taken to be equal to the number of stored patterns $N$. We want to map stored patterns $\\mathbf{X}$ and state patterns $\\boldsymbol{\\Xi}$ respectively to keys $\\mathbf{K} \\in \\mathbb{R}^{N \\times d}$ and queries $\\mathbf{Q} \\in \\mathbb{R}^{S \\times d}$ in a common feature space using linear transformations $\\mathbf{W_{K}}$ and $\\mathbf{W_{Q}}$. We want introduce another linear transformation $\\mathbf{W_{V}}$ on stored patterns to transform them into values $\\mathbf{V} \\in \\mathbb{R}^{N \\times d}$ appropriate for the keys' content. We want to modify the learning dynamics by decreasing the inverse temperature to $\\beta = 1 / \\sqrt{d}$, effectively making the $\\mathrm{softmax}$ softer by increasing the temperature of the system13.  The result is the update rule we stated without explanation in Section 2: \\begin{equation} \\mathbf{Q}^{\\mathrm{updated}} = \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V}, \\label{eq:transformerattnupdate} \\end{equation} where the $\\mathrm{softmax}$ acts row-wise. In practice, the vanilla Transformer module additionally wraps the above attention module in (1) residual connections to control the flow of gradients, (2) layer norms to control pattern normalisations and learning dynamics, and (3) a positional feed-forward network for additional model capacity.\nWhere are patterns stored in a Transformer? Let\u0026rsquo;s try to digest the implications of these quite substantial changes. It\u0026rsquo;s useful to think of Transformer (self-)attention modules as dynamic pattern storage and retrieval systems. In modern continuous Hopfield networks, stored patterns are considered a given. However, in the Transformer (self-)attenton module, patterns to be matched and retrieved are dependent on inputs and implicitly stored in the weights $\\mathbf{W_{Q}}$, $\\mathbf{W_{K}}$, and $\\mathbf{W_{V}}$ of the linear transformations. In every layer, the module needs to learn how to map a set of inputs to patterns it wants to store (keys and values) as well as how to best retrieve them (queries). Within the same layer, dynamically generated queries are matched to keys within the same latent space. Between attention modules of neighboring layers, the non-linear activation function in the positional feed-forward network warps latent spaces.\n4. Training Transformers Now that we are aware of an energy-based interpretation of dot-product (self-)attention, we can start hand-waving about what could be going on during the supervised training procedure of Transformer models and how energy-based models suggest a qualitatively different approach to improving attention mechanisms.\nPretraining loss functions The goal of pretraining loss functions is to induce useful data-dependent pattern storage and retrieval behavior. Pretraining strategies for Transformer-based language models rely on loss functions derived from auxiliary tasks to learn statistical patterns in natural language. Starting from almost identical model architectures, autoregressive models like GPT-3 leverage all their parameters to predict the next token in a sequence given previous tokens while autoencoding models like BERT try to reconstruct corrupted tokens. In both cases, the loss function is a cross-entropy loss involving predictions in the space of the model\u0026rsquo;s token vocabulary.\nStepping through the Transformer: implicit energy minimization Although no energy function is explicitly optimized during training14, let\u0026rsquo;s see how far we can push hand-wavy energy-based arguments by stepping through the forward and backward pass of a Transformer model. We have learned that the attention update \\eqref{eq:transformerattnupdate} in every Transformer layer is actually a hidden gradient step. This trivial insight leads to a trio of trivial observations.\nTrivial Observation #1: During training, the update step \\eqref{eq:transformerattnupdate} of the attention mechanism in a Transformer layer acts as an inner-loop optimization step, minimizing an implicit energy function determined by the queries, keys, and values constructed from the output of the previous layer.\nTrivial Observation #2: During the forward pass of a deep Transformer model, a nested hierarchy of energy functions is minimized.\nTrivial Observation #3: During the backward pass of a deep Transformer model, the parameters of its attention modules get updated such that the inner-loop optimization steps conspire to pattern match queries to keys in such a way that the sequentially-updated final latent representations are useful for improving the loss.\nMeta-learning and few-shot inference Squinting our eyes, we can see traces of a meta-learning problem: how to tune model parameters \u0026mdash; in particular the attention mechanisms' linear transformation matrices \u0026mdash; such that applying a sequence of one-step attention updates to sets of input patterns converges to representations useful for minimizing the (meta-)loss function. Learnable modules of a differentiable program can of course often be considered part of a larger meta-learning setup. But what this point of view suggests is that confining the one-step inner-loop update to a simple associative memory pattern lookup might be quite restrictive.\nYet even with with a simple dense associative memory, OpenAI\u0026rsquo;s paper Language Models are Few-Shot Learners showed that large-capacity models like GPT-3 already exhibit quite impressive meta-learning capabilities. The energy-based perspective provides a naive yet attractive explanation for this phenomenon. At inference time, the few-shot demonstrations, which make up the initial part of a few-shot learning query, condition the sequential generation process by providing basins of attraction in the energy landscape for other energy minimization steps to be pulled towards. The GPT-3 model is memorizing to the extent the demonstrations match patterns seen during training and generalizing within the possibilites of the rudimentary attention dynamics of the simple underlying energy functions.\n5. Beyond dot-product attention Let\u0026rsquo;s conclude this post with two related thoughts inspired by an energy-based perspective on current attention architectures: attention dynamics and modeling very long sequences.\nAttention dynamics: embracing collective phenomena We have seen that the energy function of a modern continuous Hopfield network \\eqref{eq:energyfunc} is rather uninspiring from a physics perspective. Theoretically, the exponential storage and efficient retrieval of patterns is obtained by burning deep valleys into the energy landscape around stored patterns (keys) for neighbouring state patterns (queries) to quickly roll into. In practice, the authors of Hopfield Networks is All You Need observed three kinds of fixed-point behavior in a pretrained BERT model: (1) global fixed points averaging over all stored patterns, (2) metastable states averaging over a subset of stored patterns, and (3) fixed points returning a single, well-separated stored pattern.\nWhat does this tell us? Assuming the attention updates converge faithfully during training, the linear maps turning input vectors into queries, keys, and values can become bottlenecks in terms of being able to separate patterns and organise the energy landscape. Additionally, the lack of interactions among patterns and the decoupled dot-product overlap between queries and keys puts considerable limits on how the network can process information. In practice, this is being partially addressed by using multiple attention heads (see Section 2), but this solution does not feel satisfactory.\nWhy very long sequences should not be needed Recurrent neural networks try to compress patterns in a single hidden state via sequential propagation but often fail to do so and forget stuff along the way. Transformers bake patterns into a hierarchical energy landscape but focus on a fixed-length context window to store and retrieve patterns. As we\u0026rsquo;ve seen in Section 2, a lot of research on improving Transformers focuses on alleviating the $\\mathcal{O}(N^2)$ bottleneck of the attention computation with the implicit goal of scaling to longer sequences and enabling larger context windows.\nBut very long sequences should not be needed if patterns are allowed to talk to each other. A model should not need all of the world as context if patterns and emergent concepts can be connected. It\u0026rsquo;s definitely worthwhile to try to reduce the computational complexity of current attention architectures, but it might be far more valuable to swap the simple energy-based model \\eqref{eq:energyfunc} for more interesting energy-based models. Why not dust off the old unrestricted Boltzmann machine once again? Or experiment with any one of a century\u0026rsquo;s worth of physics models? Not to train them explicitly, but have them serve as implicit models underlying more intricate attention mechanisms, mediated by (local) interactions among patterns. Naturally, after so much hand-waving, our journey has to end here.\n6. Conclusion Even if attention turns out to not be all we need, (self-)attention modules have established themselves as highly parallelizable neural network building blocks capable of dynamically routing information based on context. We have seen that dot-product attention modules in Transformer models work by encoding high-dimensional patterns into the landscapes of simple energy functions, enabling fast pattern storage and retrieval. During training, these landscapes are sculpted to accommodate statistical patterns found in data by hierarchically matching and combining latent pattern representations through a sequence of implicit energy function minimizations.\nWe argued that an energy-based perspective on attention provides an intuitive explanation of meta-learning capabilities of large-capacity language models and encourages the exploration of qualitatively different attention mechanisms for pattern storage and retrievel. Rather than naively scaling the current generation of Transformers, it might be more rewarding to scale learning itself by exploring more powerful, expressive, and computationally efficient attention mechanisms, guided by energy-based models. Perhaps we should consider looking at neural networks again like John Hopfield already did in 1982: physical systems with emergent collective computational abilities.\nReferences \u0026amp; footnotes   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention Is All You Need (2017) \u0026#x21a9;\u0026#xfe0e;\n Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, Hopfield Networks is All You Need (2020) \u0026#x21a9;\u0026#xfe0e;\n Johannes Brandstetter, https://ml-jku.github.io/hopfield-layers/ (2020) \u0026#x21a9;\u0026#xfe0e;\n Johannes Brandstetter and Hubert Ramsauer, https://ml-jku.github.io/blog-post-performer/ (2020) \u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020) \u0026#x21a9;\u0026#xfe0e;\n If you have only just joined the attention revolution, there are a lot of great resources out there to get you started. Yannic Kilcher provides a great introduction in his video on Attention is All You Need. The High Performance NLP tutorial slides presented at EMNLP 2020 contain a thorough and visually appealing introduction to attention-based models. Because code is usually more to the point than papers that need to sell themselves, I highly recommend Phil Wang\u0026rsquo;s excellent collection of self-contained repositories showcasing some of the latest models and techniques. \u0026#x21a9;\u0026#xfe0e;\n John Hopfield, Neural Networks and Physical Systems with Emergent Collective Computational Abilities (1982) \u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Dense Associative Memory for Pattern Recognition (2016) \u0026#x21a9;\u0026#xfe0e;\n Mete Demircigil, Judith Heusel, Matthias Löwe, Sven Upgang, and Franck Vermet, On a Model of Associative Memory with Huge Storage Capacity (2017) \u0026#x21a9;\u0026#xfe0e;\n A physicist might consider these continuous patterns spin configurations of the degrees of freedom in a vector spin model where the internal dimension $D \\sim 10^2-10^4$ is much bigger than familiar small-$D$ cases like the XY model or the Heisenberg model but much smaller than infinity. \u0026#x21a9;\u0026#xfe0e;\n Yann LeCun, Sumit Chopra, Raia Hadsell, Marc\u0026rsquo;Aurelio Ranzato, and Fu Jie Huang, A Tutorial on Energy-Based Learning (2006) \u0026#x21a9;\u0026#xfe0e;\n The generator in a Generative Adverserial Network (GAN) setup can be considered a clever way to generate negative samples for the implicit energy function optimization taking place in the discriminator. \u0026#x21a9;\u0026#xfe0e;\n As we have seen in Section 2, the naive interpretation of $\\beta$ as the effective inverse temperature is tenuous in practice given the influence of the surrounding layer normalisation modules. \u0026#x21a9;\u0026#xfe0e;\n The implicitly defined energy functions in Tranformer layers are not optimized directly because they arguably do not provide a meaningful training signal on their own. Verifying whether this is true or not could make for an interesting experiment. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1606557261,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606557261,"objectID":"8e7322e243b34c70767e389210fb4d59","permalink":"https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/","publishdate":"2020-11-28T10:54:21+01:00","relpermalink":"/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/","section":"post","summary":"Can an energy-based perspective shed light on training and improving Transformer models?","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Energy-Based Models","Neural Networks","Statistical Physics","Transformers"],"title":"An Energy-Based Perspective on Attention Mechanisms in Transformers","type":"post"},{"authors":[],"categories":[],"content":"","date":1605375722,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605375722,"objectID":"ef2281265836fab1d2405974a0a82f55","permalink":"https://mcbal.github.io/project/ebm-tns/","publishdate":"2020-11-14T18:42:02+01:00","relpermalink":"/project/ebm-tns/","section":"project","summary":"","tags":[],"title":"Energy-Based Models and Tensor Networks","type":"project"},{"authors":null,"categories":null,"content":"Content\u0026hellip; So you want to learn something new? Good.\nThe eventual goal of the courses listed below is for them to become centralized repositories with scattered pointers to the good stuff: useful blogposts, nuanced reviews, elegant papers, elucidating visualizations, comprehensive books, and intuitive explanations. The kind of stuff you\u0026rsquo;re often only able to appreciate after getting acquainted with a subject first but still wished you\u0026rsquo;d found out about earlier. Feel free to reach out if you have any more recommendations.\n","date":1605225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605225600,"objectID":"80d40b4dcf28bd7f9904180b36127b63","permalink":"https://mcbal.github.io/learning/capita/","publishdate":"2020-11-13T00:00:00Z","relpermalink":"/learning/capita/","section":"learning","summary":"Content\u0026hellip; So you want to learn something new? Good.\nThe eventual goal of the courses listed below is for them to become centralized repositories with scattered pointers to the good stuff: useful blogposts, nuanced reviews, elegant papers, elucidating visualizations, comprehensive books, and intuitive explanations.","tags":null,"title":"Capita Selecta","type":"book"},{"authors":null,"categories":null,"content":"A good understanding of probability theory, information theory, statistics, linear algebra, and calculus is useful to appreciate the possibilities and limitations of machine learning algorithms. Knowledge of programming, numerical optimization, data structures, and numerical algorithms is required to turn ideas into efficient implementations. Most of the resources listed below provide a mix of all of the above topics, to varying degrees of rigour.\nProbability Theory After going through Wikipedia, consider reading:\n Review of Probability Theory by Arian Maleki and Tom Do Visual Information Theory by Christopher Olah contains beautifully intuitive explanations of the basics of information theory (probability distributions, entropy, cross-entropy, KL divergence, and mutual information) Probability Theory: The Logic of Science by E. T. Jaynes if you woke up feeling particularly Bayesian today and want to punch a frequentist in the face  Machine Learning The following textbooks are more or less considered classic references in the fields of pattern recognition, statistical learning, statistical inference, and probabilistic machine learning. If you are intimidated by the length of some of these books (I have definitely not read all of them yet), just start with the introductory chapters and then do a random walk through the book. If anything, these books should teach you that there you do not need deep learning to do clever stuff and that most of the latest trends are built on top of very solid foundations that predate deep learning.\n Information Theory, Inference, and Learning Algorithms by David J. C. MacKay Pattern Recognition and Machine Learning by Christopher M. Bishop The Elements of Statistical Learning: Data Mining, Inference, and Prediction by Trevor Hastie, Robert Tibshirani, Jerome Friedman All of Statistics: A Concise Course in Statistical Inference by Larry Wasserman Machine Learning: A Probabilistic Perspective by Kevin P. Murphy + errata and notes  According to its author, Machine Learning: a Probabilistic Perspective is more Bayesian than the Hastie or Wasserman books but more frequentist than Bishop. If you care deeply about these kinds of things: choose wisely.\nDeep Learning  Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville is an overview of the field in 2016, split in three parts which gradually increase in difficulty: Applied Math and Machine Learning Basics, Modern Practical Deep Networks, and Deep Learning Research  Online Courses  Neural Networks and Deep Learning by Michael Nielsen CS229 Stanford course on machine learning and statistical pattern recognition Machine Learning for Physicists: Neural Networks and their Applications by Florian Marquardt is a nice introduction for physicists who cannot help but always want to understand everything in terms of their own biased framework  ","date":1605225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605225600,"objectID":"b34e8e89c0543290d10889e6300c7458","permalink":"https://mcbal.github.io/learning/foundations/","publishdate":"2020-11-13T00:00:00Z","relpermalink":"/learning/foundations/","section":"learning","summary":"A good understanding of probability theory, information theory, statistics, linear algebra, and calculus is useful to appreciate the possibilities and limitations of machine learning algorithms. Knowledge of programming, numerical optimization, data structures, and numerical algorithms is required to turn ideas into efficient implementations.","tags":null,"title":"Foundations","type":"book"},{"authors":null,"categories":null,"content":"Content\u0026hellip; So you want to learn something new? Good.\nThe eventual goal of the courses listed below is for them to become centralized repositories with scattered pointers to the good stuff: useful blogposts, nuanced reviews, elegant papers, elucidating visualizations, comprehensive books, and intuitive explanations. The kind of stuff you\u0026rsquo;re often only able to appreciate after getting acquainted with a subject first but still wished you\u0026rsquo;d found out about earlier. Feel free to reach out if you have any more recommendations.\n","date":1605225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605225600,"objectID":"c071efc5178f7a3cacafd014364ee181","permalink":"https://mcbal.github.io/learning/generative/","publishdate":"2020-11-13T00:00:00Z","relpermalink":"/learning/generative/","section":"learning","summary":"Content\u0026hellip; So you want to learn something new? Good.\nThe eventual goal of the courses listed below is for them to become centralized repositories with scattered pointers to the good stuff: useful blogposts, nuanced reviews, elegant papers, elucidating visualizations, comprehensive books, and intuitive explanations.","tags":null,"title":"Generative Modelling","type":"book"},{"authors":null,"categories":null,"content":"Twitter A lot of machine researchers and engineers are on Twitter. If you properly curate who you are following, your Twitter feed can become a great overview of all the latest hype.\ndd Personal Blogs  Distill by Google Brain, OpenAI, MIT CSAIL and others inFERENCe by Ferenc Huszár Daniel Daza\u0026rsquo;s blog by Daniel Daza Erik Bernhardsson\u0026rsquo;s blog by Erik Bernhardsson The Shape of Data by Jesse Johnson FastML by Zygmunt Zając Colah\u0026rsquo;s blog by Christopher Olah  On probabilistic programming, deep generative modeling, and Bayesian deep learning:\n Dustin Tran\u0026rsquo;s blog by Dustin Tran While My MCMC Gently Samples by Thomas Wiecki Eigenfoo by George Ho  ","date":1605225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605225600,"objectID":"4bf4d76b06a64567e20df303145e499c","permalink":"https://mcbal.github.io/learning/uptodate/","publishdate":"2020-11-13T00:00:00Z","relpermalink":"/learning/uptodate/","section":"learning","summary":"Twitter A lot of machine researchers and engineers are on Twitter. If you properly curate who you are following, your Twitter feed can become a great overview of all the latest hype.","tags":null,"title":"Keeping up-to-date","type":"book"}]