[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mcbal.github.io/author/matthias-bal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matthias-bal/","section":"authors","summary":"","tags":null,"title":"Matthias Bal","type":"authors"},{"authors":[],"categories":[],"content":"  Introduction Mean-field theory of asymmetric Ising models with binary spins  Setting the scene: the kinetic Ising model Mean-field theory and Kullback-Leibler divergence The Plefka expansion: interpolating distributions Naive mean-field and Thouless-Anderson-Palmer approximations A simple JAX implementation   Mean-field theory of asymmetric Ising models with vector spins  Vector spins: distributions on hyperspheres Magnetizations and limit of large vector dimension First-order naive mean-field approximation Second-order Thouless-Anderson-Palmer approximation A simple JAX implementation   A family of transformer-like modules  Fast- and slow-moving parameters   Conclusion Appendices  Vector-spin distribution: normalization constant Vector-spin distribution: expected value (first moment) Vector-spin distribution: variance (second moment) Ratio of modified Bessel functions of the first kind General case: partial derivatives with respect to $\\alpha$     1. Introduction  ✨ This blog post is a work in progress exploring connections between transformer neural networks and mean-field dynamics of (asymmetric) vector-spin models. Code in JAX and PyTorch of the content outlined in this post should eventually become available at mcbal/spin-model-transformers. Come join the fun.\n In a series of previous blog posts, we have tried to connect the forward pass of a transformer neural-network module to computing mean magnetizations in disordered Ising-like vector-spin models with parameterized couplings and external magnetic fields. In this framework, the forward pass of a transformer module computes statistical observables given a specific realization of quenched couplings and external magnetic fields while the backward pass nudges the parameterized couplings and external magnetic fields. Physically, the transformer module represents an interacting many-body system modulating its behavior by learning to respond to being driven in all kinds of funny ways.\nHowever, both the mean-field message-passing approach of Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms (2021) and the saddle-point free-energy approach of Transformers from Spin Models: Approximate Free Energy Minimization (2021) inherently rely on methods that are only well-defined for spin models with symmetric coupling matrices, whose stochastic dynamics obey detailed balance and converge to a steady-state equilibrium characterized by the Boltzmann distribution. Since the softmax attention matrix in transformer modules is famously asymmetric, we had better come up with a more convincing approach to establish a correspondence.\nTo capture spin models with asymmetric coupling matrices, we turn to non-equilibrium spin systems, whose dynamics can be pretty wild yet gentle enough to support regimes where relaxation to a non-equilibrium steady state can occur. In the past few decades, dynamical mean-field approaches have been developed for the binary kinetic Ising model, which exhibits non-equilibrium behavior when couplings are asymmetric or when parameters are subject to rapid changes. In this post, we generalize one of these approaches from binary spins to vector spins with the aim of relating the resulting mean-field update equations for the magnetizations to the forward pass of a transformer module. We find that the spin-system structure is rich enough for the update equations to yield residual connections, attention terms, and feed-forward terms, motivating a physics-inspired class of transformer modules.\n2. Mean-field theory of asymmetric Ising models with binary spins In this section, we review known results on mean-field theory approaches to capturing the stochastic dynamics of binary kinetic Ising models. Readers familiar with this framework can skip to Section 3 where we develop a generalization to vector spins. We primarily follow the discussion outlined in A unifying framework for mean-field theories of asymmetric kinetic Ising systems (Aguilera et al., 2021). We implement the mean-field update equations for the mean magnetizations in JAX and run a few numerical experiments.\n2.1. Setting the scene: the kinetic Ising model We consider a kinetic Ising model describing a system made up of $N$ interacting binary spins $s_{i,t} \\in \\{-1, 1\\}$ that evolve in discrete time steps $t$ according to synchronous dynamics, i.e. all spins get updated at the same time in parallel. Given a spin configuration $\\mathbf{s}_{t-1} = \\{ s_{1,t-1}, s_{2,t-1}, \\ldots, s_{N,t-1} \\}$ at time $t-1$, we consider the spins $\\mathbf{s}_{t}$ at time $t$ to be conditionally independent random variables captured by a discrete-time Markov chain transition probability\n\\begin{equation} P( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) = \\prod_{i=1}^{N} \\frac{\\mathrm{e}^{s_{i,t} h_{i,t}}}{\\sum_{s_{i,t}} \\mathrm{e}^{s_{i,t} h_{i,t}}} = \\prod_{i=1}^{N} \\frac{\\mathrm{e}^{s_{i,t} h_{i,t}}}{2 \\cosh h_{i,t}}, \\label{eq:pcond} \\end{equation}\nwhere the effective external field is given by\n\\begin{equation} h_{i,t} = x_{i,t} + \\sum_{j=1}^{N} J_{ij} s_{j,t-1}. \\end{equation}\nHere, the parameters $\\mathbf{x}$ represent the (possibly time-dependent) local external fields at each site while the coupling parameters $\\mathbf{J}$ are a specific realization of quenched disorder encoding the interactions between pairs of spins. Using the probability mass function of the previous state $P( \\mathbf{s}_{t-1} )$ we can write the distribution of the current state as\n\\begin{equation} P( \\mathbf{s}_{t} ) = \\sum_{\\mathbf{s}_{t-1}} P( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) P( \\mathbf{s}_{t-1} ), \\label{eq:marginal} \\end{equation}\nwhich, when applied recursively, traces the evolution of the system starting from some initial distribution $P( \\mathbf{s}_{0} )$. Unless we turn off the couplings by setting $\\mathbf{J} = \\mathbf{0}$, the marginal distribution $P( \\mathbf{s}_{t} )$ is not factorized and tends to be quite complicated. Our goal is to compute statistical properties of the system, such as the mean magnetizations\n\\begin{equation} m_{i,t} = \\sum_{\\mathbf{s}_{t}} s_{i,t} P( \\mathbf{s}_{t} ), \\end{equation}\nas well as correlations\n\\begin{equation} C_{ik,t} = \\sum_{\\mathbf{s}_{t}} s_{i,t} s_{k,t} P( \\mathbf{s}_{t} ) - m_{i,t} m_{k,t}, \\end{equation}\nand delayed correlations\n\\begin{equation} D_{il,t} = \\sum_{\\mathbf{s}_{t},\\mathbf{s}_{t-1}} s_{i,t} s_{l,t-1} P( \\mathbf{s}_{t}, \\mathbf{s}_{t-1} ) - m_{i,t} m_{l,t-1}. \\end{equation}\nSince the above expressions involve summing over a large amount of possible spin configurations, they are not very useful in practice. So we will try to approximate the tricky marginal distribution $P( \\mathbf{s}_{t} )$ defined in Eq. \\eqref{eq:marginal} using a mean-field theory approach.\n2.2. Mean-field theory and Kullback-Leibler divergence Mean-field theory tries to approximate a complicated object ${\\color{red}P}$ by wiggling around the parameters of a simple, analytically tractable parameterized ansatz ${\\color{green}Q_{\\theta}}$ to get as close as possible to ${\\color{red}P}$. At risk of inducing headaches in mathematicians by calling everything a manifold, we can picture what is going on geometrically as trying to approximate a target probability distribution $P( \\mathbf{s}_{t} \\vert \\mathbf{x}, \\mathbf{J})$ and its statistical properties $\\mathbf{m}_{t}$, $\\mathbf{C}_{t}$, and $\\mathbf{D}_{t}$ by restricting ourselves to a submanifold of tractable probability distributions. A particularly convenient submanifold is that of factorized models, where each point on the submanifold corresponds to a distribution parameterized by a vector $\\boldsymbol{\\theta}_{t}$,\n\\begin{equation} Q( \\mathbf{s}_{t} \\vert \\boldsymbol{\\theta}_{t} ) = \\prod_{i=1}^{N} \\frac{\\mathrm{e}^{s_{i,t} \\theta_{i,t}}}{2 \\cosh \\theta_{i,t}}, \\label{eq:q} \\end{equation}\nso that the mean magnetizations are simply given by\n\\begin{equation} m_{i,t} = \\tanh \\theta_{i,t} \\label{eq:meanmagstanh} \\end{equation}\nas there are no couplings between spins. The factorized model $Q( \\mathbf{s}_{t} \\vert \\boldsymbol{\\theta}^{*}_{t} )$ that minimizes the Kullback-Leibler (KL) divergence\n\\begin{equation} D_{\\mathrm{KL}} ({\\color{red}P}\\vert\\vert{\\color{green}Q_{\\theta}}) = \\sum_{\\mathbf{s}_{t}} P( \\mathbf{s}_{t}) \\log \\frac{P( \\mathbf{s}_{t})}{Q_{\\theta}( \\mathbf{s}_{t})} \\label{eq:kl} \\end{equation}\nhas mean magnetizations $\\mathbf{m}_{t}$ identical to those of the target distribution $P( \\mathbf{s}_{t})$ since, for all spins $i=1,2,\\ldots,N$, we find that\n\\begin{align} \\frac{\\partial D_{\\mathrm{KL}} ({\\color{red}P}\\vert\\vert{\\color{green}Q_{\\theta}}) }{\\partial \\theta_{i, t}} \\Biggr\\rvert_{\\boldsymbol{\\theta}_{t}=\\boldsymbol{\\theta}^{*}_{t}} \u0026amp;= - \\sum_{\\mathbf{s}_{t}} P( \\mathbf{s}_{t}) \\frac{\\partial \\log Q_{\\theta}( \\mathbf{s}_{t}) }{\\partial \\theta_{i, t}} \\Biggr\\rvert_{\\boldsymbol{\\theta}_{t}=\\boldsymbol{\\theta}^{*}_{t}} \\\\ \u0026amp;= - \\sum_{\\mathbf{s}_{t}} s_{i,t} P( \\mathbf{s}_{t}) + \\tanh \\theta^{*}_{i,t} \\\\ \u0026amp;= -m^{{\\color{red}P}}_{i,t} + m^{{\\color{green}Q_{\\theta^{*}}}}_{i,t} = 0, \\label{eq:klm} \\end{align}\nwhere $m^{{\\color{red}P}}_{i,t}$ and $m^{{\\color{green}Q_{\\theta^{*}}}}_{i,t}$ respectively denote the expectation values of $s_{i,t}$ with respect to ${\\color{red}P}$ and ${\\color{green}Q_{\\theta^{*}}}$. Indeed, minimizing $D_{\\mathrm{KL}} ({\\color{red}P}\\vert\\vert{\\color{green}Q_{\\theta}})$ tries to cover the modes of ${\\color{red}P}$ by moment matching since the expectation value in Eq. \\eqref{eq:kl} is calculated with respect to ${\\color{red}P}$.\n2.3. The Plefka expansion: interpolating distributions Great, but is it even possible to find the parameters\n\\begin{equation} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\boldsymbol{\\theta}^{*}_{t} = \\argmin_{\\boldsymbol{\\theta}_{t}} \\left( - \\sum_{\\mathbf{s}_{t}} P( \\mathbf{s}_{t}) \\log Q_{\\theta}( \\mathbf{s}_{t}) \\right) \\end{equation}\nthat minimize the KL divergence? Well, that\u0026rsquo;s going to be hard, unless you already know the target distribution $P( \\mathbf{s}_{t})$, or you have a clever way of approximately evaluating the expectation value of $\\log {\\color{green}Q_{\\theta}}$ with respect to ${\\color{red}P}$. So let\u0026rsquo;s introduce some more distributions to get around this issue. To apply the Plefka expansion to our problem, we introduce the conditional distribution\n\\begin{equation} P_{\\alpha}( \\mathbf{s}_{t}\\vert \\mathbf{s}_{t-1} ) = \\prod_{i=1}^{N} \\frac{\\mathrm{e}^{s_{i,t} h_{i,t}(\\alpha) }}{2 \\cosh h_{i,t}(\\alpha)}, \\label{eq:pcondalt} \\end{equation}\n\\begin{equation} h_{i,t}(\\alpha) = (1-\\alpha) \\theta_{i,t} + \\alpha \\left( x_{i,t} + \\sum_{j=1}^{N} J_{ij} s_{j,t-1} \\right), \\label{eq:pcondalth} \\end{equation}\nparameterized by a scalar $\\alpha$ interpolating between $P_{\\alpha=0}( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) = Q( \\mathbf{s}_{t} \\vert \\boldsymbol{\\theta}_{t} )$ (Eq. \\eqref{eq:q}) and $P_{\\alpha=1}( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) = P( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} )$ (Eq. \\eqref{eq:pcond}). Using Eq. \\eqref{eq:pcondalt}, we can construct an approximate marginal distribution $P_{\\alpha}( \\mathbf{s}_{t})$, leading to $\\alpha$-dependent statistical properties $\\mathbf{m}_{t}(\\alpha)$, $\\mathbf{C}_{t}(\\alpha)$, and $\\mathbf{D}_{t}(\\alpha)$ for the approximate system. The Plefka expansion then boils down to writing these properties as Taylor series expansions around the factorized model $\\alpha=0$. For the mean magnetizations, the expansion up to $n$-th order looks like\n\\begin{equation} \\mathbf{m}_{t}(\\alpha) = \\mathbf{m}_{t}(\\alpha=0) + \\sum_{k=1}^{n} \\frac{\\alpha^k}{k!} \\frac{\\partial^{k} \\mathbf{m}_{t}(\\alpha=0)}{\\partial \\alpha^{k}} + \\mathcal{O}(\\alpha^{n+1}), \\label{eq:mtaylor} \\end{equation}\nwhere all coefficients in the expansion are functions of $\\boldsymbol{\\theta}_{t}$ via Eq. \\eqref{eq:pcondalth}. The mean-field approximation is computed by setting $\\alpha=1$ so that the original marginal distribution is recovered and Eq. \\eqref{eq:klm} holds, which implies that $\\mathbf{m}_{t}(\\alpha=1) = \\mathbf{m}_{t}(\\alpha=0)$ and thus\n\\begin{equation} \\sum_{k=1}^{n} \\frac{1}{k!} \\frac{\\partial^{k} \\mathbf{m}_{t}(\\alpha=0)}{\\partial \\alpha^{k}} + \\mathcal{O}(\\alpha^{n+1}) = 0. \\label{eq:mftheta} \\end{equation}\nFinally, we solve Eq. \\eqref{eq:mftheta} for $\\boldsymbol{\\theta}_{t}$ to find the mean-field values $\\boldsymbol{\\theta}^{*}_{t}$ of the parameters of the distribution Eq. \\eqref{eq:q}. Physically, we are tuning the effective external magnetic fields of the factorized ansatz to $\\boldsymbol{\\theta}^{*}_{t}$ so that its approximate mean magnetizations get as close as possible to the true ones.\n2.4. Naive mean-field and Thouless-Anderson-Palmer approximations We now consider first and second order approximations of the mean magnetizations Eq. \\eqref{eq:mtaylor} to recover respectively the naive mean-field and Thouless-Anderson-Palmer (TAP) approximations for the binary kinetic Ising model. The starting point is a Plefka expansion around factorized models at times $t-1$ and $t$. From Eq. \\eqref{eq:marginal} and Eq. \\eqref{eq:pcondalt}, we construct a marginal probability distribution\n\\begin{equation} P^{[t-1:t]}_{\\alpha}( \\mathbf{s}_{t} ) = \\sum_{\\mathbf{s}_{t-1},\\mathbf{s}_{t-2}} P_{\\alpha}( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) P( \\mathbf{s}_{t-2} ), \\end{equation}\ninterpolating between $P^{[t-1:t]}_{\\alpha=0}( \\mathbf{s}_{t} ) = Q( \\mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\\alpha=1}( \\mathbf{s}_{t} ) = P( \\mathbf{s}_{t} )$. The corresponding mean magnetizations are\n\\begin{align} m_{i,t}(\\alpha) \u0026amp;= \\sum_{\\mathbf{s}_{t},\\mathbf{s}_{t-1},\\mathbf{s}_{t-2}} s_{i,t} \\, P_{\\alpha}( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) P( \\mathbf{s}_{t-2} ) \\\\ \u0026amp;= \\sum_{\\mathbf{s}_{t-1},\\mathbf{s}_{t-2}} \\tanh h_{i,t}(\\alpha) \\, P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) P( \\mathbf{s}_{t-2} ) \\end{align}\nFollowing Eq. \\eqref{eq:mftheta}, the first-order approximation should satisfy\n\\begin{equation} \\frac{\\partial m_{i,t}(\\alpha=0)}{\\partial\\alpha} = \\left( 1-m^{2}_{i,t} \\right) \\left( -\\theta_{i,t} + x_{i,t} + \\sum_{j} J_{ij} m_{j,t-1} \\right) = 0, \\end{equation}\nso that $\\theta^{*}_{i,t} = x_{i,t} + \\sum_{j} J_{ij} m_{j,t-1}$ and we end up with the naive mean-field equations:\n\\begin{equation} \\boxed{m_{i,t} = \\tanh \\left( x_{i,t} + \\sum_{j} J_{ij} m_{j,t-1} \\right)} \\label{eq:naivem} \\end{equation}\nAgain following Eq. \\eqref{eq:mftheta}, the second-order approximation should satisfy\n\\begin{equation} \\frac{\\partial m_{i,t}(\\alpha=0)}{\\partial\\alpha} + \\frac{1}{2} \\frac{\\partial^{2} m_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha} = 0, \\end{equation}\nwhere the second-order derivative, neglecting terms higher than $\\mathcal{O}(\\alpha^2)$, is\n\\begin{equation} \\frac{\\partial^{2} m_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha} \\approx -2 m_{i,t} \\left( 1-m^{2}_{i,t} \\right) \\sum_{j} J^{2}_{ij} \\left( 1-m^{2}_{j,t-1} \\right) \\end{equation}\nso that\n\\begin{equation} \\theta^{*}_{i,t} = x_{i,t} + \\sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \\sum_{j} J^{2}_{ij} \\left( 1-m^{2}_{j,t-1} \\right) \\end{equation}\nand we end up with the TAP mean-field equations:\n\\begin{equation} \\boxed{m_{i,t} = \\tanh \\left( x_{i,t} + \\sum_{j} J_{ij} m_{j,t-1} - m_{i,t} \\sum_{j} J^{2}_{ij} \\left( 1-m^{2}_{j,t-1} \\right) \\right)} \\label{eq:tapm} \\end{equation}\nThe mean-field equations obtained above can also be elegantly derived using a Legendre transformation of the generating functional of the set of trajectories of the model, as outlined in e.g. Dynamical TAP equations for non-equilibrium Ising spin glasses (2011). We can also derive second-order TAP approximations of the correlations\n\\begin{equation} C_{ik,t} = \\begin{cases} 1 - m^{2}_{i,t} \u0026amp; i = k \\\\ \\left( 1-m^{2}_{i,t} \\right) \\left( 1-m^{2}_{k,t} \\right) \\sum_{j} J_{ij} J_{kj} \\left( 1-m^{2}_{j,t-1} \\right) \u0026amp; i \\neq k \\label{eq:tapc} \\end{cases} \\end{equation}\nand delayed correlations\n\\begin{equation} D_{il,t} = J_{il} \\left( 1-m^{2}_{i,t} \\right) \\left( 1-m^{2}_{l,t-1} \\right) \\left( 1 + 2 J_{il} m_{i,t} m_{l,t-1} \\right). \\label{eq:tapd} \\end{equation}\nWe refer to (Aguilera et al., 2020) for full derivations of the above mean-field results as well as variations based on different approximations of the marginal distribution $P( \\mathbf{s}_{t} )$.\nIn summary, given the mean magnetizations $\\mathbf{m}_{t-1}$ of the system at time $t-1$, we can use equations \\eqref{eq:tapm} \\eqref{eq:tapc} \\eqref{eq:tapd} to compute a tuple $(\\mathbf{m}_{t},\\mathbf{C}_{t},\\mathbf{D}_{t})$ of approximate statistical properties of the system at time $t$. The time evolution of the system can thus be captured at the mean-field level by recursively computing $\\mathbf{m}_{t}$ starting from an initial state $\\mathbf{m}_{0}$, although approximation errors accumulate over the iterations.\n2.5. A simple JAX implementation To get more insight into what is going on, let us turn the mean-field update equations \\eqref{eq:naivem} and \\eqref{eq:tapm} for the mean magnetizations into code. But before we show a few plots, we need to know a bit more background about the model we are about to simulate. In (Aguilera et al., 2020), the authors derive a solution of the asymmetric version of the kinetic Sherrington-Kirkpatrick mean-field spin-glass model using a generating functional or dynamical partition function approach to capture the distribution of trajectories. They consider the same kinetic Ising model as in Eq. \\eqref{eq:pcond} but with an inverse temperature parameter $\\beta$ in the exponentials:\n\\begin{equation} P( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) = \\prod_{i=1}^{N} \\frac{\\mathrm{e}^{\\beta s_{i,t} h_{i,t}}}{2 \\cosh \\beta h_{i,t}}. \\label{eq:pcondwithbeta} \\end{equation}\nFor Gaussian couplings $J_{ij} \\sim \\mathcal{N}\\left( J_{\\mu} / N, J^{2}_{\\sigma} / N\\right)$ and uniformly distributed external magnetic fields $x_{i} \\sim \\mathcal{U}(-X_{0}, X_{0})$, they show the existence of a ferromagnetic phase transition. In particular for $X_{0}=0.5$, $J_{\\mu}=1.0$, and $J_{\\sigma}=0.1$, a phase transition happens when tuning $\\beta$ to a critical value $\\beta_{c} \\approx 1.1108$.\nSimulating magnetization trajectories We now turn to a JAX implementation of the mean-field time evolution of the magnetizations according to the model described above. We use lax.scan to implement the update steps and vmap to parallelize trajectories starting from a batch of initial magnetization configurations $\\mathbf{m}_{0}$. For the second-order TAP equations, jaxopt\u0026rsquo;s Anderson acceleration is used to find the fixed point magnetizations $\\mathbf{m}_{t}$ given $\\mathbf{m}_{t-1}$.\nfrom functools import partial import jax import jax.numpy as jnp from jaxopt import AndersonAcceleration def update_naive_mf(m0, _, x, J): m1 = jnp.tanh(x + jnp.einsum(\u0026quot;... i j, ... j -\u0026gt; ... i\u0026quot;, J, m0)) return m1, m0 def update_tap_mf(m0, _, x, J): def tap(m): return jnp.tanh( x + jnp.einsum(\u0026quot;... i j, ... j -\u0026gt; ... i\u0026quot;, J, m0) - m * jnp.einsum(\u0026quot;... i j, ... j -\u0026gt; ... i\u0026quot;, J**2, (1.0 - m0**2)) ) m1 = AndersonAcceleration(fixed_point_fun=tap, tol=1e-3, maxiter=10).run(m0).params return m1, m0 def time_evolution(m0, steps, update_fun): final_carry, stacked_outputs = jax.lax.scan(update_fun, init=m0, xs=steps) return final_carry, stacked_outputs def init_params(key, N, beta, X0, J_mu, J_sigma): x_key, J_key = jax.random.split(key) x = jax.random.uniform(x_key, shape=(N,), minval=-beta * X0, maxval=beta * X0) J = beta * J_mu * N**-1 + beta * J_sigma * N**-0.5 * jax.random.normal( J_key, shape=(N, N) ) return x, J def simulate( key, m0, steps, beta, X0=0.5, J_mu=1.0, J_sigma=0.1, update_fun=update_tap_mf ): x, J = init_params(key, m0.shape[-1], beta, X0, J_mu, J_sigma) wrapped_time_evolution = partial( time_evolution, steps=steps, update_fun=partial(update_fun, x=x, J=J), ) final_carry, stacked_outputs = jax.vmap(wrapped_time_evolution)(m0) return final_carry, stacked_outputs  Naive mean-field vs. Thouless-Anderson-Palmer (TAP) We fix the seed for the randomly initialized model parameters $\\mathbf{x}$ and $\\mathbf{J}$ and simulate $N=512$ spins at the critical temperature $\\beta_{c}$ for $t=128$ time steps starting from an all-ones intial state. We first consider the naive mean-field update step.\nThe left axis shows the individual magnetization trajectories for each spin plotted horizontally while the red line associated to the right axis describes the average of the magnetizations across all spins for each time step. We observe convergence to what looks like a steady state.\nComparing the naive first-order mean-field update equations to the second-order Thouless-Anderson-Palmer (TAP) ones, we observe lower values for the mean magnetization across all spins, which (Aguilera et al., 2020) showed to be closer to ground truth values (not shown) obtained via sampling and averaging spin configurations.\nSampling trajectories Let\u0026rsquo;s now consider 100 randomly-initialized initial states and simulate their associated trajectories in three different model regimes: far below the critical point ($\\beta=\\beta_c / 2 $), at the critical point ($\\beta=\\beta_c$), and far above the critical point ($\\beta=2 \\beta_c$).\nWe observe that the trajectories of randomly-initialized initial states converge to identical final states in each regime. These final states map to a simple ferromagnetic Ising phase diagram, where a high-temperature disordered phase $\\langle m_{i,t} \\rangle \\to 0$ (left) is separated from a low-temperature locally-ordered phase $\\langle m_{i,t} \\rangle \\to \\pm 1$ (right) by a critical point (center). The behavior around $\\beta=\\beta_{c}$ is pretty interesting: the non-trivial non-equilibrium steady state looks like an attractor implicitly encoded in the dynamics of the model. If we were to parametrize the couplings, we could train the model to act as an associative memory.\nSampling model parameters Let us now go back to considering just a single trajectory since we just saw that trajectories seem to converge to the same final steady-state magnetizations for fixed model parameters. To get a feel for the variation of these values across different realizations of model parameters, we plot the absolute value1 $| \\langle m_{i} \\rangle |$ of the final steady-state magnetizations across 100 samples of model parameters and a range of inverse temperatures. We are using JAX, so we can easily sample model parameters by vmap\u0026lsquo;ing the random key fed into the simulate function followed by another vmap to sweep across $\\beta$.\nEvery curve in the above plot describes the final steady-state value of the \u0026ldquo;order parameter\u0026rdquo; $| \\langle m_{i} \\rangle |$ for a fixed set of model parameters sweeping across $\\beta$. We observe a greater spread of values near the critical point and hence an improved capacity to map input external fields to a range of output magnetizations. If we were to let the number of spins $N \\to \\infty$ and average over a large number of model parameter samples, the finite-size results above would probably transform into a sharp curve with zero magnetization below the critical point and a sudden non-zero magnetization emerging at the critical point.\n3. Mean-field theory of asymmetric Ising models with vector spins We now transpose the binary-spin results of the previous section to a setting where local spin degrees of freedom are $D$-dimensional vector spins restricted to wiggle around on $(D-1)$-dimensional spheres. We start by generalizing the conditional distribution Eq. \\eqref{eq:pcondalt} to vector spins. Next, we motivate the limit of large vector dimension and derive first-order and second-order mean-field update equations for the mean magnetizations. We finish this section with a JAX implementation and some numerical experiments.\n3.1. Vector spins: distributions on hyperspheres A vector-spin equivalent of Eq. \\eqref{eq:pcondalt} looks like\n\\begin{equation} P_{\\alpha}( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) = \\prod_{i=1}^{N} \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s}_{i,t} \\cdot \\mathbf{h}_{i,t}(\\alpha)}}{\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s}_{i,t} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s}_{i,t} \\cdot \\mathbf{h}_{i,t}(\\alpha)} }, \\label{eq:pcondaltvector} \\end{equation}\nwhere we immediately included an inverse temperature scaling $\\beta$ like in Eq. \\eqref{eq:pcondwithbeta}. A vector-spin equivalent of Eq. \\eqref{eq:pcondalth} is\n\\begin{equation} \\mathbf{h}_{i,t}(\\alpha) = (1-\\alpha) \\boldsymbol{\\theta}_{i,t} + \\alpha \\left( \\mathbf{x}_{i,t} + \\sum_{j=1}^{N} J_{ij} \\mathbf{s}_{j,t-1} \\right) \\equiv \\boldsymbol{\\theta}_{i,t} + \\alpha \\Delta \\mathbf{h}_{i,t}, \\label{eq:pcondalthvector} \\end{equation}\nwhere $S_{D-1}(R) = \\{ x \\in \\mathbb{R}^{D} : \\lVert x \\rVert = R \\}$ denotes the $(D-1)$-dimensional sphere with radius $R$ embedded in $D$ dimensions. Let us focus on the distribution for a single site and drop all subscripts and dependencies for clarity:\n\\begin{equation} p ( \\mathbf{s} ; \\beta, \\mathbf{h}) = \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}}}{\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} }. \\label{eq:pcondsinglesitevector} \\end{equation}\nThe normalization constant in the denominator can be shown to be (see Appendix A.1)\n\\begin{equation} \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} = \\frac{ \\left( 2 \\pi R \\right)^{D/2} I_{D/2 - 1}(\\beta R \\lVert \\mathbf{h}\\rVert) }{ \\left(\\beta \\lVert \\mathbf{h}\\rVert\\right)^{D/2-1} } \\equiv Z(\\beta, R, \\lVert \\mathbf{h}\\rVert) \\label{eq:partfun} \\end{equation}\nwhere $I_{\\nu}(z)$ denotes the modified Bessel function of the first kind and $\\lVert \\mathbf{h}\\rVert = \\sqrt{\\mathbf{h} \\cdot \\mathbf{h}}$. Physically, we can think of this single-site distribution as measuring dot-product alignment to an effective external magnetic field $\\mathbf{h}$ at inverse temperature $\\beta$.\nIf we consider spins living on the unit sphere $R=1$ as well as unit vectors $\\mathbf{h}$, the distribution boils down to a von Mises–Fisher distribution with mean direction $\\boldsymbol{\\mu} \\equiv \\mathbf{h}$ and concentration parameter $\\kappa \\equiv \\beta$. This distribution is unimodal for $\\kappa \u0026gt; 0$ and can be derived from restricting an isotropic multivariate Gaussian to the unit hypersphere. The greater the value of $\\kappa$ (the inverse temperature $\\beta$), the higher the concentration of the distribution around the mean direction $\\boldsymbol{\\mu}$ (the more the spin tends to align to the effective external field $\\mathbf{h}$). Instead of a fixed parameter $\\boldsymbol{\\mu}$, we have a very funky parameter Eq. \\eqref{eq:pcondalthvector} that depends on all other spins to spice things up.\n3.2. Magnetizations and limit of large vector dimension Before we derive mean-field approximations for the mean magnetizations of our vector-spin system, let us first consider the decoupled $\\alpha \\to 0$ limit of the distribution Eq. \\eqref{eq:pcondaltvector},\n\\begin{equation} Q( \\mathbf{s}_{t} \\vert \\boldsymbol{\\theta}_{t} ) = \\prod_{i=1}^{N} \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s}_{i,t} \\cdot \\boldsymbol{\\theta}_{i,t}}}{Z_{i,t}\\left(\\beta, R, \\lVert \\boldsymbol{\\theta}_{i,t} \\rVert\\right)}, \\end{equation}\nand find an expression for its mean magnetizations. For every decoupled site, the mean magnetization can be shown to be (see Appendix A.2)\n\\begin{equation} \\mathbf{m}_{i,t} = \\frac{I_{D/2}(\\beta R \\lVert \\boldsymbol{\\theta}_{i,t} \\rVert)}{I_{D/2 - 1}(\\beta R \\lVert \\boldsymbol{\\theta}_{i,t} \\rVert)} \\frac{R \\boldsymbol{\\theta}_{i,t}}{\\lVert \\boldsymbol{\\theta}_{i,t} \\rVert} \\equiv \\boldsymbol{\\varphi} \\left(\\boldsymbol{\\theta}_{i,t}\\right), \\label{eq:meanmagsbessels} \\end{equation}\nwhich plays the role of $m_{i,t} = \\tanh \\theta_{i,t}$ in the binary setting, see Eq. \\eqref{eq:meanmagstanh}. Looking ahead at turning the above equation into code, we note that there exist efficient algorithms to compute the ratio of modified Bessel functions of the first kind. We implement a fast JAX version in Appendix A.4 and show numerically how the ratio flattens out quickly for large values of the order $\\nu = D/2 -1$, motivating some kind of large-order expansion.\nAs the vector dimension in practical transformer modules tends be somewhere between $\\mathcal{O}(10^2)$ and $\\mathcal{O}(10^5)$, it makes sense to focus on the limit of large vector dimension. A relevant uniform asymptotic expansion of the ratio of modified Bessel functions of the first kind can be found in (Kiefer \u0026amp; Weiss, 1972):\n\\begin{align} \\frac{I_{\\nu+\\alpha}(\\nu x)}{I_{\\nu}(\\nu x)} = \\left( \\frac{x}{1+\\sqrt{1+x^2}} \\right)^{\\alpha} \\left( 1 - \\frac{1+\\alpha\\sqrt{1+x^2}}{2(1+x^2)} \\frac{\\alpha}{\\nu} + \\mathcal{O}\\left( \\frac{1}{\\nu^2} \\right) \\right) \\end{align}\nIndeed, if we choose to tie the radius $R$ of our little spins to their vector dimension $D$ via\n\\begin{align} \\nu=D/2-1=R^2, \\end{align}\nwe can apply the leading order of the asymptotic expansion to \\eqref{eq:meanmagsbessels} to find\n\\begin{equation} \\mathbf{m}^{D \\to \\infty}_{i,t} \\approx \\frac{\\beta}{1+\\sqrt{1+\\beta^2 \\lVert \\boldsymbol{\\theta}_{i,t} \\rVert^2 / R^2 }} \\boldsymbol{\\theta}_{i,t} \\equiv \\boldsymbol{\\varphi}^{D \\to \\infty} \\left(\\boldsymbol{\\theta}_{i,t}\\right). \\label{eq:largedevmag} \\end{equation}\nFrom here on, we will default to using the large-$D$ approximation because keeping track of (derivatives of) Bessel functions gets boring real quick. We refer to Appendix A.5 for some truly outrageous expressions pertaining to the general case valid for all $D\u0026gt;1$.\n3.3. First-order naive mean-field approximation Closely mimicking the binary case, we start from the following approximated marginal probability distribution\n\\begin{equation} P^{[t-1:t]}_{\\alpha}( \\mathbf{s}_{t} ) = \\int \\mathrm{d} \\mathbf{s}_{t-1} \\int \\mathrm{d} \\mathbf{s}_{t-2} \\; P_{\\alpha}( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) P( \\mathbf{s}_{t-2} ), \\end{equation}\ninterpolating between $P^{[t-1:t]}_{\\alpha=0}( \\mathbf{s}_{t} ) = Q( \\mathbf{s}_{t} )$ and $P^{[t-1:t]}_{\\alpha=1}( \\mathbf{s}_{t} ) = P( \\mathbf{s}_{t} )$. Our lazy integral notation $\\int \\mathrm{d} \\mathbf{s}_{t}$ should be understood as $\\int \\prod_{i=1}^{N} \\mathrm{d}^{D} \\mathbf{s}_{i, t}$, i.e. integrating over all the little spins at a fixed time $t$. The estimated mean magnetizations are\n\\begin{align} \\mathbf{m}_{i,t}(\\alpha) \u0026amp;= \\int \\mathrm{d} \\mathbf{s}_{t} \\int \\mathrm{d} \\mathbf{s}_{t-1} \\int \\mathrm{d} \\mathbf{s}_{t-2} \\; \\mathbf{s}_{i,t} P_{\\alpha}( \\mathbf{s}_{t} \\vert \\mathbf{s}_{t-1} ) P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) P( \\mathbf{s}_{t-2} ) \\nonumber\\\\ \u0026amp;= \\int \\mathrm{d} \\mathbf{s}_{t-1} \\int \\mathrm{d} \\mathbf{s}_{t-2} \\; \\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right) \\, P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) P( \\mathbf{s}_{t-2} ). \\end{align}\nThe first-order derivative with respect to $\\alpha$ is given by\n\\begin{align} \\frac{\\partial \\mathbf{m}_{i,t}(\\alpha)}{\\partial\\alpha} = \\int \u0026amp;\\mathrm{d} \\mathbf{s}_{t-1} \\int \\mathrm{d} \\mathbf{s}_{t-2} \\Biggl( \\frac{\\partial\\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right)}{\\partial\\alpha} \\, P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) \\nonumber\\\\ \u0026amp;+ \\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right) \\, \\frac{\\partial P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} )}{\\partial\\alpha} \\Biggr) P( \\mathbf{s}_{t-2} ), \\label{eq:mitfirstorderalpha} \\end{align}\nwhere\n\\begin{align} \\frac{\\partial \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha))}{\\partial\\alpha} = - \u0026amp; \\frac{\\beta}{R^2} \\frac{ \\left( \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\cdot \\Delta \\mathbf{h}_{i,t} \\right) }{ \\sqrt{1+\\beta^2 \\lVert \\mathbf{h}_{i,t}(\\alpha) \\rVert^2 / R^2 } } \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\nonumber \\\\ \u0026amp;+ \\frac{\\beta}{1+\\sqrt{1+\\beta^2 \\lVert \\mathbf{h}_{i,t}(\\alpha) \\rVert^2 / R^2 }} \\Delta \\mathbf{h}_{i,t} \\label{eq:firstorderphialpha} \\end{align}\nEvaluating \\eqref{eq:mitfirstorderalpha} at $\\alpha=0$, the second term drops out because the first-order derivative of $P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} )$ becomes independent of $\\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right)$ and $\\int \\mathrm{d} \\mathbf{s}_{t-1} P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} )=1$. We thus end up with\n\\begin{align} \\frac{\\partial \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial\\alpha} = - \u0026amp;\\frac{\\beta}{R^2}\\frac{\\left( \\mathbf{m}_{i,t} \\cdot \\boldsymbol{v}_{i,t} \\right)}{\\sqrt{1+\\beta^2 \\lVert \\boldsymbol{\\theta}_{i,t} \\rVert^2 / R^2 }} \\mathbf{m}_{i,t} \\nonumber \\\\ \u0026amp;+ \\frac{\\beta}{1+\\sqrt{1+\\beta^2 \\lVert \\boldsymbol{\\theta}_{i,t} \\rVert^2 / R^2 }}\\boldsymbol{v}_{i,t} \\label{eq:mfirstorderalphazero} \\end{align}\nwhere\n\\begin{align} \\boldsymbol{v}_{i,t} = -\\boldsymbol{\\theta}_{i,t} + \\mathbf{x}_{i,t} + \\sum_{j=1}^{N} J_{ij} \\mathbf{m}_{j,t-1} \\label{eq:vmf} \\end{align}\ncaptures the result of integrating $\\Delta \\mathbf{h}_{i,t}$ over the spins $\\mathbf{s}_{t-1}$. Following Eq. \\eqref{eq:mftheta}, the first-order approximation should satisfy\n\\begin{equation} \\left[ \\alpha \\frac{\\partial \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial\\alpha} \\right]_{\\alpha=1} = \\mathbf{0} + \\left[ \\mathcal{O}\\left(\\alpha^2\\right)\\right]_{\\alpha=1},\\label{eq:firstorderapproxreqs} \\end{equation}\nso that we are encouraged to set $\\boldsymbol{v}_{i,t}=0$ and hence $\\boldsymbol{\\theta}^{*}_{i,t} = \\mathbf{x}_{i,t} + \\sum_{j} J_{ij} \\mathbf{m}_{j,t-1}$, leading to the naive mean-field equations:\n\\begin{equation} \\boxed{ \\mathbf{m}_{i,t} = \\frac{\\beta \\left( \\mathbf{x}_{i,t} + \\sum_{j} J_{ij} \\mathbf{m}_{j,t-1} \\right)}{1+\\sqrt{1+\\beta^2 \\lVert \\mathbf{x}_{i,t} + \\sum_{j} J_{ij} \\mathbf{m}_{j,t-1} \\rVert^2 / R^2 }} } \\label{eq:naivemvector} \\end{equation}\nLooking ahead at the transformer-module correspondence in Section 4, we squint our eyes and recognize a scaled sum of a residual connection and an attention term. No feed-forward terms though.\nBefore moving on to the second-order approximation, let us end this section with an interesting observation about Eq. \\eqref{eq:mfirstorderalphazero}. In Appendix A.3, we show that the variance matrix of a single spin in the large-$D$ limit equals a rank-1 perturbation of a diagonal matrix\n\\begin{align} \\mathrm{Var} [ \\mathbf{s}_{i,t} ] \u0026amp;= \\frac{\\mathbb{1}}{1+\\gamma(\\mathbf{h}_{i,t})} - \\frac{ \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\otimes \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) }{ R^2 \\gamma(\\mathbf{h}_{i,t}) }, \\label{eq:spinvariance} \\end{align}\nwhere\n\\begin{align} \\gamma(\\mathbf{h}_{i,t}) = \\sqrt{1+\\beta^{2}\\lVert\\mathbf{h}_{i,t}\\rVert^{2}/R^2}, \\end{align}\nTaking the $\\alpha \\to 0$ limit of the above expressions, we can reinterpret Eq. \\eqref{eq:mfirstorderalphazero} as the matrix-vector multiplication of the decoupled spin\u0026rsquo;s variance matrix with $\\boldsymbol{v}_{i,t}$,\n\\begin{align} \\frac{\\partial \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial\\alpha} = \\beta \\mathrm{Var} [ \\mathbf{s}_{i,t} ] \\boldsymbol{v}_{i,t}. \\end{align}\nLet us now try to find out whether going to the second-order approximation generates additional feed-forward like terms in the update equations for the magnetizations.\n3.4. Second-order Thouless-Anderson-Palmer approximation Again following Eq. \\eqref{eq:mftheta}, the second-order approximation should satisfy\n\\begin{equation} \\left[ \\alpha \\frac{\\partial \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial\\alpha} \\right]_{\\alpha=1} + \\left[ \\frac{\\alpha^2}{2} \\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha}\\right]_{\\alpha=1} = \\mathbf{0} + \\left[ \\mathcal{O}\\left(\\alpha^3\\right)\\right]_{\\alpha=1}, \\label{eq:secondorderconstraint} \\end{equation}\nwhere the second-order derivative is given by\n\\begin{align} \\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha)}{\\partial^{2}\\alpha} = \\int \u0026amp;\\mathrm{d} \\mathbf{s}_{t-1} \\int \\mathrm{d} \\mathbf{s}_{t-2} \\Biggl( \\frac{\\partial^{2}\\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right)}{\\partial^{2}\\alpha} \\, P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} ) \\nonumber\\\\ \u0026amp;+ 2\\frac{\\partial\\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right)}{\\partial\\alpha} \\, \\frac{\\partial P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} )}{\\partial\\alpha} \\nonumber \\\\ \u0026amp;+ \\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right) \\, \\frac{\\partial^{2} P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} )}{\\partial^{2}\\alpha} \\Biggr) P( \\mathbf{s}_{t-2} ). \\label{eq:mhasecordder} \\end{align}\nEvaluated at $\\alpha=0$, the third term in the expression above will drop out because the derivative becomes independent of $\\boldsymbol{\\varphi} \\left(\\mathbf{h}_{i,t}(\\alpha)\\right)$ and $\\int \\mathrm{d} \\mathbf{s}_{t-1} P_{\\alpha}( \\mathbf{s}_{t-1} \\vert \\mathbf{s}_{t-2} )=1$. Letting\n\\begin{align} \\gamma (\\alpha) \\equiv \\gamma(\\mathbf{h}_{i,t}(\\alpha)) = \\sqrt{1+\\beta^2 \\lVert \\mathbf{h}_{i,t}(\\alpha) \\rVert^2 / R^2 }, \\end{align}\nthe first term in Eq. \\eqref{eq:mhasecordder} can be shown to look something like\n\\begin{align} \\frac{\\partial^2 \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha))}{\\partial\\alpha^2} = \u0026amp; \\frac{\\beta^2}{R^4} \\frac{ 1+\\gamma(\\alpha) }{ \\gamma(\\alpha)^3 } \\left( \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\cdot \\Delta \\mathbf{h}_{i,t} \\right)^2 \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\nonumber \\\\ \u0026amp;- \\frac{\\beta}{R^2} \\frac{1}{\\gamma(\\alpha)} \\left( \\frac{\\partial\\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha))}{\\partial\\alpha} \\cdot \\Delta \\mathbf{h}_{i,t} \\right) \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\nonumber \\\\ \u0026amp;- \\frac{\\beta}{R^2} \\frac{1}{\\gamma(\\alpha)} \\left( \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\cdot \\Delta \\mathbf{h}_{i,t} \\right) \\frac{\\partial\\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha))}{\\partial\\alpha} \\nonumber \\\\ \u0026amp;- \\frac{\\beta^2}{R^2} \\frac{1}{\\gamma(\\alpha)^2 + \\gamma(\\alpha) } \\left( \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\cdot \\Delta \\mathbf{h}_{i,t} \\right) \\Delta \\mathbf{h}_{i,t}, \\end{align}\nwhich, after substituting the first-order derivative Eq. \\eqref{eq:firstorderphialpha}, simplifies to\n\\begin{align} \\frac{\\partial^2 \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha))}{\\partial\\alpha^2} = \u0026amp; \\frac{\\beta^2}{R^4} \\frac{ 1+3\\gamma(\\alpha) }{ \\gamma(\\alpha)^3 } \\left( \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\cdot \\Delta \\mathbf{h}_{i,t} \\right)^2 \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\nonumber \\\\ \u0026amp;- \\frac{\\beta^2}{R^2} \\frac{1}{\\gamma(\\alpha)^2 + \\gamma(\\alpha)} \\left( \\Delta \\mathbf{h}_{i,t} \\cdot \\Delta \\mathbf{h}_{i,t} \\right) \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\nonumber \\\\ \u0026amp;- \\frac{\\beta^2}{R^2} \\frac{2}{\\gamma(\\alpha)^2 + \\gamma(\\alpha)} \\left( \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\cdot \\Delta \\mathbf{h}_{i,t} \\right) \\Delta \\mathbf{h}_{i,t} . \\label{eq:secondorderphialpha} \\end{align}\nThe second term in Eq. \\eqref{eq:mhasecordder} contains non-vanishing contributions in the $\\alpha \\to 0$ limit coming from the $\\sum_{j=1}^{N} J_{ij} \\mathbf{s}_{j, t-1}$ terms in $\\Delta \\mathbf{h}_{i,t}$. The surviving terms in the integrand are proportional to\n\\begin{align} \\sum_{j} J_{ij} \\Biggl( \u0026amp;\\frac{2 \\beta^2}{1+\\gamma(\\alpha)} \\frac{\\partial\\mathbf{m}_{j, t-1}(\\alpha)}{\\partial\\alpha} \\nonumber \\\\ \u0026amp;- \\frac{2 \\beta^2}{R^2 \\gamma(\\alpha)} \\left( \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\cdot \\frac{\\partial\\mathbf{m}_{j, t-1}(\\alpha)}{\\partial\\alpha} \\right) \\boldsymbol{\\varphi}(\\mathbf{h}_{i,t}(\\alpha)) \\Biggr), \\end{align}\nwhich we can ignore since they are $\\mathcal{O}(\\alpha)$ on their own, and thus $\\mathcal{O}(\\alpha^3)$ when multiplied with $\\alpha^2$ in the second-order approximation.\nBefore taking the $\\alpha \\to 0$ limit of whatever is left in Eq. \\eqref{eq:mhasecordder}, we list a few useful tricks to make the evaluation easier. First of all, we use Eq. \\eqref{eq:vmf} to introduce the following sneaky substitution\n\\begin{align} \\Delta \\mathbf{h}_{i,t} = -\\boldsymbol{\\theta}_{i,t} + \\mathbf{x}_{i,t} + \\sum_{j=1}^{N} J_{ij} \\mathbf{s}_{j,t-1} = \\boldsymbol{v}_{i,t} + \\sum_{j=1}^{N} J_{ij} \\left( \\mathbf{s}_{j,t-1} - \\mathbf{m}_{j,t-1} \\right), \\end{align}\nwhich conveniently separates terms with fluctuating spin variables from magnetizations that can be pulled out of the integrals. Secondly, all terms that contain only one spin variable with a dependence looking like $\\mathbf{s}_{j,t-1} - \\mathbf{m}_{j,t-1}$ drop out because, schematically,\n\\begin{align} \\mathbf{s}_{j,t-1} - \\mathbf{m}_{j,t-1} \\overset{\\int \\mathrm{d} \\mathbf{s}_{t-1}}{\\to} \\boldsymbol{\\varphi}(\\mathbf{h}_{j,t}(\\alpha)) - \\mathbf{m}_{j,t-1} \\overset{\\alpha \\to 0}{\\to} \\mathbf{0}. \\end{align}\nThirdly, since the $\\alpha \\to 0$ limit decouples all spins $\\mathbf{s}_{t-1}$, any term containing dot products $(\\mathbf{s}_{j,t-1}-\\mathbf{m}_{j,t-1}) \\cdot (\\mathbf{s}_{k,t-1}-\\mathbf{m}_{k,t-1})$ of two spin variables is zero for $j \\neq k$ and equal to $R^2 - \\mathbf{m}^2_{j,t-1}$ for $j=k$. We will also encounter terms containing (tensor contractions with) outer products $(\\mathbf{s}_{j,t-1}-\\mathbf{m}_{j,t-1}) \\otimes (\\mathbf{s}_{k,t-1}-\\mathbf{m}_{k,t-1})$, which we can think of as projection operators. For $j \\neq k$, these and similar terms again evaluate to zero, while, for $j=k$, we get the variance contributions we mentioned previously in Eq. \\eqref{eq:spinvariance}.\nFinally, we take the $\\alpha \\to 0$ limit of Eq. \\eqref{eq:mhasecordder} only to end up with the following mess:\n\\begin{align} \u0026amp;\\qquad \\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha} = \\nonumber \\\\ \u0026amp;\\frac{\\beta^2}{R^4} \\frac{1+3\\gamma(0)}{\\gamma(0)^3} \\left( \\left( \\mathbf{m}_{i,t} \\cdot \\mathbf{v}_{i,t} \\right)^2 + \\sum_{j} J_{ij} \\left( \\frac{\\mathbf{m}_{i,t}^2}{1+\\gamma(0)} - \\frac{\\left(\\mathbf{m}_{i,t}\\cdot\\mathbf{m}_{j,t-1}\\right)^2}{R^2 \\gamma(0)} \\right) \\right) \\mathbf{m}_{i,t} \\nonumber \\\\ \u0026amp;- \\frac{\\beta^2}{R^2} \\frac{1}{\\gamma^2 (0) + \\gamma(0)} \\left( \\mathbf{v}_{i,t}^2 + {\\color{red}\\sum_{j} J_{ij} \\left( R^2 - \\mathbf{m}_{j,t-1}^2 \\right)} \\right) \\mathbf{m}_{i,t} \\nonumber \\\\ \u0026amp;- \\frac{\\beta^2}{R^2} \\frac{2}{\\gamma^2 (0) + \\gamma(0)} \\left( \\mathbf{v}_{i,t} \\otimes \\mathbf{v}_{i,t} + {\\color{red}\\sum_{j} J_{ij} \\left( \\frac{\\mathbb{1}}{1+\\gamma(0)} - \\frac{\\mathbf{m}_{j,t-1}\\otimes\\mathbf{m}_{j,t-1}}{R^2 \\gamma(0)} \\right)} \\right) \\mathbf{m}_{i,t} \\nonumber \\end{align}\nAt this point, it is already too late. We should have remembered that the second-order approximation lives in the neighborhood of the first-order approximation. We probably ended up doing too much work by taking terms into account that are of higher order than $\\mathcal{O}(\\alpha^2)$. Eyeballing the result, it might make sense to only keep the ${\\color{red}\\mathrm{red\\ terms}}$, which are linear in $\\mathbf{m}_{i,t}$. We can always reintroduce additional terms later on if it turns out we have been too careless.\nTo arrive at the second-order mean-field equations for the magnetizations, we now only have to solve Eq. \\eqref{eq:secondorderconstraint} for $\\boldsymbol{\\theta}^{*}_{i,t}$,\n\\begin{equation} \\frac{\\partial \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial\\alpha} + \\frac{1}{2} \\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha} = \\mathbf{0} + \\mathcal{O}\\left(\\alpha^3\\right) \\end{equation}\nLet us substitute $\\frac{\\partial \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial\\alpha}$ but keep $\\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha}$ for clarity:\n\\begin{align} \\beta \\left( \\frac{\\mathbb{1}}{1+\\gamma(0)} - \\frac{\\mathbf{m}_{i,t}\\otimes\\mathbf{m}_{i,t}}{R^2 \\gamma(0)} \\right) \\mathbf{v}_{i,t} - \\frac{1}{2} \\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha} = \\mathbf{0} + \\mathcal{O}\\left(\\alpha^3\\right) \\end{align}\nWe can isolate $\\boldsymbol{\\theta}_{i,t}$ in $\\mathbf{v}_{i,t}$ to find\n\\begin{align} \\boldsymbol{\\theta}_{i,t} = \\mathbf{x}_{i,t} \u0026amp;+ \\sum_{j} J_{ij} \\mathbf{m}_{j,t-1} \\nonumber \\\\ \u0026amp;+ \\frac{1+\\gamma(0)}{2\\beta} \\left( \\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha} + \\frac{\\mathbf{m}_{i,t} \\cdot \\frac{\\partial^{2} \\mathbf{m}_{i,t}(\\alpha=0)}{\\partial^{2}\\alpha}}{\\frac{R^2 \\gamma(0)}{1+\\gamma(0)} - \\mathbf{m}_{i,t}^2} \\mathbf{m}_{i,t} \\right), \\end{align}\nwhere we have used the Sherman–Morrison formula to compute the matrix inverse. But we are not done yet, since the expression on the right-hand side also depends (implicitly) on $\\boldsymbol{\\theta}_{i,t}$. It looks like we have stumbled upon a fixed-point equation\n\\begin{align} \\boldsymbol{\\theta}_{i,t} = \\mathbf{f} (\\boldsymbol{\\theta}_{i,t}, \\mathbf{x}_{i,t}, \\mathbf{m}_{i,t}, \\mathbf{m}_{t-1}), \\end{align}\nwhich we should solve for $\\boldsymbol{\\theta}^{*}_{i,t}$. The second-order mean-field equations then become yet another fixed-point equation\n\\begin{equation} \\boxed{\\mathbf{m}_{i,t} = \\boldsymbol{\\varphi} \\left(\\boldsymbol{\\theta}^{*}_{i,t}(\\mathbf{x}_{i,t}, \\mathbf{m}_{i,t}, \\mathbf{m}_{t-1})\\right) } \\label{eq:tapmvector} \\end{equation}\nLike in the TAP approximation Eq. \\eqref{eq:tapm} for the binary case, the dependence of $\\boldsymbol{\\theta}^{*}_{i,t}$ on $\\mathbf{m}_{i,t}$ indicates that we should solve for fixed-point magnetizations $\\mathbf{m}^{*}_{i,t}$. However, in contrast to the binary case, the dependence here is implicit since $\\boldsymbol{\\theta}^{*}_{i,t}$ is obtained from solving another fixed-point equation involving $\\mathbf{m}_{i,t}$. Numerically, we should try to jointly optimize for the fixed-point tuple $(\\mathbf{m}^{*}_{i,t}, \\boldsymbol{\\theta}^{*}_{i,t})$ such that the following pair of equations holds:\n\\begin{align} \\boldsymbol{\\theta}^{*}_{i,t} \u0026amp;= \\mathbf{f} (\\boldsymbol{\\theta}^{*}_{i,t}, \\mathbf{x}_{i,t}, \\mathbf{m}^{*}_{i,t}, \\mathbf{m}_{t-1}) \\\\ \\mathbf{m}^{*}_{i,t} \u0026amp;= \\boldsymbol{\\varphi} \\left( \\boldsymbol{\\theta}^{*}_{i,t}(\\mathbf{x}_{i,t}, \\mathbf{m}^{*}_{i,t}, \\mathbf{m}_{t-1}) \\right) \\end{align}\nLooking ahead at the transformer-module correspondence in Section 4, we recognize a scaled sum of a residual connection, an attention term, and a self-consistent expression in terms of magnetizations and couplings playing the role of the feed-forward network, without having to introduce additional free parameters.\n3.5. A simple JAX implementation $\\ldots$\n4. A family of transformer-like modules $\\ldots$\n4.1. Fast- and slow-moving parameters $\\ldots$\nAs mentioned previously in Transformers Are Secretly Collectives of Spin Systems (2021), each example in a batch of sequential data can be thought of as probing the spin system in a particular way. Physically, the fast-moving parameterized couplings $\\mathbf{J}(\\mathbf{X})$ are determined by the fast-moving parameterized external fields $\\mathbf{X}$, which in turn depend on the magnetizations of the previous layer and ultimately on the input data. The external fields act as an environment of patterns that gets transformed instantly into the values of the coupling matrix, effectively inducing some kind of state of quenched disorder. The slow-moving parameters are those receiving gradient updates during training, e.g. the query-key matrices in the softmax couplings. On the level of the spin-model transformer module, training can be understood as shaping the input-dependent distribution of coupling parameters by amassing information from a huge amount of quenched disorder realizations, sculpting a spin glass with data.\n$\\ldots$\n5. Conclusion $\\ldots$\nAppendices A.1. Vector-spin distribution: normalization constant We consider the single-site vector-spin distribution Eq. \\eqref{eq:pcondsinglesitevector}:\n\\begin{equation} p ( \\mathbf{s} ; \\beta, \\mathbf{h}) = \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}}}{\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} }. \\end{equation}\nLet $Z(\\beta, R, \\mathbf{h})=\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}}$. We switch to $D$-dimensional spherical coordinates to make our life easier and use rotational symmetry to choose the polar axis parallel to $\\mathbf{h}$,\n\\begin{equation} Z(\\beta, R, h) = R^{D-1} \\int_{\\Omega} \\int_{0}^{\\pi} \\mathrm{d}^{D-2} \\Omega \\;\\mathrm{d}\\theta \\; \\mathrm{e}^{\\beta R h \\cos \\theta } \\sin^{D-2} \\theta , \\end{equation}\nwhere $h=\\lVert\\mathbf{h}\\rVert$ and where $\\int_{\\Omega} \\mathrm{d}^{D-2} \\Omega$ represents the integral over all other spherical angles, which coincides with the surface area of the unit sphere in $D-1$ dimensions,\n\\begin{equation} S_{D-1} = \\frac{2\\pi^{\\frac{D-1}{2}}}{\\Gamma\\left( \\frac{D-1}{2} \\right)}, \\end{equation}\nso that\n\\begin{equation} Z(\\beta, R, h) = \\frac{2 \\pi^{\\frac{D-1}{2}} R^{D-1}}{\\Gamma\\left( \\frac{D-1}{2} \\right)} \\int_{0}^{\\pi} \\mathrm{d}\\theta \\; \\mathrm{e}^{\\beta R h \\cos \\theta } \\sin^{D-2} \\theta . \\end{equation}\nIf we now let $u = \\cos \\theta$, then\n\\begin{equation} Z(\\beta, R, h) = \\frac{2 \\pi^{\\frac{D-1}{2}} R^{D-1}}{\\Gamma\\left( \\frac{D-1}{2} \\right)} \\int_{-1}^{1} \\mathrm{d}u \\; \\mathrm{e}^{\\beta R h u } \\left(1 - u^2\\right)^{(D-3)/2} . \\end{equation}\nRecognizing an integral representation of the modified Bessel function of the first kind,\n\\begin{equation} I_{\\nu}(z) = \\frac{2^{-\\nu}}{\\sqrt{\\pi}\\, \\Gamma\\left(\\nu+\\frac{1}{2}\\right)} z^{\\nu} \\int_{-1}^{1} \\mathrm{d}t \\; \\mathrm{e}^{\\pm zt} \\left(1-t^2\\right)^{\\nu-\\frac{1}{2}}, \\end{equation}\nwe identify $\\nu = D/2 - 1$ and $z = \\beta R h$ to find\n\\begin{equation} Z(\\beta, R, h) = \\frac{ \\left( 2 \\pi R \\right)^{D/2} I_{D/2 - 1}(\\beta R h) }{ \\left(\\beta h\\right)^{D/2-1} }. \\end{equation}\nA.2. Vector-spin distribution: expected value (first moment) We consider the single-site vector-spin distribution Eq. \\eqref{eq:pcondsinglesitevector}:\n\\begin{equation} p ( \\mathbf{s} ; \\beta, \\mathbf{h}) = \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}}}{\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} }. \\end{equation}\nStarting from the expression of the normalization constant Eq. \\eqref{eq:partfun},\n\\begin{equation} \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} = \\frac{ \\left( 2 \\pi R \\right)^{D/2} I_{D/2 - 1}(\\beta R \\lVert \\mathbf{h}\\rVert) }{ \\left(\\beta \\lVert \\mathbf{h}\\rVert\\right)^{D/2-1} } = Z(\\beta, R, \\lVert \\mathbf{h}\\rVert) , \\end{equation}\nwe write the expected value as\n\\begin{equation} \\mathbb{E}_{p} [ \\mathbf{s} ] = \\frac{1}{Z} \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathbf{s} \\, \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} = \\frac{1}{\\beta Z} \\frac{ \\partial }{ \\partial \\mathbf{h} } \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} \\end{equation}\nso that\n\\begin{align} \\mathbb{E}_{p} [ \\mathbf{s} ] = \\frac{1}{\\beta Z} \\frac{ \\partial }{ \\partial \\mathbf{h} } \\left( \\frac{ \\left( 2 \\pi R \\right)^{D/2} I_{D/2 - 1}(\\beta R \\lVert\\mathbf{h} \\rVert) }{ \\left(\\beta \\lVert\\mathbf{h}\\rVert \\right)^{D/2-1} } \\right) \\end{align}\nwhich evaluates to\n\\begin{align} \\mathbb{E}_{p} [ \\mathbf{s} ] = \\left( \\frac{I\u0026rsquo;_{D/2 - 1}(\\beta R \\lVert \\mathbf{h}\\rVert)}{I_{D/2 - 1}(\\beta R \\lVert\\mathbf{h}\\rVert)} - \\frac{ D/2-1 }{ \\beta R \\lVert\\mathbf{h}\\rVert} \\right) \\frac{R \\mathbf{h}}{\\lVert\\mathbf{h}\\rVert}. \\end{align}\nUsing the modified Bessel function recurrence relations,\n\\begin{align} I_{\\nu-1}(z) - I_{\\nu+1}(z) \u0026amp;= \\frac{2\\nu}{z} I_{\\nu}(z), \\label{eq:irecurr}\\\\ I_{\\nu-1}(z) + I_{\\nu+1}(z) \u0026amp;= 2 I'_{\\nu}(z), \\label{eq:irecurrderiv} \\end{align}\nwe end up with\n\\begin{align} \\mathbb{E}_{p} [ \\mathbf{s} ] = \\frac{I_{D/2}(\\beta R \\lVert \\mathbf{h}\\rVert)}{I_{D/2 - 1}(\\beta R \\lVert\\mathbf{h}\\rVert)} \\frac{R \\mathbf{h}}{\\lVert\\mathbf{h}\\rVert}\\equiv \\boldsymbol{\\varphi} (\\mathbf{h}). \\label{eq:app:expectedvalue} \\end{align}\nA.3. Vector-spin distribution: variance (second moment)  ✨ TODO: Add variance for general case.\n We consider the single-site vector-spin distribution Eq. \\eqref{eq:pcondsinglesitevector}:\n\\begin{equation} p ( \\mathbf{s} ; \\beta, \\mathbf{h}) = \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}}}{\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} }. \\end{equation}\nUsing the expression of the normalization constant Eq. \\eqref{eq:partfun},\n\\begin{equation} \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} = \\frac{ \\left( 2 \\pi R \\right)^{D/2} I_{D/2 - 1}(\\beta R \\lVert \\mathbf{h}\\rVert) }{ \\left(\\beta \\lVert \\mathbf{h}\\rVert\\right)^{D/2-1} } = Z(\\beta, R, \\lVert \\mathbf{h}\\rVert) , \\end{equation}\nwe write the symmetric outer-product variance matrix as\n\\begin{align} \\mathrm{Var}_{p} [ \\mathbf{s} ] \u0026amp;= \\frac{1}{Z} \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} \\, ( \\mathbf{s} - \\mathbb{E}_{p} [ \\mathbf{s} ])( \\mathbf{s} - \\mathbb{E}_{p} [ \\mathbf{s} ])^{T} \\\\ \u0026amp;= \\frac{1}{\\beta^2 Z} \\frac{ \\partial^2 }{ \\partial \\mathbf{h} \\partial \\mathbf{h}^{T} } \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}} - \\mathbb{E}_{p} [ \\mathbf{s} ] \\mathbb{E}_{p} [ \\mathbf{s} ]^{T}, \\end{align}\nso that\n\\begin{align} \\mathrm{Var}_{p} [ \\mathbf{s} ] \u0026amp;= \\frac{1}{\\beta Z} \\frac{ \\partial }{ \\partial \\mathbf{h} } \\left( Z \\mathbb{E}_{p} [ \\mathbf{s} ]^{T} \\right) - \\mathbb{E}_{p} [ \\mathbf{s} ] \\mathbb{E}_{p} [ \\mathbf{s} ]^{T}, \\\\ \u0026amp;= \\frac{1}{\\beta} \\frac{ \\partial }{ \\partial \\mathbf{h} } \\mathbb{E}_{p} [ \\mathbf{s} ]^{T}, \\end{align}\nwhich evaluates to\n\\begin{align} \\mathrm{Var}_{p} [ \\mathbf{s} ] \u0026amp;= \\ldots \\label{eq:app:var} \\end{align}\nfor the general case with the expected value given by Eq. \\eqref{eq:app:expectedvalue} and to\n\\begin{align} \\mathrm{Var}_{p} [ \\mathbf{s} ] \u0026amp;= \\frac{\\mathbb{1}}{1+\\gamma(\\mathbf{h})} - \\frac{\\beta^2\\mathbf{h} \\otimes \\mathbf{h}}{R^2\\gamma(\\mathbf{h})\\left(1+\\gamma(\\mathbf{h})\\right)^2}\\\\ \u0026amp;= \\frac{\\mathbb{1}}{1+\\gamma(\\mathbf{h})} - \\frac{\\boldsymbol{\\varphi} (\\mathbf{h}) \\otimes \\boldsymbol{\\varphi}(\\mathbf{h})}{R^2\\gamma(\\mathbf{h})} \\end{align}\nfor the large-$D$ limit with the expected value given by Eq. \\eqref{eq:largedevmag}, where\n\\begin{align} \\gamma(\\mathbf{h}) = \\sqrt{1+\\beta^{2}\\lVert\\mathbf{h}\\rVert^{2}/R^2} \\end{align}\nA.4. Ratio of modified Bessel functions of the first kind To compute the ratio $I_{\\nu+1}(x) / I_{\\nu}(x)$ of modified Bessel functions of the first kind for $\\nu \\geq 0$ and $x \\geq 0$, we implement a JAX version of the algorithm described in (Amos, 1974). A pseudocode implementation can be found in (Kurz et al., 2013). We compare our implementation against explicitly calculating the ratio using scipy.special.ive across a range of orders $\\nu$ for several different values of $x$ to get a feel for its behavior.\nWe observe a satisfying agreement between the two approaches. For $x=\\sqrt{\\nu}$, the ratio takes on very small values for large orders. For $x=\\nu^2$, the oppositive happens and we see saturation. The case $x=\\nu$ seems to sit in between, which suggests it might be opportune to fix the radius of our little spins to $R=\\sqrt{D}$ so that with $\\lVert\\mathbf{h}\\rVert \\sim \\mathcal{O}(\\sqrt{D})$ we might maximize the \u0026ldquo;sensitivity\u0026rdquo; of the expected value. In this regime, we can get away with known asymptotic expansions for large $\\nu$ given that the ratio flattens out quickly.\nA.5. General case: partial derivatives with respect to $\\alpha$  ✨ TODO: Clean up and verify.\n We are interested in computing the first-order and second-order derivative with respect to $\\alpha$ of the function\n\\begin{equation} \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) = \\frac{I_{D/2}(\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert)}{I_{D/2 - 1}(\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert)} \\frac{R \\mathbf{h}(\\alpha)}{\\lVert \\mathbf{h}(\\alpha) \\rVert}, \\end{equation}\nwhere $\\mathbf{h}(\\alpha) = \\boldsymbol{\\theta} + \\alpha \\Delta \\mathbf{h}$. Using\n\\begin{equation} \\frac{\\partial \\lVert \\mathbf{h}(\\alpha) \\rVert}{\\partial\\alpha} = \\frac{\\mathbf{h}(\\alpha) \\cdot \\Delta \\mathbf{h}}{\\lVert \\mathbf{h}(\\alpha) \\rVert} \\end{equation}\nand Eqs. \\eqref{eq:irecurr}-\\eqref{eq:irecurrderiv}, we find\n\\begin{align} \\frac{\\partial \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha))}{\\partial\\alpha} = \\beta \u0026amp;\\lambda_{D} (\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert) \\left( \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\cdot \\Delta \\mathbf{h} \\right) \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\nonumber \\\\ \u0026amp;+ \\frac{I_{D/2}(\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert)}{I_{D/2 - 1}(\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert)} \\frac{R \\Delta \\mathbf{h}}{\\lVert \\mathbf{h}(\\alpha) \\rVert} \\label{eq:generalgradalphafirstorder} \\end{align}\nwhere\n\\begin{equation} \\lambda_{D} (x) = \\frac{I^2_{D/2-1}(x)}{I^2_{D/2}(x)} - \\frac{D}{x} \\frac{I_{D/2-1}(x)}{I_{D/2}(x)} - 1. \\label{eq:app:lambda} \\end{equation}\nFor the second-order derivative, we need to slog through even more tedious algebra,\n\\begin{align} \\frac{\\partial^2 \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha))}{\\partial^2\\alpha} = \\beta \u0026amp;\\frac{\\partial}{\\partial\\alpha}\\biggl( \\lambda_{D} (\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert) \\left( \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\cdot \\Delta \\mathbf{h} \\right) \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\biggr) \\nonumber \\\\ \u0026amp;+ \\frac{\\partial}{\\partial\\alpha}\\biggl( \\frac{I_{D/2}(\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert)}{I_{D/2 - 1}(\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert)} \\frac{R \\Delta \\mathbf{h}}{\\lVert \\mathbf{h}(\\alpha) \\rVert} \\biggr) , \\end{align}\nwhich eventually leads to something like\n\\begin{align} \\frac{\\partial^2 \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha))}{\\partial^2\\alpha} = -2\\beta^2 \u0026amp; \\, \\kappa_{D} (\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert) \\left( \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\cdot \\Delta \\mathbf{h} \\right)^{2} \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\nonumber \\\\ \u0026amp;+ \\beta \\lambda_{D} (\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert) \\left( \\frac{\\partial\\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha))}{\\partial\\alpha} \\cdot \\Delta \\mathbf{h} \\right) \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\nonumber \\\\ \u0026amp;+ \\beta \\lambda_{D} (\\beta R \\lVert \\mathbf{h}(\\alpha) \\rVert) \\left( \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\cdot \\Delta \\mathbf{h} \\right) \\frac{\\partial\\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha))}{\\partial\\alpha} \\nonumber \\\\ \u0026amp;- \\frac{D}{\\lVert \\mathbf{h}(\\alpha) \\rVert^2} \\left( \\boldsymbol{\\varphi}(\\mathbf{h}(\\alpha)) \\cdot \\Delta \\mathbf{h} \\right) \\Delta \\mathbf{h} , \\label{eq:generalgradalphasecondorder} \\end{align}\nwhere\n\\begin{align} \\kappa_{D} (x) = \\lambda^2_{D} (x) + \\left( 1 + \\frac{D/2 + 1}{x} \\frac{I_{D/2-1}(x)}{I_{D/2}(x)} \\right) \\lambda_{D} (x) + \\frac{1}{x} \\frac{I_{D/2-1}(x)}{I_{D/2}(x)}. \\end{align}\nEquation \\eqref{eq:generalgradalphasecondorder} can be further simplified by substituting the first-order derivative Eq. \\eqref{eq:generalgradalphafirstorder} and further simplifying the resulting expression. The derivation of the mean-field equations proceeds in a similar fashion as in the main text, but uses \\eqref{eq:generalgradalphafirstorder} and \\eqref{eq:generalgradalphasecondorder} as expressions for the partial derivatives instead of their large-$D$ approximations.\nAnother useful derivative is that of the single-site probability distribution \\eqref{eq:pcondsinglesitevector},\n\\begin{align} \\frac{\\partial}{\\partial\\alpha} \\left( \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}(\\alpha)}}{\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}(\\alpha)} } \\right) = \\frac{\\partial}{\\partial\\mathbf{h}(\\alpha)} \\left( \\frac{\\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}(\\alpha)}}{\\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}(\\alpha)} } \\right) \\cdot \\Delta \\mathbf{h}, \\end{align}\nwhich evaluates to\n\\begin{align} \\beta \\left( \\mathbf{s} - \\boldsymbol{\\varphi}\\left(\\mathbf{h}(\\alpha)\\right) \\right) \\cdot \\Delta \\mathbf{h} \\frac{ \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}(\\alpha)} }{ \\int_{S_{D-1}} \\mathrm{d}^{D} \\mathbf{s} \\; \\mathrm{e}^{\\beta \\, \\mathbf{s} \\cdot \\mathbf{h}(\\alpha)} } \\end{align}\nand can be used to calculate derivatives of the conditional distribution \\eqref{eq:pcondaltvector}.\nReferences \u0026amp; footnotes A non-exhaustive list of references and inspiration includes:\n  M. Aguilera, S.A. Moosavi, and H. Shimazaki, A unifying framework for mean-field theories of asymmetric kinetic Ising systems, Nat Commun 12, 1197 (2021) https://arxiv.org/abs/2002.04309\n  Y. Roudi and J. Hertz, Dynamical TAP equations for non-equilibrium Ising spin glasses, J. Stat. Mech., P03031 (2011) https://arxiv.org/abs/1103.1044\n  H.J. Kappen and J.J. Spanjers, Mean field theory for asymmetric neural networks, Phys. Rev. E 61, 5658 (2000)\n  G. Parisi, Asymmetric neural networks and the process of learning, J. Phys. A: Math. Gen. 19 L675 (1986)\n   If you happen to find this work useful, please consider citing it as:\n@article{bal2023spinmodeltransformers, title = {Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules}, author = {Bal, Matthias}, year = {2023}, month = {?}, url = {https://mcbal.github.io/post/spin-model-transformers} }    We plot the absolute value to get rid of artificial \u0026ldquo;jumps\u0026rdquo; between the two branches. These occur because all models are simulated independently when sweeping across $\\beta$ and the some combinations of initial state and model parameters might just happen to bounce to the other branch when $\\beta$ changes in the $\\beta \u0026gt; \\beta_c$ regime.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1655627297,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694852561,"objectID":"8ef99a068df49092c0855ec79cce781f","permalink":"https://mcbal.github.io/post/spin-model-transformers/","publishdate":"2022-06-19T09:28:17+01:00","relpermalink":"/post/spin-model-transformers/","section":"post","summary":"A non-equilibrium statistical mechanics perspective on transformers","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Ising Models","Many-Body Systems","Mean-Field Theory","Neural Networks","Non-Equilibrium Dynamics","Spin Glasses","Spin Models","Statistical Physics","Transformers","Vector-Spin Models"],"title":"Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules","type":"post"},{"authors":[],"categories":[],"content":" ✨ Update (April 2023): Consider reading Spin-Model Transformers: A Physics-Inspired Class of Transformer Modules where we continue building on the intuition of probing a spin system to engineer its collective response but get rid of the assumption of symmetric coupling matrices by shifting focus from equilibrium free energies to dynamical mean-field approximations of non-equilibrium vector-spin models. Come join the fun.\n  Introduction Where does the transformer module architecture come from? Deriving attention from energy functions only gets you so far Back to the roots: physical spin systems and vector-spin models Why don\u0026rsquo;t we just probe a vector-spin system with data? A slice of statistical mechanics: magnetizations and free energies Turning a differentiable spin system into a neural network An exercise in squinting: recognizing the transformer module Training transformer modules shapes collective behavior Training deep transformers orchestrates spin-system collectives Conclusion   1. Introduction In this post, we try to distill a unifying perspective out of ideas developed in a series of longer posts on understanding transformers as physical systems:\n Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms Transformers from Spin Models: Approximate Free Energy Minimization  We argue that a blueprint of the neural-network architecture of the archetypical transformer module can be derived from the structure of physical spin systems familiar from classical statistical mechanics. More specifically, we claim that the forward pass of transformer modules maps onto computing magnetizations in vector-spin models in response to incoming data. We imagine transformers as collectives of differentiable spin systems whose behavior can be shaped through training.\n2. Where does the transformer module architecture come from? Taking a bird\u0026rsquo;s eye view of the evergrowing zoo of transformer architectures in natural language processing and computer vision suggests that the design pattern introduced in Attention is All You Need (Vaswani et al., 2017)1 is still dominant. Almost all architectural variations of transformer modules published in the last four years have stuck to a successful combination of residual connections, an attention-like operation (token-mixing), normalization layers, and a feed-forward-like operation (channel-mixing).\n\nRecent work like MetaFormer is Actually What You Need for Vision (Yu et al., 2021)2 appropriately shifts focus to the high-level architecture of the transformer module and argues that its full structure, rather than just the token-mixing attention operation, is essential for transformers to achieve competitive performance.\nSo where does this archetypical design pattern come from? Why does it seem to stick around? Is there any physical intuition behind its structure?\n3. Deriving attention from energy functions only gets you so far Recent papers like Hopfield Networks is All You Need (Ramsauer et al., 2020)3 and Large Associative Memory Problem in Neurobiology and Machine Learning (Krotov and Hopfield, 2020)4 have looked for physical intuition behind attention mechanisms using an energy-based perspective phrased in terms of modern continuous Hopfield networks. The main idea is to derive the softmax-attention update rule\n\\begin{equation} \\boldsymbol{Q}' = \\text{softmax}\\left( \\frac{\\boldsymbol{Q} \\boldsymbol{K}^T}{\\sqrt{d}} \\right) \\boldsymbol{K} \\end{equation}\nby taking a large gradient descent update step using the derivative with respect to input queries $\\boldsymbol{Q}$ of some judiciously chosen energy function\n\\begin{equation} E = \\frac{1}{2} \\boldsymbol{Q} \\boldsymbol{Q}^T -\\mathrm{logsumexp} \\left( \\frac{\\boldsymbol{Q} \\boldsymbol{K}^T}{\\sqrt{d}} \\right). \\label{eq:logsumexp} \\end{equation}\nIn this way, vanilla softmax attention can be recast as taking a large gradient step. The energy landscape defined by Eq. \\eqref{eq:logsumexp} implements an associative memory system for storing and retrieving vector patterns where queries flow towards valleys associated with their nearest keys (see Attention as Energy Minimization: Visualizing Energy Landscapes):\n\nBut there is more to transformer modules than just attention. In practice, we know that residual connections, normalization layers, and feed-forward layers are all essential to achieve good empirical performance.\nCan we generalize this physical intuition of taking derivatives with respect to an energy function to recover the full transformer module? Yes, we can. But we have to take a step back from energy functions and focus on their underlying physical systems instead.\n4. Back to the roots: physical spin systems and vector-spin models Energy functions in classical statistical mechanics are succinct descriptions encoding interactions and constraints in physical systems. Spin systems are prototypical physical systems which often serve as toy models for all kinds of phenomena5.\nThe Ising model is a simple toy model describing a classical binary spin system with local spin degrees of freedom at every site pointing either up or down. The energy function of the binary random Ising model for $N$ spins in the presence of a site-dependent external magnetic field is given by\n\\begin{equation} E = - \\sum_{i,j=1}^{N} J_{ij} \\sigma_{i} \\sigma_{j} - \\sum_{i=1}^{N} h_{i} \\sigma_{i}, \\label{eq:binaryrandomising} \\end{equation}\nwhere the $J_{ij}$ encode coupling strengths between all pairs of spins and the external magnetic fields $h_{i}$ act as biases by providing a preferential value of alignment at every site. The model defined by \\eqref{eq:binaryrandomising} is also known as a Boltzmann machine or Hopfield network. A cartoon of this model looks like a graph of little arrows that are pairwise coupled6:\nAt thermal equilibrium, the Boltzmann probability distribution $e^{-\\beta E\\left( \\sigma \\right)} / Z$ reflects what patterns of up-down spins, or spin configurations, are preferred. The partition function $Z = \\sum_{\\sigma} e^{-\\beta E\\left( \\sigma \\right)}$ of a spin system is not only a normalization constant but also a magical object relating the microscopic world of fluctuating spins to thermodynamic, observable quantities via the free energy $F = - \\beta^{-1} \\log Z$. Even for simple spin systems, computing partition functions by summing over all possible configurations is a shockingly hard thing to do in most scenarios.\nBinary spin models are nice but rarely excite machine learning practitioners anymore nowadays. Modern neural networks like transformers act on sequences of vectors like token embeddings or image patches. Instead of abandoning spin models altogether, we could consider vector-spin models. Replacing binary degrees of freedom with $d$-dimensional vector degrees of freedom, we can define a spin-model energy function\n\\begin{align} E = - \\sum_{i,j=1}^{N} J_{ij} \\; \\boldsymbol{\\sigma}_{i} \\cdot \\boldsymbol{\\sigma}_{j} - \\sum_{i=1}^{N} \\boldsymbol{h}_{i} \\cdot \\boldsymbol{\\sigma}_{i}, \\label{eq:vectorrandomising} \\end{align}\nwhere the scalar products have turned into dot products. Models of this form first popped up in 1960s statistical mechanics literature as classical $d$-vector models. They also appear in recent studies on higher-dimensional generalizations of spin glass models7.\nNow how can we relate vector-spin systems like Eq. \\eqref{eq:vectorrandomising} to modern neural networks?\n5. Why don’t we just probe a vector-spin system with data? Let\u0026rsquo;s pursue an intuitive idea. Imagine we want to expose our vector-spin system Eq. \\eqref{eq:vectorrandomising} to a sequence of vector data. We can do this by having the sequence act as the spin system\u0026rsquo;s external magnetic field $(\\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N})$. We would then like to observe how the spin system responds to this particular environment of patterns.\nIf all of the steps in the computation of the spin system\u0026rsquo;s responses can be implemented in a differentiable way, we should be able to engineer its collective behavior by optimizing the coupling parameters to better respond to future incoming data. We propose to observe spin-system responses in terms of magnetizations computed from free energies.\n6. A slice of statistical mechanics: magnetizations and free energies For ease of notation, let\u0026rsquo;s call the model parameters $\\theta \\equiv \\{ J_{ij} \\}$, the spins $\\sigma \\equiv \\{ \\boldsymbol{\\sigma}_{i} \\}$, and the external magnetic fields $h \\equiv (\\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N})$. We can then schematically write our spin system\u0026rsquo;s partition function as\n\\begin{align} Z_{\\theta} \\left( h \\right) = \\int \\mathrm{d} \\sigma \\ \\mathrm{e}^{ - \\beta E_{\\theta}\\left( \\sigma, h \\right) } \\label{eq:partfun} \\end{align}\nand the corresponding free energy as $F_{\\theta} \\left( h \\right) = - \\beta^{-1} \\log Z_{\\theta} \\left( h \\right)$.\nMagnetizations are responses of our spin system to the external magnetic field imposed by $(\\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N})$. From standard thermodynamics, we know that we can calculate magnetizations from the free energy by differentiating with respect to the external field8\n\\begin{align} \\boldsymbol{m}_{i} = - \\frac{\\mathrm{d} F_{\\theta} \\left( \\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N} \\right)}{\\mathrm{d} \\boldsymbol{h}_{i}} = \\langle \\boldsymbol{\\sigma}_{i} \\rangle , \\label{eq:sigma} \\end{align}\nwhich, in this case, boils down to calculating spin expectation values. The magnetization for every site depends on the couplings and, through the couplings between spins, on the values of the external field at all sites. Magnetizations reveal how spins will collectively tend to align themselves when we place the spin system in an environment of patterns.\nBefore we move on, we have to account for one more complication. If we want to draw a correspondence between transformer modules and vector-spin systems, we will have to allow for couplings that depend on the external magnetic field. For example, the attention matrix in vanilla transformers looks something like\n\\begin{equation} J_{ij} \\left( \\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N} \\right) = \\left[\\mathrm{softmax}\\left( \\frac{\\boldsymbol{H} \\boldsymbol{W}_{\\boldsymbol{Q}} \\boldsymbol{W}_{\\boldsymbol{K}}^{T} \\boldsymbol{H}^{T}}{\\sqrt{d}} \\right)\\right]_{ij}, \\label{eq:softmaxcouplings} \\end{equation}\nwhere the matrix $\\boldsymbol{H}$ denotes the stack of external magnetic field vectors. The interactions between spins are determined dynamically based on the inputs. From a physics perspective, these \u0026ldquo;amortized\u0026rdquo; couplings are very weird and highly unusual, but such is the transformer.\nThe potential dependency of the couplings on the external field changes the magnetization of Eq. \\eqref{eq:sigma} to an expression of the form\n\\begin{align} \\boldsymbol{m}_{i} \u0026amp;= - \\frac{\\mathrm{d} F_{\\theta} \\left( \\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N} \\right)}{\\mathrm{d} \\boldsymbol{h}_{i}} \\nonumber \\\\ \u0026amp;= \\langle \\boldsymbol{\\sigma}_{i} \\rangle + \\sum_{m,n} \\langle \\boldsymbol{\\sigma}_{m} \\cdot \\boldsymbol{\\sigma}_{n} \\rangle \\frac{\\partial J_{mn} \\left( \\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N} \\right) }{ \\partial \\boldsymbol{h}_{i} } , \\label{eq:sigmaweird} \\end{align}\nwhere two-point correlation functions are seen to act as weights for the coupling contributions9. In practice, we should of course let an automatic differentiation framework keep track of dependencies so that we can get away with simply computing\n\\begin{align} \\boldsymbol{m}_{i} = - \\frac{\\mathrm{d} F_{\\theta} \\left( \\boldsymbol{h}_{1}, \\boldsymbol{h}_{2}, \\ldots, \\boldsymbol{h}_{N} \\right)}{\\mathrm{d} \\boldsymbol{h}_{i}}, \\label{eq:magnetization} \\end{align}\nassuming we have a differentiable expression for the (approximate) free energy available.\n7. Turning a differentiable spin system into a neural network Let\u0026rsquo;s now use the ingredients introduced above to construct a neural network module which wraps around a vector-spin system. Given the energy function Eq. \\eqref{eq:vectorrandomising} and the free energy $F_{\\theta} \\left( h \\right) = - \\beta^{-1} \\log \\int \\mathrm{d} \\sigma \\ \\mathrm{e}^{ - \\beta E_{\\theta}\\left( \\sigma, h \\right) }$, we let incoming data play the role of the external magnetic field and return magnetizations in response.\nNice. But didn\u0026rsquo;t we mention before that partition functions (and hence free energies and thus magnetizations) are shockingly hard to compute? Why introduce all these formal expressions if we cannot compute anything?\nLooking back at statistical mechanics papers from the 1950s-1970s, it turns out that physicists have already developed several tricks and approximation methods that can be applied to deal with vector-spin systems. Computational evidence that the partition function approach outlined above is possible for vector-spin systems can be found in Deep Implicit Attention (below, left) and Approximate Free Energy Minimization (below, right).\nIn these examples, approximations of the partition function Eq. \\eqref{eq:partfun} were obtained following respectively a mean-field theory and a steepest-descent approach. Our numerical implementations of both approaches rely internally on deep implicit layers to ensure that fixed-point calculations and root-solving steps are efficiently differentiable.\n8. An exercise in squinting: recognizing the transformer module Computing magnetizations according to Eq. \\eqref{eq:magnetization} from the (approximate) free energies obtained in Deep Implicit Attention and Approximate Free Energy Minimization reveals a high-level structure that is surprisingly familiar: a pattern of residual connections, token-mixing, normalization, and channel-mixing. Approaching the crux from the other direction, we argue that transformer modules react to inputs by implementing particular approximations to the general magnetization response Eq. \\eqref{eq:sigmaweird}.\nResidual connections are proportional to the inputs and arise from the presence of the external magnetic field. Token-mixing contributions emerge from the coupling terms in the energy function and mix inputs without acting on the local vector-spin dimension. Normalization follows from requiring that the energy of the spin system remain linearly proportional to the number of lattice sites and from normalizing the external magnetic field vectors. Channel-mixing contributions include terms in the magnetization that can be applied locally, like Onsager self-correction terms in mean-field approaches or (approximations to) contributions coming from input-dependent couplings in Eq. \\eqref{eq:sigmaweird}.\nTaken together, these observations suggest that we can picture the forward pass of a transformer module as a wrapper around a vector-spin system: module inputs are routed to the external magnetic field (and, optionally, to a parametrized couplings function) after which magnetizations are returned as outputs. The transformer module bears an uncanny resemblance to a differentiable physical system whose collective behavior we can control through training.\n9. Training transformer modules shapes collective behavior Now that we can picture transformer modules as physical spin systems responding to getting probed with data, let\u0026rsquo;s imagine what training them looks like.\nOn the level of the energy function of our spin system Eq. \\eqref{eq:vectorrandomising}, we can model the training process of a transformer module by introducing a (discrete) time dimension and making the external magnetic field time-dependent, leading to10\n\\begin{equation} E(t) = - \\sum_{i,j=1}^{N} J_{ij} \\; \\boldsymbol{\\sigma}_{i} \\cdot \\boldsymbol{\\sigma}_{j} - \\sum_{i=1}^{N} \\boldsymbol{h}_{i}(t) \\cdot \\boldsymbol{\\sigma}_{i} \\label{eq:sloppyenergy} \\end{equation}\nAt every training step $t$, a sequence of incoming data $\\{ \\boldsymbol{h}_{1}(t), \\boldsymbol{h}_{2}(t), \\ldots, \\boldsymbol{h}_{N}(t) \\}$ takes on the role of external magnetic field. During the forward pass, magnetizations $\\boldsymbol{m}_{i}$ are computed in a differentiable way according to the current model parameters and in the presence of the current external magnetic field. Physically, we consider \u0026ldquo;quenched\u0026rdquo; systems with \u0026ldquo;frozen\u0026rdquo; couplings at every training step. During the backward pass, the module\u0026rsquo;s coupling parameters $J_{ij}$ get updated, nudging the interactions in the spin system so as to influence its magnetization responses to similar data in future iterations.\nWe can think about this training process as gradually shaping the collective behavior of a differentiable vector-spin system that is driven by data. If the couplings depend on the inputs, like in Eq. \\eqref{eq:softmaxcouplings}, we should make the couplings time-dependent as well in Eq. \\eqref{eq:sloppyenergy}. In that case, the external magnetic fields as well as the parametrized couplings change instantaneously at every training step.\n10. Training deep transformers orchestrates spin-system collectives Training a deep transformer model corresponds to orchestrating a stack of transformer modules by building up a differentiable structure of correlations where the magnetizations of one spin system drive the next one. Wiggling (billions of) parameters during training nudges the cascading response behavior of the collective of spin systems to better adapt to the collective\u0026rsquo;s (meta-)tasks as specified by the data and the loss function.\n11. Conclusion In this post, we argued that the forward pass of a transformer module maps onto computing magnetizations in a vector-spin model responding to data. Generalizing previous work on understanding softmax attention modules in terms of modern continuous Hopfield networks by taking derivatives of a judiciously chosen energy function, we propose to take derivatives of the free energy of a general vector-spin system to get to a blueprint of the architecture of a full transformer module.\nBy zooming out and approaching transformers from a tangential, statistical-mechanical point of view, we arrived at a physical intuition of transformers that seems hard to obtain when restricting oneself to perpetually perturbing explicit neural network architectures. Recognizing transformer modules as spin models in disguise might not only unify architectural variations as different ways to approximately compute magnetizations but also elucidate the empirical success of transformers in deep learning.\nAcknowledgements We would like to thank ML Collective for hosting its research jams and providing a friendly environment to present ideas.\nReferences \u0026amp; footnotes If you happen to find this work useful, please consider citing it as:\n@article{bal2021isingisallyouneed, title = {Transformers Are Secretly Collectives of Spin Systems}, author = {Bal, Matthias}, year = {2021}, month = {November}, url = {https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/} }    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention Is All You Need (2017)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan, MetaFormer is Actually What You Need for Vision (2021)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, Hopfield Networks is All You Need (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Consider reading the Physics Today article on Statistical Mechanics of Neural Networks (Sompolinsky, 1988) for an introduction to disordered systems, spin glasses, Ising spin systems, emergent collective computational abilities, associative memories, Hopfield models, and the idea of learning patterns as shaping the behavior of systems. Essentially, what we\u0026rsquo;re trying to do in this post is figuring out a way to relate modern transformer models back to these old ideas.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n We plot spin sites at random positions to emphasize that there is no spatial notion of \u0026ldquo;closeness\u0026rdquo; in a fully-connected system: every site is just a hop away. To not overload the graph, we only draw connections strongest in absolute value.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For example, see The Free Energy of Spherical Vector Spin Glasses (Ko, 2018) and Free Energy in the Mixed p-spin Models With Vector Spins (Panchenko, 2015).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For example, see the content of Chapter 2 in the lecture notes on statistical field theory by Thierry Giamarchi.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n In the absence of an explicit expression for the free energy, one of the feed-forward network\u0026rsquo;s roles might be to try to approximate the complicated dependencies in the magnetization expression Eq. \\eqref{eq:sigmaweird}, at the cost of introducing a large amount of additional free parameters beyond just the coupling parameters. It would be interesting to look into this numerically at scale using the free energy expression obtained in Approximate Free Energy Minimization.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The time-dependence in Eq. \\eqref{eq:sloppyenergy} smells of non-equilibrium statistical mechanics. Incoming data might be considered as time-dependent \u0026ldquo;probes\u0026rdquo; which inject energy (and useful information if its content is low-entropy enough) into a non-equilibrium system. By nudging its dynamical response behavior across spatiotemporal scales, the system could potentially learn how to deal with being driven by all kinds of patterns in incoming data. For an interesting toy example of such behavior, see this talk by Jeremy England on Low rattling: a principle for understanding driven many-body self-organization.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1637666237,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638213138,"objectID":"c52cc052d6413db0517fcd0c908ee2c3","permalink":"https://mcbal.github.io/post/transformers-are-secretly-collectives-of-spin-systems/","publishdate":"2021-11-23T12:17:17+01:00","relpermalink":"/post/transformers-are-secretly-collectives-of-spin-systems/","section":"post","summary":"A statistical mechanics perspective on transformers","tags":["Artificial Intelligence","Associative Memories","Attention","Boltzmann Machine","Deep Learning","Emergent Collective Computational Capabilities","Free Energy","Hopfield Networks","Ising Models","Many-Body Systems","Neural Networks","Partition Function","Statistical Physics","Transformers","Vector-Spin Models"],"title":"Transformers Are Secretly Collectives of Spin Systems","type":"post"},{"authors":[],"categories":[],"content":" ✨ Update (November 2021): Consider reading Transformers Are Secretly Collectives of Spin Systems for a high-level overview of some of the ideas outlined in this post.\n  Introduction Massaging partition functions  A vector-spin model and its partition function Peeking into a physicist’s bag of tricks Steepest descent: hunting for the saddle Taking stock of what we have done  Questioning steepest descent and the large-$D$ limit Energy-based models and effective energy functions Spin glasses and mean-field approximation     Implementing approximate free-energy minimization  The algorithm: bold moves on a tricky landscape  Initialization and normalization Implicit layers for steepest-descent root-finding Fun with free energies   The attention module: probing spins with data  Spin expectation values Wrapping around the spin model Comparison with vanilla transformers     Conclusion   1. Introduction  ✨ Code: A PyTorch implementation of the ideas outlined in this blog post is available in the GitHub repository mcbal/afem.\n In Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms, we introduced a mean-field theory perspective on transformer modules. We showed how their outputs can be understood as mean-field spin expectation values of simple Ising-like vector-spin systems. Physically, the process of training a transformer module can be understood as driving a classical many-body system with data and iteratively shaping its collective response behaviour through coupling-weight parameter updates. Stacking transformer modules corresponds to building up a differentiable structure of correlations by using the spin expectation values of one physical system to drive the next one.\nIn this post, we flesh out the idea of looking at transformer modules as physical systems. Having identified vector spin systems as plausible physical models underlying transformers, we turn to 1960s statistical-mechanics literature to look for inspiration on how to deal with their partition functions1. We rediscover that the partition function of a particular class of vector-spin models can be approximated in the limit of large local spin dimension using steepest descent, leading to approximate yet tractable expressions for the free energy and other derived quantities.\nCombining these canonical results from statistical mechanics with modern differentiable programming, we implement a differentiable vector-spin model based on an approximate free-energy minimization algorithm. Internally, the model uses an implicit layer to solve for the stationary point of the partition function in a differentiable way. We then construct a transformer-like attention module which encapsulates the spin model by routing inputs to applied magnetic fields and spin expectation values to outputs. The latter are obtained by following the familiar recipe of statistical mechanics: differentiating the spin model\u0026rsquo;s $\\log Z$ with respect to conjugate input variables. Finally, we contextualize our approach by comparing it to vanilla transformers, deep equilibrium transformers, and deep implicit attention.\n✨ TL;DR: We consider transformer modules as wrappers around a differentiable steepest-descent approximation of simple Ising-like vector-spin models familiar from statistical mechanics. We observe that a blueprint of the successful transformer-like architectural pattern of token-mixing (attention) and channel-mixing (feed-forward) naturally emerges when computing spin expectation values in vector-spin models with input-dependent couplings. Feel free to skip to the final section for a visual comparison of this work to vanilla transformers, deep equilibrium transformers, and deep implicit attention.\n2. Massaging partition functions In this section, we set out to derive an approximate, analytical expression for the free energy of a classical disordered vector-spin system exposed to a site-dependent external magnetic field. In deriving the results below, we found inspiration in H. E. Stanley\u0026rsquo;s Spherical Model as the Limit of Infinite Spin Dimensionality (1968) and Chapter 5 of R. J. Baxter\u0026rsquo;s bible on Exactly Solved Models in Statistical Mechanics (1982).\n2.1. A vector-spin model and its partition function We start from the following Hamiltonian (or energy function) of a classical vector spin system of $N$ spins in a site-dependent external magnetic field,\n\\begin{equation} E = - \\sum_{i,j=1}^{N} J_{ij} \\; \\boldsymbol{\\sigma}_{i} \\cdot \\boldsymbol{\\sigma}_{j} - \\sum_{i=1}^{N} \\boldsymbol{h}_{i} \\cdot \\boldsymbol{\\sigma}_{i}, \\label{eq:vectrandomising} \\end{equation}\nwhere both $\\boldsymbol{\\sigma}_{i} = \\left[ \\sigma_{1}(i), \\sigma_{2}(i), \\ldots, \\sigma_{D}(i) \\right]$ and $\\boldsymbol{h}_{i} = \\left[ h_{1}(i), h_{2}(i), \\ldots, h_{D}(i) \\right]$ are vectors of dimension $D$. The coupling matrix $\\boldsymbol{J}$ is assumed to be traceless and symmetric but can otherwise have real elements with both negative and positive signs. We take the vector degrees of freedom $\\boldsymbol{\\sigma}_{i}$ to be constrained by a set of $N$ constraints\n\\begin{equation} \\lVert \\boldsymbol{\\sigma}_{i} \\rVert _{2}^{2} = \\sum_{a=1}^{D} \\sigma_{a}^{2}(i) = D, \\quad i = 1,2,\\ldots,N, \\end{equation}\nso that their magnitudes equal $\\sqrt{D}$. One can picture the classical spin degrees of freedom as arrows rotating along the surface of $(D-1)$-dimensional spheres at every site.\nIn statistical mechanics, the model Eq. \\eqref{eq:vectrandomising} is known as a vector model whose familiar small-$D$ cases include the Ising model ($D=1$), the XY model ($D=2$), and the Heisenberg model ($D=3$). For infinite-dimensional spins $D \\to \\infty$, one can show that the system approaches the spherical model. The model defined by \\eqref{eq:vectrandomising} can also be regarded as a vector generalization of Boltzmann machines or Hopfield networks or disordered Sherrington-Kirkpatrick spin-glass models (but with just a single sample of non-local couplings instead of an underlying probability distribution). Similar models also appear in recent studies on higher-dimensional generalizations of spin glass models2.\nThe partition function for our spin system looks like:\n\\begin{align} Z_{N}^{(D)} \u0026amp;\\left( \\beta, J_{ij}, \\{ \\boldsymbol{h}_{i} \\} \\right) \\nonumber \\\\ \u0026amp;= \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} \\ \\mathrm{d}\\sigma_{1}(1) \\cdots \\mathrm{d}\\sigma_{D}(N) \\nonumber \\\\ \u0026amp; \\qquad \\times \\ \\prod_{j=1}^{N} \\delta \\left( D - \\lVert \\boldsymbol{\\sigma}_{j} \\rVert _{2}^{2} \\right) \\nonumber \\\\ \u0026amp; \\qquad \\times \\exp \\left[ \\beta \\sum_{i,j=1}^{N} J_{ij} \\; \\boldsymbol{\\sigma}_{i} \\cdot \\boldsymbol{\\sigma}_{j} + \\beta \\sum_{i=1}^{N} \\boldsymbol{h}_{i} \\cdot \\boldsymbol{\\sigma}_{i} \\right] \\label{eq:fullpartfun} \\end{align}\nwhere we have made all dependencies explicit. This looks absolutely mental. We somehow need to find a way to do $N \\times D$ integrals while taking into account all the constraints and interactions.\n2.2. Peeking into a physicist\u0026rsquo;s bag of tricks Let\u0026rsquo;s first of all get rid of the explicit Dirac delta functions by substituting their complex integral representations\n\\begin{align} \\delta \\left( D - \\lVert \\boldsymbol{\\sigma}_{j} \\rVert _{2}^{2} \\right) = \\frac{\\beta}{2 \\pi i} \\int_{-i\\infty}^{i\\infty} \\mathrm{d} t_{j} \\exp \\left[ \\beta t_{j} \\left( D - \\lVert \\boldsymbol{\\sigma}_{j} \\rVert _{2}^{2} \\right) \\right] \\end{align}\nso that\n\\begin{align} Z_{N}^{(D)} \u0026amp;= \\left(\\frac{\\beta}{2 \\pi i}\\right)^{N} \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} \\ \\mathrm{d}\\sigma_{1}(1) \\cdots \\mathrm{d}\\sigma_{D}(N) \\nonumber \\\\ \u0026amp; \\times \\int_{-i\\infty}^{i\\infty} \\cdots \\int_{-i\\infty}^{i\\infty} \\ \\mathrm{d}t_{1} \\cdots \\mathrm{d}t_{N} \\ \\exp \\left( \\beta D \\sum_{j=1}^{N} t_{j} \\right)\\nonumber \\\\ \u0026amp; \\times \\prod_{\\alpha=1}^{D} \\exp \\left[ -\\beta \\sum_{i,j=1}^{N} \\left(t_{j}\\delta_{ij}-J_{ij}\\right) \\; \\sigma_{\\alpha}(i) \\sigma_{\\alpha}(j) + \\beta \\sum_{i=1}^{N} h_{\\alpha}(i) \\sigma_{\\alpha}(i) \\right] \\nonumber \\end{align}\nGreat, even more integrals. The next frustrating trick involves writing the number 1 as a judiciously chosen exponential,\n\\begin{align} \\exp \\left( \\beta \\sum_{j=1}^{N} a \\left( D - \\lVert \\boldsymbol{\\sigma}_{j} \\rVert _{2}^{2} \\right) \\right) = 1, \\end{align}\nfor some arbitrary constant $a$, which, inside the integral, indeed evaluates to $\\exp (0) = 1$ because of the constraints. Inserting this expression gives\n\\begin{align} \u0026amp;Z_{N}^{(D)} = \\left(\\frac{\\beta}{2 \\pi i}\\right)^{N} \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} \\ \\mathrm{d}\\sigma_{1}(1) \\cdots \\mathrm{d}\\sigma_{D}(N) \\nonumber \\\\ \u0026amp; \\times \\int_{-i\\infty}^{i\\infty} \\cdots \\int_{-i\\infty}^{i\\infty} \\ \\mathrm{d}t_{1} \\cdots \\mathrm{d}t_{N} \\ \\exp \\left( \\beta D \\sum_{j=1}^{N} \\left( t_{j} + a\\right) \\right)\\nonumber \\\\ \u0026amp; \\times \\prod_{\\alpha=1}^{D} \\exp \\left[ -\\beta \\sum_{i,j=1}^{N} \\left( \\left( t_{j} + a \\right) \\delta_{ij}-J_{ij}\\right) \\; \\sigma_{\\alpha}(i) \\sigma_{\\alpha}(j) + \\beta \\sum_{i=1}^{N} h_{\\alpha}(i) \\sigma_{\\alpha}(i) \\right] \\nonumber \\end{align}\nNext, we\u0026rsquo;d like to swap the order of the $\\mathrm{d}\\sigma_{a}(j)$ and $\\mathrm{d}t_{j}$ integrations to start integrating. But we are only allowed to do this if we assume $a$ to be a sufficiently large positive real number. Why? Essentially, we are deforming the contours of the complex integrals sufficiently far to the right such that the real part the quadratic form appearing in the exponential is positive definite, see e.g. Helfand \u0026amp; Langer (1967).\nLet\u0026rsquo;s go ahead and assume that everything is fine. We swap integrals and do a change of variables $t_j \\to t_j + a$ so that\n\\begin{align} Z_{N}^{(D)} \u0026amp;= \\left(\\frac{\\beta}{2 \\pi i}\\right)^{N} \\int_{a-i\\infty}^{a+i\\infty} \\cdots \\int_{a-i\\infty}^{a+ i\\infty} \\ \\mathrm{d}t_{1} \\cdots \\mathrm{d}t_{N} \\\\ \u0026amp; \\times \\exp \\left( \\beta D \\sum_{j=1}^{N} t_{j} \\right)\\nonumber \\prod_{\\alpha=1}^{D} I_{\\alpha} \\left( \\beta, \\{ t_{j} \\}, \\{ h_{\\alpha}(i) \\} \\right)\\nonumber \\end{align}\nwhere\n\\begin{align} I_{\\alpha} \u0026amp;\\left( \\beta, \\{ t_{j} \\}, \\{ h_{\\alpha}(i) \\} \\right) = \\int_{-\\infty}^{\\infty} \\cdots \\int_{-\\infty}^{\\infty} \\ \\mathrm{d}\\sigma_{\\alpha}(1) \\cdots \\mathrm{d}\\sigma_{\\alpha}(N) \\nonumber \\\\ \u0026amp; \\times \\exp \\left[ -\\beta \\sum_{i,j=1}^{N} \\left( t_{j} \\delta_{ij}-J_{ij}\\right) \\; \\sigma_{\\alpha}(i) \\sigma_{\\alpha}(j) + \\beta \\sum_{i=1}^{N} h_{\\alpha}(i) \\sigma_{\\alpha}(i) \\right]\\nonumber \\\\ \\end{align}\nNotice how the integrals have kind of factorized over the vector dimension: for every $\\alpha$-component we can evaluate an $N$-dimensional Gaussian integral with a linear term. The $I_{\\alpha}$ functions depend on the sources $\\{ \\boldsymbol{h}_{i} \\}$ indexed along local dimension instead of spin. Introducing the symmetric $N \\times N$ matrix $V_{ij} = t_{j} \\delta_{ij}-J_{ij}$, we can evaluate the Gaussian integrals and find\n\\begin{align} I_{\\alpha} \u0026amp;\\left( \\beta, \\{ t_{j} \\}, \\{ h_{\\alpha}(i) \\} \\right) = \\left( \\frac{\\pi}{\\beta} \\right)^{N/2} \\left[ \\det \\left( \\boldsymbol{V} \\right) \\right]^{-1/2} \\exp \\left(\\frac{\\beta}{4} \\boldsymbol{h}_{\\alpha}^{T} \\boldsymbol{V}^{-1} \\boldsymbol{h}_{\\alpha} \\right) \\nonumber \\\\ \\end{align}\nwhere $\\boldsymbol{h}_{\\alpha} = \\left[ h_{\\alpha}(1), h_{\\alpha}(2), \\ldots, h_{\\alpha}(N) \\right]$ denote $N$-dimensional vectors. The expression for the partition function becomes\n\\begin{align} \u0026amp;Z_{N}^{(D)} = \\left(\\frac{\\beta}{2 \\pi i}\\right)^{N} \\left( \\frac{\\pi}{\\beta} \\right)^{DN/2} \\int_{a-i\\infty}^{a+i\\infty} \\cdots \\int_{a-i\\infty}^{a +i\\infty} \\ \\mathrm{d}t_{1} \\cdots \\mathrm{d}t_{N} \\nonumber \\\\ \u0026amp; \\times \\exp \\left( D \\left( \\beta \\sum_{j=1}^{N} t_{j} - \\frac{1}{2} \\log \\det \\left( \\boldsymbol{V} \\right) \\right) \\right) \\exp \\left( \\frac{\\beta}{4} \\mathrm{Tr} \\left( \\boldsymbol{H}^{T} \\boldsymbol{V}^{-1} \\boldsymbol{H} \\right) \\right) \\nonumber \\end{align}\nwhere we have introduced the matrix notation $\\boldsymbol{H} \\in \\mathbb{R}^{N \\times D}$ to group the vectors $\\{ \\boldsymbol{h}_{i} \\}$.\n2.3. Steepest descent: hunting for the saddle But there\u0026rsquo;s still $N$ complex integrals over the auxiliary variables $\\{ t_{j} \\}$ left to do. Can we avoid doing them? Maybe. Let\u0026rsquo;s rewrite our partition function as\n\\begin{align} Z_{N}^{(D)} = \\left(\\frac{\\beta}{2 \\pi i}\\right)^{N} \u0026amp;\\left( \\frac{\\pi}{\\beta} \\right)^{DN/2} \\int_{a-i\\infty}^{a+i\\infty} \\cdots \\int_{a-i\\infty}^{a +i\\infty} \\ \\mathrm{d}t_{1} \\cdots \\mathrm{d}t_{N} \\ \\mathrm{e}^{D \\varphi \\left(\\boldsymbol{t} \\right) } \\label{eq:partfunsteep} \\\\ \\end{align}\nwith\n\\begin{align} \\varphi \\left(\\boldsymbol{t}; \\beta, J_{ij} \\right) = \\beta \\sum_{j=1}^{N} t_{j} - \\frac{1}{2} \\log \\det \\left( \\boldsymbol{V} \\right) + \\frac{\\beta}{4D} \\mathrm{Tr} \\left( \\boldsymbol{H}^{T} \\boldsymbol{V}^{-1} \\boldsymbol{H} \\right) \\label{eq:varphi} \\end{align}\nAs $D \\to \\infty$, the method of steepest-descent or the saddle-point method suggests that the partition function will be dominated by its largest contribution, i.e. in the neigbourhood of the maximum $\\varphi(\\boldsymbol{t^{*}})$ along the integration paths.\n ✨ Hmm, this doesn\u0026rsquo;t quite seem right #1: What does $D \\to \\infty$ even look like for the last term in Eq. \\eqref{eq:varphi}? What does it mean for the input vectors $\\{ \\boldsymbol{h}_{i} \\}$ to become infinite-dimensional? Good points, but let\u0026rsquo;s carry on.\n The saddle-point values $\\boldsymbol{t^{*}}$ are obtained from the set of stationary conditions\n\\begin{align} \\frac{\\partial \\varphi \\left( \\boldsymbol{t} \\right)}{\\partial t_j} \\Biggr\\rvert_{t_j = t^{*}_{j}} = 0, \\qquad j=1,\\ldots,N \\label{eq:statcond} \\end{align}\n ✨ Hmm, this doesn\u0026rsquo;t quite seem right #2: In the single-variable case, Baxter (1985) argues that $\\varphi (t)$ is analytic for $\\mathrm{Re}(t)\u0026gt;0$ and that we should consider $\\varphi (t)$ first for $t$ real and positive. For positive $\\beta$ and non-zero magnetic field, the function tends to plus infinity as $t$ tends to either zero or infinity. Thus in between $\\varphi(t)$ must have a minimum at some positive value $t^{*}$ of $t$. Since $\\varphi''(t) \u0026gt; 0$ there is also only one such minimum. If we take the constant $a$ in the integral limits to be $t^{*}$, then along the (imaginary) integration path $\\varphi (t)$ has a maximum at $t=t^{*}$. We naively assume that this kind of saddle-point reasoning transfers to our case in several complex variables with $\\varphi : \\mathbb{C}^{N} \\to \\mathbb{C}$ where the equivalent of $\\mathrm{Re}(t)\u0026gt;0$ is to try to steer clear of the singularity at $\\det \\left( \\boldsymbol{V} \\right)=0$. We will check the numerical behaviour of our $\\varphi$-function in Section 3.1.\n Expanding $\\varphi$ around $\\boldsymbol{t^{*}}$ and then taking the logarithm of Eq. \\eqref{eq:partfunsteep} leads to\n\\begin{align} \\ln Z_{N}^{(D)} = \\frac{DN}{2} \\ln \\left( \\frac{\\pi}{\\beta} \\right) + D \\varphi \\left( \\boldsymbol{t^{*}} \\right) + \\ln R \\nonumber \\end{align}\nwhere we have collected all higher-order contributions and remaining nastiness in $R$. Following Stanley (1968), the free energy in the limit of large local dimension $D \\to \\infty$ then becomes\n\\begin{align} -\\beta f_{N}^{(\\infty)} = \\lim_{D \\to \\infty} D^{-1} \\ln \\left( Z_{N}^{(D)} / Z_{N}^{(D)}(0) \\right) \\nonumber \\end{align}\nwhere\n\\begin{align} Z_{N}^{(D)}(0) = \\left( \\left(\\pi\\right)^{D/2} D^{(D-1)/2} / \\Gamma \\left(D/2\\right) \\right)^{N} \\nonumber \\end{align}\nis a normalization factor3 accounting for the surface areas of the $(D-1)$-dimensional spheres with radius $\\sqrt{D}$ associated to each and every spin degree of freedom. After applying Stirling\u0026rsquo;s asymptotic expansion to the $\\Gamma$-function in the normalization factor and doing some algebra, we end up with\n\\begin{align} \\boxed{-\\beta f_{N}^{(\\infty)} = - \\frac{N}{2} - \\frac{N}{2} \\ln \\left( 2\\beta \\right) + \\varphi \\left( \\boldsymbol{t^{*}} \\right)} \\label{eq:afe} \\end{align}\nwhere we have dropped the last term $\\lim_{D \\to \\infty} D^{-1} \\ln R$ assuming it tends to zero. Since $\\varphi \\left( \\boldsymbol{t^{*}} \\right) \\propto N$, the last term actually also survives the limit $N \\to \\infty$.\n2.4. Taking stock of what we have done We have derived a closed-form expression Eq. \\eqref{eq:afe} for the approximate free energy of a vector-spin model in the limit of large local spin dimension. Let us take a brief moment to reflect on what we have done and touch on some tangential points.\n2.4.1. Questioning steepest descent and the large-$D$ limit The result \\eqref{eq:afe} is only sensible if steepest descent is a valid thing to do, which depends on how outrageous the landscape defined by the $\\varphi$-function \\eqref{eq:varphi} really is. More practically, we will also never really let the vector-spin dimension $D$ tend towards infinity since our goal is to implement a numerical attention-like neural network module. So large but finite vector dimensions better behave as if they were sufficiently close to infinity. We will find out in Section 3.1 to what extent these assumptions are valid in practice.\n2.4.2. Energy-based models and effective energy functions Let us take another look at our model\u0026rsquo;s partition function \\eqref{eq:fullpartfun} from an energy-based perspective. For ease of notation, let us call the model parameters $\\theta \\equiv \\{ J_{ij} \\}$, the spins $\\sigma \\equiv \\{ \\boldsymbol{\\sigma}_{i} \\}$, and the external magnetic fields $h \\equiv \\{ \\boldsymbol{h}_{i} \\}$. We can schematically write our model\u0026rsquo;s partition function as\n\\begin{align} Z_{\\theta} \\left( h \\right) = \\int \\mathrm{d} \\sigma \\ \\mathrm{e}^{ - E_{\\theta}\\left( \\sigma, h \\right) } \\end{align}\nwhere $E_{\\theta}\\left( \\sigma, h \\right)$ denotes the energy function Eq. \\eqref{eq:vectrandomising}. If we now introduce an energy-based model $p_{\\theta} \\left( \\sigma, h \\right) = \\mathrm{e}^{-E_{\\theta}\\left( \\sigma, h \\right)} / Z_{\\theta}$, we can define the marginal distribution\n\\begin{align} p_{\\theta} \\left( h \\right) = \\frac{\\int \\mathrm{d} \\sigma \\ \\mathrm{e}^{-E_{\\theta}\\left( \\sigma, h \\right)}}{Z_{\\theta}} = \\frac{\\mathrm{e}^{-E_{\\theta}\\left( h \\right)}}{Z_{\\theta}} \\label{eq:ph} \\end{align}\nwhere the applied magnetic fields act as observables and the spins as latent variables. The effective energy $E_{\\theta}\\left( h \\right)$ equals $E_{\\theta}\\left( h \\right) = - \\log \\int \\mathrm{d} \\sigma \\ \\mathrm{e}^{-E_{\\theta}\\left( \\sigma, h \\right)} \\approx - \\log Z^{\\ast}_{\\theta} \\left( h \\right)$, where we have used the steepest-descent approximation for the integral. Taking the logarithm of Eq. \\eqref{eq:ph}, we find that $\\log p_{\\theta} \\left( h \\right) \\approx \\log Z^{\\ast}_{\\theta} \\left( h \\right) - \\log \\int \\mathrm{d} h \\ Z^{\\ast}_{\\theta} \\left( h \\right)$.\n2.4.3. Spin glasses and mean-field approximation Ordered systems have a long history in statistical mechanics. Couplings in these models often encode a translation-invariant lattice geometry, e.g. nearest-neighbour interactions between spins living on a $d$-dimensional hypercubic lattice. One reason for this focus is practical: the regularity in these systems enables mathematical physicists to deploy all kinds of tricks and make progress towards some kind of understanding. In contrast, disordered systems, like spin glasses, are a mess and studying them is all about finding order where there seems to be none. From the perspective of spin glasses, we can summarize our approach as follows: we want to arrive at an approximate yet tractable mean-field spin-glass model where its couplings are treated as parameters learned from data4.\nFully-connected models like Sherrington-Kirkpatrick spin-glass models (or Eq. \\eqref{eq:vectrandomising}) naturally lead to mean-field theory because the couplings $J_{ij}$ encode long-range interactions where every other spin is just a hop away, see e.g. Janiš (2015). Intuitively, all-to-all interactions correspond to the mean-field limit of infinite spatial dimension. To see this, consider a spin in a local nearest-neighbour lattice model getting ever more neighbours as the spatial dimension grows: the notion of nearest neighbours melts away and all spins effectively become connected to each other5. Fully-connected non-local couplings and the limit of infinite spatial dimension are two sides of the same mean-field coin.\n3. Implementing approximate free-energy minimization  ✨ Code: A PyTorch implementation of the ideas outlined in this blog post is available in the GitHub repository mcbal/afem.\n In this section, we turn the equations of the previous section into the algorithmic backbone of a differentiable vector-spin model. We begin by sketching an approximate free-energy minimization algorithm. We then show how to wrap around the spin model to turn it into an attention module.\n3.1. The algorithm: bold moves on a tricky landscape Our goal is to compute the steepest-descent approximation of our model\u0026rsquo;s partition function in a differentiable way. Essentially, we need to solve the set of equations\n\\begin{align} \\frac{\\partial \\varphi \\left( \\boldsymbol{t} \\right)}{\\partial t_j} \\Biggr\\rvert_{t_j = t^{*}_{j}} = 0, \\qquad j=1,\\ldots,N \\end{align}\nwhich corresponds to finding a value $\\boldsymbol{t^{*}} = \\mathrm{argmin}_{\\boldsymbol{t}} \\varphi \\left( \\boldsymbol{t} \\right)$ for which the scalar function\n\\begin{align} \\varphi \\left(\\boldsymbol{t}; \\beta, J_{ij} \\right) = \\beta \\sum_{j=1}^{N} t_{j} - \\frac{1}{2} \\log \\det \\left( \\boldsymbol{V} \\right) + \\frac{\\beta}{4D} \\mathrm{Tr} \\left( \\boldsymbol{H}^{T} \\boldsymbol{V}^{-1} \\boldsymbol{H} \\right) \\nonumber \\end{align}\nattains its minimum, or, equivalently, we need to solve for the root of $\\nabla \\varphi \\left( \\boldsymbol{t} \\right)$.\n3.1.1. Initialization and normalization Until now we have not been explicit about the values of the couplings $\\boldsymbol{J}$ and inputs $\\boldsymbol{H}$. If we want to implement any of this, we have to be more careful. Recall that the energy function of our model looks like $ E = - \\sum_{i,j=1}^{N} J_{ij} \\; \\boldsymbol{\\sigma}_{i} \\cdot \\boldsymbol{\\sigma}_{j} - \\sum_{i=1}^{N} \\boldsymbol{h}_{i} \\cdot \\boldsymbol{\\sigma}_{i},\\nonumber $ where all spins $\\boldsymbol{\\sigma}_{i}$ are fixed to norm $\\sqrt{D}$. We\u0026rsquo;d like this energy to remain linearly proportional to the the number of lattice sites. Numerically, we observe that stable root-finding is possible when initializing the couplings according to \\begin{equation} J_{ij} \\sim \\mathcal{N} (0, 1/\\sqrt{ND} ) \\end{equation} The factor $1/\\sqrt{N}$ can be explained from spin-glass mean-field theory6 whereas the $1/\\sqrt{D}$ factor follows from additionally normalizing with respect to the vector dimension to ensure $\\sum_{i,j=1}^{N} J_{ij} \\; \\boldsymbol{\\sigma}_{i} \\cdot \\boldsymbol{\\sigma}_{j} \\sim \\mathcal{O}(N)$. One strategy to normalize the inputs $\\boldsymbol{H}$ is to feed them into a layer normalization layer so that $\\left\\lVert \\boldsymbol{h}_{i} \\right\\rVert \\sim \\mathcal{O}(\\sqrt{D})$ and then explicitly dividing by $\\sqrt{D}$ to make them $\\mathcal{O}(1)$. A practical consequence of these initialization and normalization choices at the level of the energy function is that the $\\varphi$-function changes to\n\\begin{align} \\varphi \\left(\\boldsymbol{t}; \\beta, J_{ij} \\right) = \\beta \\sum_{j=1}^{N} t_{j} - \\frac{1}{2} \\log \\det \\left( \\boldsymbol{V} \\right) + \\frac{\\beta}{4} \\mathrm{Tr} \\left( \\boldsymbol{H}^{T} \\boldsymbol{V}^{-1} \\boldsymbol{H} \\right) \\label{eq:varphinorm} \\end{align}\nwhere the prefactor in the last term changed since we decided on explicitly dividing the layer-normalized $\\boldsymbol{H}$ by $1/\\sqrt{D}$.\n3.1.2. Implicit layers for steepest-descent root-finding Let\u0026rsquo;s now find the root of the gradient of $\\varphi$ in a differentiable way by combining implicit layers with a black-box root-finding algorithm like Newton\u0026rsquo;s method, which requires access to both a function (the gradient of $\\varphi$) and its gradient (the Jacobian of the gradient of $\\varphi$). We could rely on automatic differentiation to calculate these gradients, but we just as well exploit the fact that we have an analytical expression Eq. \\eqref{eq:varphinorm}. Grabbing a coffee and peeking at the Matrix Cookbook, we can figure out what happens\n when we wiggle around $t_{i}$ (the gradient vector at $\\boldsymbol{t}$)  \\begin{align} \\left[ \\nabla \\varphi \\left( \\boldsymbol{t} \\right) \\right]_{i} = \\beta - \\frac{1}{2} \\left[ \\boldsymbol{V}^{-1} \\right]_{ii} - \\frac{\\beta}{4} \\left[ \\boldsymbol{V}^{-T} \\boldsymbol{H} \\boldsymbol{H}^{T} \\boldsymbol{V}^{-T} \\right]_{ii} \\nonumber \\end{align}\n when we wiggle around both $t_{i}$ and $t_{j}$ (the symmetric Hessian matrix at $\\boldsymbol{t}$)  \\begin{align} \\left[ \\boldsymbol{J}(\\nabla \\varphi \\left( \\boldsymbol{t} \\right)) \\right]_{ij} = \\frac{1}{2} \u0026amp;\\left[ \\boldsymbol{V}^{-1} \\odot \\boldsymbol{V}^{-T} \\right]_{ij} \\nonumber \\\\ \u0026amp;+ \\frac{\\beta}{4} \\left[ \\boldsymbol{V}^{-T} \\boldsymbol{H} \\boldsymbol{H}^{T} \\boldsymbol{V}^{-T} \\boldsymbol{V}^{-T} \\odot \\boldsymbol{I} \\right]_{ij} \\nonumber \\\\ \u0026amp;+ \\frac{\\beta}{4} \\left[ \\boldsymbol{V}^{-T} \\boldsymbol{V}^{-T} \\boldsymbol{H} \\boldsymbol{H}^{T} \\boldsymbol{V}^{-T} \\odot \\boldsymbol{I} \\right]_{ij} \\nonumber \\end{align}\nGiven an initial guess $\\boldsymbol{t_{0}} \\in \\mathbb{R}^{N}_{\u0026gt;0}$ and input data $\\boldsymbol{H} \\in \\mathbb{R}^{N \\times D}$, we can now construct a differentiable root-solver which returns $\\boldsymbol{t^{*}}$. It is important to keep in mind that the stationary value $\\boldsymbol{t^{*}}$ actually depends on $\\left(\\beta, \\boldsymbol{J}, \\boldsymbol{H} \\right)$ implicitly. Since we make use of implicit layers within an automatic differentation framework, these dependencies are kept track of and are included in the computational graph.\n3.1.3. Fun with free energies Let\u0026rsquo;s test the algorithm by initializing a random vector-spin model and applying a random magnetic field at every site. For visualization purposes, we restrict the auxiliary variables to be effectively one-dimensional by defining $\\boldsymbol{t} = t \\boldsymbol{1}_{N}$ with just a single scalar parameter $t \\in \\mathbb{R}_{\u0026gt;0}$. We can probe a VectorSpinModel and get the approximate free energy for a given set of parameters and inputs by running the following script:\nfrom afem.models import VectorSpinModel num_spins, dim = 32, 128 model = VectorSpinModel(num_spins=num_spins, dim=dim, beta=1.0) x = (torch.randn(1, num_spins, dim) / np.sqrt(dim)).requires_grad_() t0 = torch.ones(1) afe = model(x, t0, return_afe=True).afe  Inside the forward pass, the root $\\boldsymbol{t^{*}}$ is computed and then fed into Eq. \\eqref{eq:afe} to calculate the approximate free energy. We can verify that our algorithm is doing something sensible by sweeping across the auxiliary $t$-values and plotting $\\varphi$ and its derivatives:\nThe region close to $t=0$ looks terrifying. In this regime, $t$ is likely not large enough to overshadow the largest eigenvalue of the couplings so we lose positive definiteness and its nice properties. Let\u0026rsquo;s try to stay away from that region by always initializing $\\boldsymbol{t}_{0}$ sufficiently far from it. Depending on the parameters and initial guess provided to the solver, one can of course end up in less favourable landscapes where root-solving can become difficult due to zero gradients or extreme sensitivity to initial conditions. Fortunately, when the root-solving step fails, it tends to fail spectacularly.\nLet\u0026rsquo;s now sweep across inverse temperature $\\beta$ to get some intuition. From the analytical expression of the free energy, we can deduce that for small $\\beta$ (high temperature) the entropy term reigns while for large $\\beta$ (low temperature) the energy terms take over.\nFinally, let\u0026rsquo;s lift the one-dimensional restriction on $\\boldsymbol{t}$ and plot $\\varphi (\\boldsymbol{t})$ for two spins. In that case, $\\boldsymbol{t}$ is also just two-dimensional so we can still visualize the optimization landscape.\n3.2. The attention module: probing spins with data In the previous section, we showed how to numerically compute the steepest-descent approximation of a vector-spin model\u0026rsquo;s partition function and hence its free energy. Since this approximation is fully differentiable, we can also take derivatives with respect to conjugate variables. Let\u0026rsquo;s use this observation to construct an attention module.\n3.2.1. Spin expectation values We can calculate spin expectation values or magnetizations from our partition function approximation by differentiating with respect to the applied magnetic fields:\n\\begin{align} \\langle \\boldsymbol{\\sigma}_{i} \\rangle = \\frac{\\mathrm{d} \\log Z \\left( \\boldsymbol{t}, \\boldsymbol{H} \\right)}{\\mathrm{d} \\boldsymbol{h}_{i}} = \\frac{\\partial \\varphi}{\\partial \\boldsymbol{t}} \\frac{\\partial \\boldsymbol{t}}{\\partial \\boldsymbol{h}_{i}} + \\frac{\\partial \\varphi}{\\partial \\boldsymbol{h}_{i}} \\label{eq:spinevgeneral} \\end{align}\nIf we evaluate the partition function approximation at the stationary point $\\boldsymbol{t^{\\ast}}$, the first term drops out because $\\partial_{\\boldsymbol{t}} \\varphi \\rvert_{\\boldsymbol{t}=\\boldsymbol{t^{\\ast}}} = 0$. Assuming that the matrix $\\boldsymbol{V}$ (and hence the couplings $\\boldsymbol{J}$) do not depend on the inputs $\\boldsymbol{H}$, the spin expectation value boils down to\n\\begin{align} \\langle \\boldsymbol{\\sigma}_{i} \\rangle = \\frac{\\partial \\varphi}{\\partial \\boldsymbol{h}_{i}} = \\frac{\\beta}{2} \\sum_{j} \\boldsymbol{V}^{-1}_{ij} \\boldsymbol{h}_{j} \\label{eq:spinev} \\end{align}\nwhich, for every site, is just a weighted sum of inputs. In the language of transformers, Eq. \\eqref{eq:spinev} resembles an update step where $\\boldsymbol{V}^{-1}$ can be interpreted as a symmetric attention matrix. Expanding the matrix inverse reveals a residual connection as the zero-th order contribution7.\nSince the couplings are scalars at the level of the energy function Eq. \\eqref{eq:vectrandomising}, getting terms to act on the hidden dimension seems to be impossible. But by considering couplings $\\boldsymbol{J}(\\boldsymbol{H})$ which do depend on inputs, additional terms can appear in Eq. \\eqref{eq:spinev} propagating via dependencies in $\\boldsymbol{V}$. Instead of calculating these gradients analytically, we should of course just let our automatic differentiation framework compute them for us.\n3.2.2. Wrapping around the spin model At this point, we have done all the heavy lifting. All that remains is to write a wrapper so that we can use our module just like any other explicit attention module:\nfrom afem.attention import VectorSpinAttention num_spins, dim = 32, 128 attention = VectorSpinAttention(num_spins=num_spins, dim=dim, beta=1.0) x = torch.randn(1, num_spins, dim).requires_grad_() attention(x) # (1, 32, 128)  Inside the forward pass of VectorSpinAttention, (normalized) inputs are sent to an internal VectorSpinModel which solves for the saddle point $\\boldsymbol{t^{*}}$ and then feeds it into the steepest descent partition function to calculate magnetizations according to Eq. \\eqref{eq:spinevgeneral}.\nLet\u0026rsquo;s finish this section by discussing some of the peculiarities of our approach:\n Stability and symmetry: The root-finding is stable as long as $\\det \\boldsymbol{V} \u0026gt; 0$, which ensures that $\\boldsymbol{V}$ is nonsingular and which is garantueed as long as the quadratic form is positive definite. A quadratic form involving a general $\\boldsymbol{V}$ (i.e. with nonsymmetric couplings $\\boldsymbol{J}$) is positive definite iff its symmetric part has all positive eigenvalues. When this is no longer the case, things tend to blow up. Scaling: Our approach is kind of slow because calculating inverses scales as $\\mathcal{O}\\left(N^3\\right)$. Yet there might be ways to approximate the slow parts of the algorithm similar to how vanilla transformers can be understood to approximate mean-field fixed-point equations8. Lack of permutation invariance: Our model is not permutation invariant with the default choice of input-independent couplings: every spin has a role to play. Input-dependent couplings: Because our default model assumes coupling-independent couplings $\\boldsymbol{J}$, Eq. \\eqref{eq:spinev} features just a \u0026ldquo;token-mixing\u0026rdquo; attention operation. Channel-mixing terms can appear when we consider the physically very weird setup where the couplings are made dependent on the applied magnetic fields. One possible choice could be: \\begin{align} \\boldsymbol{J}(\\boldsymbol{H}) = \\frac{\\tanh \\left( \\boldsymbol{H} \\boldsymbol{Q} \\boldsymbol{K}^T \\boldsymbol{H}^T \\cdot \\sqrt{D} \\right)}{\\sqrt{ND}} \\nonumber \\end{align} where $\\boldsymbol{Q}$ and $\\boldsymbol{K}$ are linear transformations acting on the hidden dimension and where the scaling factors have been inserted because of the normalization conventions we discussed in Section 3.1.1. We hypothesize that additional terms in the spin expectation value Eq. \\eqref{eq:spinev} arising from input-dependent couplings might be related to channel-mixing feed-forward networks in transformer modules.  3.2.3. Comparison with vanilla transformers In this final section, let\u0026rsquo;s summarize our approach on a high level by visually comparing it to vanilla transformers and deep equilibrium approaches.\nThe vanilla transformer [Vaswani et al. (2017)] (left above) is an explicit architecture which processes input sequences sequentially through a stack of transformer modules. Deep equilibrium transformers [Bai et al. (2019)] (right above) compute the output of a transformer module by implicitly solving for the fixed point of $f(z, x) = z$ where $f$ denotes the explicit transformer module. Data is repeatedly inserted by adding it to the current iteration of $z$ inside the module until fixed-point convergence. The converged fixed point is considered the output of the module. Backpropagation through the iterations of the solver is avoided by using the implicit function theorem to calculate gradients directly at the equilibrium point. Instead of a stack of layers, there\u0026rsquo;s just a single layer.\nBut deep equilibrium transformers still treat the transformer module as a black box. In Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms we looked for a physical spin-model interpretation of the deep equilibrium fixed-point procedure (left below). We argued how the update step of a vanilla transformer module resembled mean-field fixed-point equations of a vector-spin model, explaining the successful pattern of token-mixing, residual connections, normalization layers, and feed-forward or channel-mixing modules from a physical spin systems' perspective.\nIn this work (right above), we continued on the path of spin expectation values but replaced solving mean-field fixed-point equations with directly taking derivatives of the steepest-descent partition function of a particular class of vector-spin models. The fixed-point procedure is replaced with a root-solving step to determine the steepest-descent partition function. The structure of our module\u0026rsquo;s output reveals the same successful transformer-like pattern of token-mixing (attention) and channel-mixing (feed-forward) interspersed with normalization layers and residual connections.\n4. Conclusion In this post, we introduced transformer modules as wrappers around statistical-mechanical vector-spin models. We used implicit layers to construct a class of approximate yet tractable vector-spin models whose couplings act as parameters that can be learned from data. We showed how these models can act as transformer-like attention modules by routing inputs to applied magnetic fields and returning spin expectation values derived from their steepest-descent partition function.\nBy zooming out and approaching transformers from a tangential, statistical-mechanical point of view, we were able to develop a physical intuition of transformers that seems hard to arrive at when restricting oneself to perturbing explicit neural network architectures. Recognizing transformer modules as spin models in disguise might not only unify architectural variations but also elucidate the high-level architectural convergence and empirical success of transformers in deep learning.\nReferences \u0026amp; footnotes If you happen to find this work useful, please consider citing it as:\n@article{bal2021afem, title = {Transformers from Spin Models: Approximate Free Energy Minimization}, author = {Bal, Matthias}, year = {2021}, month = {October}, url = {https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/} }    We could have turned to the mean-field free energies associated with the adaptive TAP equations discussed in Deep Implicit Attention, but we decided on attacking the problem from the steepest-descent angle on the full partition function.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For example, see The Free Energy of Spherical Vector Spin Glasses (Ko, 2018) and Free Energy in the Mixed p-spin Models With Vector Spins (Panchenko, 2015).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The original 1968 paper has a small typo here: the $\\nu$ in the paper\u0026rsquo;s Eq. (23) should be $\\nu^{1/2}$ for the surface area of a $\\nu-1$-dimensional sphere with radius $R=\\nu^{1/2}$ embedded in $\\nu$ dimensions. Using the paper\u0026rsquo;s formula, an annoying $\\ln \\nu$ term won\u0026rsquo;t cancel out in the limiting free energy calculation.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n In contrast to spin glasses however, we do not (yet want to go full Bayesian and) treat the couplings as drawn from some kind of probability distribution. For now, we settle for obtaining point estimates of model parameters.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n By promoting sparseness in the couplings, a model might become less mean-field-y, which might be one of the reasons behind the sucess of scaled softmax attention in vanilla transformers.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n From Janiš (2015): The mean-field limit to infinite dimensions or long-range interaction introduces a new large scale. To make the thermodynamic limit meaningful the dependence of the energy on this new large scale must be compensated by rescaling the non-local spin exchange so that the energy remains linearly proportional to the volume or the number of lattice sites (spins).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n We can expand the right-hand side using a special case of the Woodbury matrix identity to find \\begin{align} \\boldsymbol{V}^{-1} \u0026amp;= \\left( \\mathrm{diag} ( \\boldsymbol{t} ) - \\boldsymbol{J} \\right)^{-1} = \\sum_{k=0}^{\\infty} \\left( \\mathrm{diag} \\left( \\boldsymbol{t}^{-1} \\right) \\boldsymbol{J} \\right)^{k} \\mathrm{diag} \\left( \\boldsymbol{t}^{-1} \\right) \\nonumber \\end{align} which converges if the largest absolute value of the eigenvalues of the matrix inside the power-brackets is less than 1. So the spin expectation value looks like a sum of contributions that mix and weigh inputs of different sites.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n As discussed previously in Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms. In that setting, calculating inverses was sidestepped by approximating part of the solution with a feed-forward neural network.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1634060417,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634064017,"objectID":"5dda45f8610ff13e12238920f8f3fba4","permalink":"https://mcbal.github.io/post/transformers-from-spin-models-approximate-free-energy-minimization/","publishdate":"2021-10-12T18:40:17+01:00","relpermalink":"/post/transformers-from-spin-models-approximate-free-energy-minimization/","section":"post","summary":"How far can we push the idea of transformers as physical systems?","tags":["Artificial Intelligence","Associative Memories","Attention","Boltzmann Machine","Deep Learning","Emergent Collective Computational Capabilities","Free Energy","Hopfield Networks","Ising Models","Many-Body Systems","Neural Networks","Partition Function","Saddle Point","Statistical Physics","Steepest Descent","Transformers","Vector-Spin Models"],"title":"Transformers from Spin Models: Approximate Free Energy Minimization","type":"post"},{"authors":[],"categories":[],"content":" ✨ Update (November 2021): Consider reading Transformers Are Secretly Collectives of Spin Systems for a high-level overview of some of the ideas outlined in this post.\n  Introduction Mean-field theory for disordered systems  Random Ising models (or Boltzmann machines or \u0026hellip;) Adaptive Thouless\u0026ndash;Anderson\u0026ndash;Palmer mean-field theory   Attention as a fixed-point method  Generalizing spin models to vector degrees of freedom Deep implicit attention: attention as a collective response Slow and explicit: solving the adaptive TAP equations Fast and neural: parametrizing the Onsager self-correction term   A mean-field theory perspective on transformers  Parametrizing the couplings: sparse graph structure from inputs Softmax attention does a single, naive mean-field update step Feed-forward layer corrects naive mean-field update Mean-field theory framework for transformer architectures Comparison with energy-based perspective   Conclusion and outlook Related work   1. Introduction  ✨ Code: A reference PyTorch implementation of the ideas outlined in this blog post is available in the repository mcbal/deep-implicit-attention. Comments welcome.\n To explore progress beyond the cage of softmax attention, we have previously looked at energy-based perspectives on attention mechanisms:\n An Energy-Based Perspective on Attention Mechanisms in Transformers Transformer Attention as an Implicit Mixture of Effective Energy-Based Models Attention as Energy Minimization: Visualizing Energy Landscapes  The main take-away so far has been that you can think of softmax attention as implementing a single, big gradient step of some energy function and that training transformers is akin to meta-learning how to best tune a stack of attention and feed-forward modules to perform well on some auxiliary (meta-)task(s). But what can an energy-based perspective actually provide beyond quaint and hand-wavy statements like implicit energy landscapes are sculpted every time you train a transformer?\nIn this post, we approach attention in terms of the collective response of a statistical-mechanical system. Attention is interpreted as an inner-loop fixed-point optimization step which returns the approximate response of a system being probed by data. This response is a differentiable compromise between the system\u0026rsquo;s internal dynamics and the data it\u0026rsquo;s being exposed to. To better respond to incoming data, outer-loop optimization steps can nudge the interactions and the self-organizing behaviour of the system.\nTo implement our proposal, we combine old ideas and new technology to construct a family of attention mechanisms based on fixed points. We use deep equilibrium models to solve a set of self-consistent mean-field equations of a vector generalization of the random Ising spin-model. By approximating these equations, we arrive at simplified update steps which mirror the vanilla transformer architecture. We conclude by showing how transformers can be understood from a mean-field theory perspective.\n2. Mean-field theory for disordered systems In physics, mean-field theory is an approximation method to study models made up of many individual degrees of freedom that interact with each other. Mean-field theory approximates the effect of the environment on any given individual degree of freedom by a single, averaged effect, and thus reduces a many-body problem to an (effective) one-body problem. This is a drastic approximation. Whether mean-field theory a sensible thing to do depends on the problem and the properties of your variational ansatz.\n Mean-field theory \u0026amp; variational methods: From the point of view of variational methods, mean-field theory tries to approximate a complicated object (like a partition function of a statistical-mechanical system) by wiggling around the parameters of a tractable variational ansatz to get as close as possible to the real thing. You can picture this process as projecting down a complicated object living in a high-dimensional space to its shadow in an easier-to-handle subspace (I can hear a mathematician fainting in the background). This effectively reduces the problem to optimizing for the best possible approximation within your variational class. A lot of mean-field machinery also shows up in probability theory, statistics, and machine learning where it appears in belief propagation, approximate variational inference, expectation propagation, etc.\n In the next two subsections, we introduce random Ising models and sketch a physics-inspired approach to deal with disordered models using mean-field theory. In Section 3 we will then generalize these results to vector spin degrees of freedom and propose two flavours of attention models.\n2.1. Random Ising models (or Boltzmann machines or \u0026hellip;) The random Ising model is a prototypical model in the study of spin glasses and disordered random systems, where it is often referred to as the Sherrington–Kirkpatrick model, famous for its replica-method solution by Giorgio Parisi in 1979. Its energy function with external field for $N$ classical, binary spin variables looks like\n\\begin{equation} E = \\sum_{i,j} J_{ij} S_{i} S_{j} + \\sum_{i} x_{i} S_{i}, \\label{eq:randomising} \\end{equation}\nwhere the couplings $J_{ij}$ between degrees of freedom are randomly distributed according to some probability distribution and self-interactions are absent ($J_{ii} = 0$). The external magnetic fields $x_{i}$ provide a preferential direction of alignment at every local site. Since the elements in the coupling matrix can have both negative and positive signs, the system is said to have both frustrated ferro- as well as antiferromagnetic couplings. The model defined by \\eqref{eq:randomising} is also known as a Boltzmann machine or a Hopfield network.\nIn contrast with disordered systems, we expect the couplings in the context of artificial neural networks to no longer be randomly drawn from a distribution but to reflect structure and organization between spins after being exposed to data. The system should self-organize in order to better respond to incoming data.\nA cartoon of a spin configuration of a 7-spin system looks something like where we have only drawn the connections strongest in absolute value. It\u0026rsquo;s helpful to think of classical spin degrees of freedom as arrows. For vector spins, we can imagine lifting the up/down restriction and letting the arrows rotate freely.\n2.2. Adaptive Thouless\u0026ndash;Anderson\u0026ndash;Palmer mean-field theory One of the approaches physicists have come up with to tackle disordered random systems with pairwise interactions like those in Eq. \\eqref{eq:randomising} is Thouless\u0026ndash;Anderson\u0026ndash;Palmer (TAP) mean-field theory (1977). The TAP equations improve mean-field theory results by adding a so-called Onsager self-correction term calculated from the couplings' distribution.\nOpper and Winther (2001) adapted this method to probabilisic modeling to be able to deal with scenarios where the distribution of the couplings between spins is not known a priori. To compensate for the lack of knowledge of the couplings distribution, they introduced a self-consistent computation to adapt the Onsager correction to the actual couplings using the cavity method and linear response relations. We will sketch the adaptive TAP approach below but refer to Opper and Winther (2001) and Raymond, Manoel, and Opper (2014) for more details and derivations.\nSingle-site partition function from cavity method The adaptive TAP equations can be derived using the cavity method, where a cavity field distribution is introduced to rewrite the marginal distributions of the spins. The cavity corresponds to the \u0026ldquo;hole\u0026rdquo; left by removing a single spin. By assuming a Gaussian cavity distribution in the large connectivity limit, one can show that the single-site partition function looks like\n\\begin{equation} Z_{0}^{(i)} = \\int \\mathrm{d} S \\ \\rho_{i}\\left(S\\right) \\exp \\left[ S \\left( a_{i} + x_{i} \\right) + \\frac{V_{i} S^2}{2} \\right] \\end{equation}\nwhere the $a_i$ denote cavity means and the $V_i$ cavity variances. The single-site partition function can be integrated to yield an explicit expression after choosing well-behaved priors $\\rho_{i}(S)$ for the spins. For binary spins $S=\\pm 1$, we can pick $\\rho_{i}(S)=\\frac{1}{2}\\left( \\delta(S-1) + \\delta(S+1) \\right)$ to find\n\\begin{equation} Z_{0}^{(i)} = \\cosh \\left( a_{i} + x_{i} \\right). \\label{eq:partfunbinaryspins} \\end{equation}\nCavity means and Onsager correction term The cavity means can be shown to be given by \\begin{equation} a_{i} = \\sum_{j} J_{ij} \\langle S_{j} \\rangle - V_{i} \\langle S_{i} \\rangle. \\label{eq:cavitymean} \\end{equation}\nwhere the last term is the Onsager correction term, a self-correction term for every spin which depends on the cavity variances.\nCavity variances and linear response The cavity variances are determined self-consistently, i.e. by calculating the same quantity in two different ways and demanding the obtained expressions to be equal. To do this, we introduce the matrix of susceptibilities\n\\begin{equation} \\chi_{ij} = \\langle S_{i} S_{j} \\rangle - \\langle S_{i} \\rangle \\langle S_{j} \\rangle = \\frac{\\partial^2}{\\partial x_{i}\\partial x_{j}} \\log Z_{0}^{(i)} \\end{equation}\n The susceptibility matrix $\\chi_{ij}$ is a covariance matrix and should thus be positive semi-definite, which is criterion for the mean-field solution be consistent. As soon this property is lost, the fixed-point procedure will no longer be stable.\n Its diagonal elements $\\chi_{ii}$ can be obtained both from the explicit calculation of the spin variances from the partition function\n\\begin{equation} \\chi_{ii} = \\langle S_{i}^2 \\rangle - \\langle S_{i} \\rangle^2 = \\frac{\\partial^2}{\\partial x_{i}^2} \\log Z_{0}^{(i)} \\label{eq:chiii} \\end{equation}\nbut also from a linear response calculation assuming fixed $V_i$,\n\\begin{align} \\chi_{ij} = \\frac{\\partial \\langle S_{i} \\rangle}{\\partial x_{j}} = \\frac{\\partial \\langle S_{i} \\rangle}{\\partial x_{i}} \\left( \\delta_{ij} + \\sum_{k} \\left( J_{ik} - V_{k} \\delta_{ik} \\right) \\chi_{kj} \\right) \\label{eq:chiijlinrespexp} \\end{align}\nwhich can be solved for $\\chi_{ij}$ to yield \\begin{equation} \\chi_{ij} = \\left[ \\left( \\boldsymbol{\\Lambda} - \\boldsymbol{J} \\right)^{-1} \\right]_{ij} \\label{eq:chiijlinresp} \\end{equation} where \\begin{align} \\boldsymbol{\\Lambda} = \\mathrm{diag} \\left( \\Lambda_1, \\ldots, \\Lambda_{N} \\right),\\\\ \\Lambda_i = V_i + \\left( \\frac{\\partial \\langle S_{i} \\rangle}{\\partial x_{i}} \\right)^{-1}. \\end{align}\nThe cavity variances $V_i$ are then determined by equating \\eqref{eq:chiii} to the diagonal elements of \\eqref{eq:chiijlinresp} and solving the following consistency condition for $V_i$ \\begin{equation} \\frac{1}{\\Lambda_i - V_i} = \\left[ \\left( \\boldsymbol{\\Lambda} - \\boldsymbol{J} \\right)^{-1} \\right]_{ii}. \\label{eq:viselfcons} \\end{equation}\nGiven updated values for the cavity means $a_i$ and the cavity variances $V_i$, spin means and spin variances can then be updated as follows:\n\\begin{align} \\langle S_{i} \\rangle \u0026amp;= \\frac{\\partial}{\\partial x_{i}} \\log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}),\\\\ \\langle S_{i}^2 \\rangle - \\langle S_{i} \\rangle^2 \u0026amp;= \\frac{\\partial^2}{\\partial x_{i}^2} \\log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}), \\end{align}\nThese equations reduce to explicit expressions given an explicit expression for $Z_{0}^{(i)}$. For the binary-spin partition function \\eqref{eq:partfunbinaryspins} where $S=\\pm 1$, we get a set of fixed-point equations for the spin means that look like\n\\begin{equation} \\langle S_{i} \\rangle = \\tanh \\left( \\sum_{j} J_{ij} \\langle S_{j} \\rangle - V_{i} \\langle S_{i} \\rangle + x_{i} \\right) \\end{equation}\nwith spin variances $\\chi_{ii} = 1 - \\langle S_{i} \\rangle^2$.\n3. Attention as a fixed-point method In this section, we attempt to generalize the mean-field equations obtained in the previous section to random Ising-like models with vector spin degrees of freedom. We then recognize the physical system as an attention model and provide both a slow, explicit implementation and a faster, neural one.\n ✨ Code: A reference PyTorch implementation of the models outlined below is available in the repository deep-implicit-attention.\n 3.1. Generalizing spin models to vector degrees of freedom Let\u0026rsquo;s return to our Ising model cartoon and replace the scalar spin degrees of freedom $S_i$ at every site with vectors $\\boldsymbol{S}_i \\in \\mathbb{R}^d$, which we visualize using arrows below\nLet\u0026rsquo;s consider a system of $N$ $d$-dimensional spins and let\u0026rsquo;s label site indices with $i,j,\\ldots$ and internal vector-space indices with Greek letters $\\alpha,\\beta,\\ldots$. We let the coupling weight matrix become a tensor $\\boldsymbol{J}_{ij} = J_{ij}^{\\alpha\\beta}$ (matrices coupling every pair of sites) and remove self-couplings by enforcing the couplings' block-diagonal to be zero. Additionally, we can symmetrize both the internal dimension and the sites to end up with $N(N-1)/2$ times $d(d+1)/2$ effective free parameters for the couplings. If we also turn the external fields into vectors, we obtain a vector generalization of Eq. \\eqref{eq:randomising}:\n\\begin{equation} E = \\sum_{i,j} \\boldsymbol{S}_{i}^{T} \\boldsymbol{J}_{ij} \\boldsymbol{S}_{j} + \\sum_{i} \\boldsymbol{X}_{i} \\cdot \\boldsymbol{S}_{i}. \\label{eq:vectrandomising} \\end{equation}\n3.2. Deep implicit attention: attention as a collective response Remember that our goal is to understand attention as the collective response of a statistical-mechanical system. Let\u0026rsquo;s now relate vector models like Eq. \\eqref{eq:vectrandomising} to attention models by treating the external magnetic fields $\\boldsymbol{X}_{i}$ as input data. Batches of sequences applied to every site act as probes for the system, pushing its behaviour into a certain direction. The system\u0026rsquo;s mean-field average magnetizations $\\langle \\boldsymbol{S}_{i} \\rangle$ are an approximation of the collective response at every site: what is the expected value of this particular vector spin? We interpret solving mean-field equations for $\\langle \\boldsymbol{S}_{i} \\rangle$ in the presence of input injections $\\boldsymbol{X}_{i}$ as an attention operation. If the whole system is differentiable, we can tune the couplings $\\boldsymbol{J}_{ij}$ in an outer-loop optimization to steer the system\u0026rsquo;s behaviour to better1 respond to future incoming data.\n3.3. Slow and explicit: solving the adaptive TAP equations What changes do we have to make to the adaptive TAP mean-field equations to turn them into a vector-based attention module and how can we implement them? Let\u0026rsquo;s explicitly enumerate the objects introduced in Section 2.2 together with their (generalized) tensor shapes:\n  Iteratively determined fixed-point variables\n Spin means $\\langle \\boldsymbol{S}_{i} \\rangle = \\left[ \\langle \\boldsymbol{S}_{i} \\rangle \\right]^{\\alpha}$ (batch_size, N, d) Cavity variances $\\boldsymbol{V}_{i} = V_{i}^{\\alpha\\beta}$ (N, d, d)    Other variables calculated during fixed-point iteration\n Cavity means $\\boldsymbol{a}_{i} = a_{i}^{\\alpha}$ (batch_size, N, d) Spin variances $\\langle \\boldsymbol{S}_{i}^2 \\rangle - \\langle \\boldsymbol{S}_{i} \\rangle^2 = \\boldsymbol{\\chi}_{ii} = \\chi_{ii}^{\\alpha\\beta}$ (N, d, d)    For every site, the scalar spin and cavity variances have turned into $d \\times d$ (inverse) covariance matrices on the level of the local dimension. Note that the \u0026ldquo;system properties\u0026rdquo; in the above list have no batch size: their values are identical across all examples and capture the properties of the system irrespective of the input injections $\\boldsymbol{X}_i$.\nThe vector translation of the single-site partition function looks like\n\\begin{equation} Z_{0}^{(i)} = \\int \\mathrm{d}^{d} \\boldsymbol{S} \\ \\rho_{i}\\left(\\boldsymbol{S}\\right) \\exp \\left[ \\boldsymbol{S} \\cdot \\left( \\boldsymbol{a}_{i} + \\boldsymbol{X}_{i} \\right) + \\frac{1}{2} \\boldsymbol{S}^T \\boldsymbol{V}_{i} \\boldsymbol{S} \\right] \\end{equation}\nwhere\n\\begin{equation} \\boldsymbol{a}_{i} = \\sum_{j} \\boldsymbol{J}_{ij} \\langle \\boldsymbol{S}_{j} \\rangle - \\boldsymbol{V}_{i}\\langle \\boldsymbol{S}_{i} \\rangle. \\label{eq:veccavmeans} \\end{equation}\nSpin means and variances are then computed from\n\\begin{equation} \\langle \\boldsymbol{S}_{i} \\rangle = \\frac{\\partial}{\\partial\\boldsymbol{X}_{i}} \\log Z_{0}^{(i)} (\\boldsymbol{X}_{i}, \\boldsymbol{a}_{i}, \\boldsymbol{V}_{i}) \\end{equation}\n\\begin{equation} \\langle \\boldsymbol{S}_{i}^2 \\rangle - \\langle \\boldsymbol{S}_{i} \\rangle^2 = \\frac{\\partial^2}{\\partial\\boldsymbol{X}_{i}^2} \\log Z_{0}^{(i)} (\\boldsymbol{X}_{i}, \\boldsymbol{a}_{i}, \\boldsymbol{V}_{i}) \\end{equation}\nAs a spin prior $\\rho_{i}\\left(\\boldsymbol{S}\\right)$, we pick a simple diagonal multivariate Gaussian $\\mathcal{N} \\left( \\boldsymbol{\\mu} = \\boldsymbol{0}_{d}, \\boldsymbol{\\Sigma}= \\boldsymbol{1}_{d \\times d} \\right)$ at every site, leading to the explicit equations:\n\\begin{equation} \\langle \\boldsymbol{S}_{i} \\rangle = \\left( \\boldsymbol{\\Sigma}^{-1} - \\boldsymbol{V}_{i} \\right)^{-1} \\left( \\boldsymbol{a}_{i} + \\boldsymbol{X}_{i} \\right) \\end{equation}\n\\begin{equation} \\langle \\boldsymbol{S}_{i}^2 \\rangle - \\langle \\boldsymbol{S}_{i} \\rangle^2 = \\left( \\boldsymbol{\\Sigma}^{-1} - \\boldsymbol{V}_{i} \\right)^{-1} \\end{equation}\nGeneralizing the cavity variance calculation The cavity variance computation can be done by generalizing Eqs. \\eqref{eq:chiijlinrespexp}\u0026ndash;\\eqref{eq:chiijlinresp} and solving the following system of equations for $\\boldsymbol{\\chi}_{ij}$,\n\\begin{equation} \\left( \\delta_{ik} \\otimes \\boldsymbol{1}_{d} - \\boldsymbol{\\Sigma}_{i} \\boldsymbol{J}_{ik} + \\boldsymbol{\\Sigma}_{i} \\boldsymbol{V}_{i} \\delta_{ik} \\right)\\boldsymbol{\\chi}_{kj} = \\boldsymbol{\\Sigma}_{i} \\delta_{ij} \\end{equation}\nThe generalization of the self-consistency condition Eq \\eqref{eq:viselfcons} is then obtained by solving $\\boldsymbol{\\chi}_{ii} \\boldsymbol{V}_{i} = \\boldsymbol{\\chi}_{ii} \\boldsymbol{\\Lambda}_{i} - \\boldsymbol{1}_{N \\times d \\times d}$ for $\\boldsymbol{V}_{i}$, where $ \\boldsymbol{\\Lambda}_{i} = \\boldsymbol{V}_{i} + \\boldsymbol{\\Sigma}^{-1}$ is computed using the current values of $\\boldsymbol{V}_{i}$. The price to pay for this added complexity is a computational cost of $O(N^3d^3)$ and an excruciatingly slow backward pass. The algorithm works, but it ain\u0026rsquo;t pretty.\n Implementation: To avoid torch.solve crashing on singular matrices during the fixed-point calculation, we found it crucial for stability and learning behaviour to initialize the couplings $J_{ij}^{\\alpha\\beta} \\sim \\mathcal{N}(0, \\sigma^2)$ with small values $\\sigma^2 = 1 / (N*d^2)$ to ensure $|J| \\sim \\mathcal{O}(1)$. It\u0026rsquo;s also beneficial if the sources satisfy $|\\boldsymbol{X}_{i}| \\sim \\mathcal{O}(1)$ so that terms are balanced in the update step, all together adding up to $\\mathcal{O}(1)$.\n 3.4. Fast and neural: parametrizing the Onsager self-correction term Can we somehow approximate the slow and explicit calculation of the cavity variances? Since $\\boldsymbol{z}^{*} = \\left( \\langle \\boldsymbol{S}_{i}^{*} \\rangle, \\boldsymbol{V}_{i}^{*} \\right)$ at the fixed point, the Onsager self-correction term in Eq. \\eqref{eq:veccavmeans} converges to a constant vector $\\boldsymbol{V}_{i}^{*}\\langle \\boldsymbol{S}_{i}^{*} \\rangle$ for every site. We propose to make a bold move by getting rid of the cavity variables altogether and reducing the equations for the fixed-point update step to\n\\begin{equation} \\langle \\boldsymbol{S}_{i} \\rangle = \\sum_{j} \\boldsymbol{J}_{ij} \\langle \\boldsymbol{S}_{j} \\rangle - f_{\\theta} \\left( \\langle \\boldsymbol{S}_{i} \\rangle \\right) + \\boldsymbol{X}_{i}, \\label{eq:diaupdate} \\end{equation}\nwhere $f_{\\theta}$ is a neural network parametrizing the action of the cavity variances on the spin means. Since the parameters $\\theta$ stay fixed during the inner-loop fixed-point calculation, we have effectively lifted the optimization of the self-correction term to the outer-loop, which also optimizes the weights $\\boldsymbol{J}_{ij}$.\nAll of this starts to look an awful lot like a transformer module. Before discussing an explicit comparison in Section 4, let\u0026rsquo;s finish this section with a simple example model.\nSimple example: MNIST A simple image classification model for MNIST using a convolutional feature extractor and a deep implicit attention layer could look something like\nclass MNISTNet(nn.Module): def __init__(self, dim=10, dim_conv=32, num_spins=16): super(MNISTNet, self).__init__() self.to_patch_embedding = nn.Sequential( nn.Conv2d(1, dim_conv, kernel_size=3), # -\u0026gt; 26 x 26 nn.ReLU(), nn.MaxPool2d(3, stride=2), # -\u0026gt; 12 x 12 nn.Conv2d(dim_conv, dim_conv, kernel_size=3), # -\u0026gt; 10 x 10 nn.ReLU(), nn.MaxPool2d(3, stride=2), # -\u0026gt; 4 x 4 Rearrange( 'b c h w -\u0026gt; b (h w) c' ), nn.Linear(dim_conv, dim) ) self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) self.deq_atn = nn.Sequential( DEQFixedPoint( DEQMeanFieldAttention( num_spins=num_spins+1, dim=dim, weight_sym_internal=True, weight_sym_sites=False, lin_response=True, ), anderson, solver_fwd_max_iter=40, solver_fwd_tol=1e-4, solver_bwd_max_iter=40, solver_bwd_tol=1e-4, ), ) self.final = nn.Linear(dim, 10) def forward(self, x): x = self.to_patch_embedding(x) cls_tokens = self.cls_token.repeat(x.shape[0], 1, 1) x = torch.cat((cls_tokens, x), dim=1) x = self.deq_atn(x) return self.final(x[:, 0, :])  The ViT-style classification token is interpreted as an additional site in the system, which is probed with a learnable input injection that is shared across examples. The model uses the classification token\u0026rsquo;s output response to do the final classification. The system has to self-organize its behaviour so that the classification token gets all the information it needs.\nYou can train this small model (26k parameters) on MNIST to find a test set accuracy hovering around 99.1%. The animation above shows a graph reflecting the (directed) connection strengths between spins during training as measured by the Frobenius norms of the matrices $\\boldsymbol{J}_{ij}$. Almost all major organization of connections is seen to happen in the first few iterations. One imagines the model getting frustrated at zeros which really look like nines and just flat-out refusing to remember edge cases out of spite.\n4. A mean-field theory perspective on transformers Let\u0026rsquo;s conclude this post by applying the mean-field theory perspective on attention to the transformer architecture. Schematically, a vanilla transformer module looks like\nwhich consists of an attention module acting on all vectors in the sequence input followed by a feed-forward layer acting \u0026ldquo;locally\u0026rdquo; across individual vectors in the sequence, mixed with some residual connections and layer normalizations.\n4.1. Parametrizing the couplings: sparse graph structure from inputs Transformers can be interpreted as fully-connected graph neural networks acting on sets of vectors. Inside an attention module, the row-stochastic attention matrix corresponds to a particular parametrization of the couplings\n\\begin{equation} J_{ij} = \\left[\\mathrm{softmax}\\left( \\frac{\\boldsymbol{X} \\boldsymbol{W}_{\\boldsymbol{Q}} \\boldsymbol{W}_{\\boldsymbol{K}}^{T} \\boldsymbol{X}^{T}}{\\sqrt{d}} \\right)\\right]_{ij}. \\label{eq:softmaxcouplings} \\end{equation}\nwhich swaps storing explicit coupling weights for parameters of linear query-key transformations. By dynamically determining the connectivity of the sites based on the inputs $\\boldsymbol{X}$ according to Eq. \\eqref{eq:softmaxcouplings}, the coupling weights are no longer completely free parameters. The introduction of queries and keys can be seen as a neural network approach to \u0026ldquo;amortizing\u0026rdquo; the coupling tensor while the softmax temperature promotes sparsity. Multiple attention heads correspond to imposing a block-diagonal structure in the hidden dimensions of the couplings: the dot product gets cut into disjoint pieces, one for each attention head.\n4.2. Softmax attention does a single, naive mean-field update step Looking at the update step \\eqref{eq:diaupdate} and the softmax couplings \\eqref{eq:softmaxcouplings}, we observe that the softmax attention module does a single, naive mean-field update step without a self-correction term. Ignoring layer normalizations, the attention update step for every input vector looks like\n\\begin{equation} \\boldsymbol{X}'_{i} = \\sum_{j} \\left[ \\mathrm{softmax} \\left( \\frac{\\boldsymbol{X} \\boldsymbol{W}_{\\boldsymbol{Q}} \\boldsymbol{W}_{\\boldsymbol{K}}^{T} \\boldsymbol{X}^{T}}{\\sqrt{d}} \\right) \\right]_{ij} \\left[ \\boldsymbol{X} \\boldsymbol{W}_{\\boldsymbol{V}} \\right]_{j} + \\boldsymbol{X}_{i}, \\nonumber \\label{eq:vanilla-attention} \\end{equation}\nwhere, crucially, the residual connection is responsible for adding the source term to the update step. Without a residual connection, the applied magnetic field is effectively turned off and the signal would only be able to propagate via the coupling term.\n4.3. Feed-forward layer corrects naive mean-field update Looking at the Onsager self-correction term $f_{\\theta} \\left( \\langle \\boldsymbol{S}_{i} \\rangle \\right)$ in the update step \\eqref{eq:diaupdate}, we observe that the full transformer attention module emerges when we substitute $\\langle \\boldsymbol{S}_{i} \\rangle$ for its naive mean-field value, leading to\n\\begin{equation} \\mathrm{Attention}(\\boldsymbol{X})_{i} = \\boldsymbol{X}'_{i} + \\mathrm{FeedForward}\\left( \\boldsymbol{X}'_{i} \\right), \\end{equation}\nwith $\\boldsymbol{X}'_{i}$ defined above. Again, the residual connection appears to be crucial for the structure of the mean-field theory equations to match the vanilla transformer module\u0026rsquo;s architecture. As previously discussed in Section 3.4, we hypothesize that feed-forward networks in transformer modules \u0026ldquo;amortize\u0026rdquo; the linear response self-corrections.\n4.4. Mean-field theory framework for transformer architectures Within the general mean-field (or approximate message-passing) structure outlined above, there is considerable freedom in parametrizing the interaction and self-correction terms. Most transformer papers parametrize the self-correction terms with a feed-forward layer, i.e. some variation of an MLP. In MLP-Mixer: An all-MLP Architecture for Vision the authors went even further and dropped the softmax parametrization of the interaction term to approximate the full action of summing over couplings with an MLP as well. Related papers like Pay Attention to MLPs, ResMLP: Feedforward networks for image classification with data-efficient training, and FNet: Mixing Tokens with Fourier Transforms can all be considered as explorations of different parametrizations of the mean-field interaction terms. In the large-scale regime, it seems like the softmax attention module can be swapped for just about any function which mixes tokens as long as the structure of residual connections and self-correction terms is preserved.\n4.5. Comparison with energy-based perspective In a previous post on An Energy-Based Perspective on Attention Mechanisms in Transformers, we introduced a picture of attention modules in transformers as stacks of energy functions which are defined dynamically at every layer depending on the outputs of the previous layer (so ultimately on the inputs of the first layer). Looking back, this interpretation feels kind of forced and is also unable to explain the presence of skip connections and fully-connected layers surrounding the attention modules. The mean-field perspective seems more interesting since it (1) relies on just one layer (one energy function) whose fixed-point (an infinite amount of \u0026ldquo;layers\u0026rdquo;) gets calculated, and (2) explains the presence of skip connections (source terms) and fully-connected layers (amortized self-correction terms).\n5. Conclusion and outlook We have shown how attention can be understood as the mean-field response of Ising-like spin systems being probed by data. By thinking of incoming data as applied magnetic fields and the output of attention modules as spin expectation values, attention can be interpreted as a fixed-point optimization process solving for a compromise between a system\u0026rsquo;s internal dynamics and the data it\u0026rsquo;s being exposed to. Since the whole system is differentiable, we can optimize the interaction weights in an outer loop to nudge the system\u0026rsquo;s behaviour.\nWe have also seen how transformers fit into the mean-field theory framework. For scalability, transformers introduce two additional constraints/approximations on top of the mean-field approximation: (1) replacing explicit couplings with parametrized couplings that are dynamically computed from the input via linear transformations (softmax query-key-value attention), and (2) replacing the expensive self-consistent computation of Onsager self-correction terms with a neural network (feed-forward layer).\nLooking ahead, the methods introduced in this post could provide ways to implicitly train mean-field approximations of Boltzmann machines and have them serve as distributed attention modules in larger interconnected systems. To go beyond mean-field approaches, it could be interesting to look at tensor network approaches. Conceptually, the physical interpretation of attention as an interacting many-body system modulating its behaviour by learning to respond to being driven in particular ways is fun to think about.\n6. Related work A non-exhaustive list of references and inspiration includes:\n On deep equilibrium models: Deep Equilibrium Models (2019) by Shaojie Bai, Zico Kolter, Vladlen Koltun and Chapter 4: Deep Equilibrium Models of the Deep Implicit Layers - Neural ODEs, Deep Equilibrium Models, and Beyond workshop (NeurIPS 2020) by Zico Kolter, David Duvenaud, and Matt Johnson On the adaptive Thouless-Anderson-Palmer (TAP) mean-field approach in disorder physics: Adaptive and self-averaging Thouless-Anderson-Palmer mean-field theory for probabilistic modeling (2001) by Manfred Opper and Ole Winther On variational inference, iterative approximation algorithms, expectation propagation, mean-field methods and belief propagation: Expectation Propagation (2014) by Jack Raymond, Andre Manoel, Manfred Opper On Boltzmann machines and mean-field theory: Efficient Learning in Boltzmann Machines Using Linear Response Theory (1998) by H. J. Kappen and F. B. Rodríguez and Mean-field theory of Boltzmann machine learning (1998) by Toshiyuki Tanaka On approximate message passing (AMP) methods in statistics: A unifying tutorial on Approximate Message Passing (2021) by Oliver Y. Feng, Ramji Venkataramanan, Cynthia Rush, Richard J. Samworth: the example on page 2 basically describes how transformers implement approximate message passing: an iterative algorithm with a \u0026ldquo;denoising\u0026rdquo; step (attention) followed by a \u0026ldquo;memory term\u0026rdquo; or Onsager correction term (feed-forward layer)  References \u0026amp; footnotes If you happen to find this work useful, please consider citing it as:\n@article{bal2021deepimplicitattention, title = {Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms}, author = {Bal, Matthias}, year = {2021}, month = {May}, url = {https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/}, }    Whatever \u0026ldquo;better\u0026rdquo; means depends on the system\u0026rsquo;s (meta-)loss function, e.g. predicting corrupted tokens BERT-style or aligning representations to a teacher BYOL/DINO-style.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1617805037,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620750617,"objectID":"7ad828f967889fef598d8e9c76ff9232","permalink":"https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/","publishdate":"2021-04-07T15:17:17+01:00","relpermalink":"/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/","section":"post","summary":"Can we model attention as the collective response of a statistical-mechanical system?","tags":["Artificial Intelligence","Associative Memories","Attention","Belief Propagation","Boltzmann Machine","Deep Learning","Emergent Collective Computational Capabilities","Energy-Based Models","Expectation Propagation","Hopfield Networks","Ising Models","Many-Body Systems","Mean-Field Theory","Neural Networks","Statistical Physics","Transformers"],"title":"Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms","type":"post"},{"authors":[],"categories":[],"content":"  Introduction Prelude: pattern terminology Attention modules  Explicit vanilla softmax attention Implicit energy-based attention   From modern Hopfield networks to multi-head attention  Energy function Verifying the update rule  Cross-attention Self-attention   Adding queries, keys, and values Adding masking and multiple attention heads   Attention in flatland: visualizing energy landscapes Conclusion   1. Introduction  📓 Colab notebook available here. Comments welcome.\n Recent work 1 2 has shown that the softmax-attention update step in transformer models can be intepreted as a one-step gradient update or \u0026ldquo;inference\u0026rdquo; step of a judiciously chosen energy function. An overview of these ideas can be found in previous blog posts:\n An Energy-Based Perspective on Attention Mechanisms in Transformers Transformer Attention as an Implicit Mixture of Effective Energy-Based Models  The goal of this educational blog post is to explicitly show how vanilla softmax attention is related to energy minimization approaches and how the former can be substituted for the latter. For pedagogical purposes, we will focus purely on the attention operation. However, for transformer models to perform well in practice, it is necessary to wrap attention in residual connections and point-wise feedforward processing layers, see e.g. Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth.\nSummary:\n We provide a pedagogical energy-based attention module that stays as close as possible to vanilla softmax attention for ease of comparison. We walk through the correspondence between modern Hopfield networks and vanilla softmax attention by gradually adding complexity. We present visualizations of energy landscapes and trajectories associated to attention update steps for two-dimensional toy patterns.  2. Prelude: pattern terminology Transformer literature almost exclusively talks about queries, keys, and values. For self-attention, these are all obtained from different linear transformations acting on the same set of input patterns. For cross-attention, only the queries derive from the input patterns; the keys and values are obtained from a different set of context patterns: think of a decoder architecture attending to encoded translations or the Perceiver model attending to multimodal input.\nHopfield networks literature starts from the idea of trying to implement an associative memory system for storing and retrieving patterns. Patterns stored in memory are called stored patterns. A state pattern is an input prompt for the associative memory system: what patterns stored in memory are closest to this particular prompt?\nDepending on the context (heh), we can refer to input patterns as state patterns or queries and to context patterns as stored patterns or memory or keys.\n3. Attention modules 3.1. Explicit vanilla softmax attention To compare the behavior of explicit attention modules to that of energy-based attention modules, we need to first of all define a vanilla softmax attention module. The annotated implementation below features a bare_attn toggle in the forward pass for ease of comparison with the \u0026ldquo;bare\u0026rdquo; modern continuous Hopfield energy function we will discuss later on. The flag essentially disables all linear mappings so input and context patterns are processed \u0026ldquo;raw\u0026rdquo;.\nclass VanillaSoftmaxAttention(nn.Module): \u0026quot;\u0026quot;\u0026quot;Vanilla softmax attention. Adapted from https://github.com/lucidrains/perceiver-pytorch (commit 37e2eb6). \u0026quot;\u0026quot;\u0026quot; def __init__( self, query_dim, context_dim=None, heads=1, dim_head=2, scale=None, ): super().__init__() # Inner dimension is expressed in terms of head count and dimensionality # and thus decoupled from query_dim/context_dim (heads always \u0026quot;fit\u0026quot;). inner_dim = dim_head * heads context_dim = context_dim if context_dim is not None else query_dim # Linear transformations (queries, keys, values, head-mixing). self.to_q = nn.Linear(query_dim, inner_dim, bias=False) self.to_k = nn.Linear(context_dim, inner_dim, bias=False) self.to_v = nn.Linear(context_dim, inner_dim, bias=False) self.to_out = nn.Linear(inner_dim, query_dim) self.heads = heads self.scale = scale if scale is not None else dim_head ** -0.5 def forward(self, x, context=None, mask=None, scale=None, bare_attn=False): # To facilitate comparison with modern Hopfield networks, setting `bare_attn` # to `True` disables all linear mappings, assures there's only a single head and # reduces the module to a barebone attention which takes in \u0026quot;raw\u0026quot; queries or state # patterns and attends to a \u0026quot;raw\u0026quot; context/memory of stored patterns. if bare_attn: assert self.heads == 1, \u0026quot;only a single head when bare attention\u0026quot; if context is not None: assert ( x.shape[-1] == context.shape[-1] ), \u0026quot;query_dim/context_dim must match\u0026quot; # Adaptive scale. scale = scale if scale is not None else self.scale # Take context either from elsewhere of from self (attention vs. self-attention). context = context if context is not None else x # Map x to queries and context to keys and values. q = x if bare_attn else self.to_q(x) k = context if bare_attn else self.to_k(context) v = context if bare_attn else self.to_v(context) # Split up latent dimension into subspaces for heads to act on. # Head dimension becomes part of batch dimension (=\u0026gt; parallel processing of heads). h = self.heads q, k, v = map(lambda t: rearrange(t, \u0026quot;b n (h d) -\u0026gt; (b h) n d\u0026quot;, h=h), (q, k, v)) # Scaled dot product of all queries against all keys (sum over `inner_dim`). sim = einsum(\u0026quot;b i d, b j d -\u0026gt; b i j\u0026quot;, q, k) * scale # Optional masking. if mask is not None: max_neg_value = -torch.finfo(sim.dtype).max mask = repeat(mask, \u0026quot;b j -\u0026gt; (b h) () j\u0026quot;, h=h) sim.masked_fill_(~mask, max_neg_value) # Softmax operation across \u0026quot;keys\u0026quot; sequence dimension. attn = sim.softmax(dim=-1) # Contract attention matrix with values. out = einsum(\u0026quot;b i j, b j d -\u0026gt; b i d\u0026quot;, attn, v) # Move head dimension out of batch again. out = rearrange(out, \u0026quot;(b h) n d -\u0026gt; b n (h d)\u0026quot;, h=h) # Mix all the heads' outputs; stir well and serve immediately. return out if bare_attn or h == 1 else self.to_out(out)  3.2. Implicit energy-based attention Next, we define our energy-based attention module. Its forward pass will make use of the simple gradient descent function defined below to do energy minimization and update queries accordingly.\ndef minimize_energy( energy_func, queries, keys, mask=None, step_size=1.0, num_steps=1, return_trajs=False, ): \u0026quot;\u0026quot;\u0026quot;Minimize energy function with respect to queries. Keeps track of energies and trajectories for logging and plotting. \u0026quot;\u0026quot;\u0026quot; out = defaultdict(list) out[\u0026quot;queries\u0026quot;].append(queries) for _ in range(num_steps): energies = energy_func(queries, keys, mask=mask) grad_queries = torch.autograd.grad( energies, queries, grad_outputs=torch.ones_like(energies), create_graph=True, # enables double backprop for optimization )[0] queries = queries - step_size * grad_queries out[\u0026quot;queries\u0026quot;].append(queries) out[\u0026quot;energies\u0026quot;].append(energies) out[\u0026quot;energies\u0026quot;].append(energy_func(queries, keys, mask=mask)) if return_trajs: return out return out[\u0026quot;queries\u0026quot;][-1]  The EnergyBasedAttention module below has been structured to look as similar as possible to the the VanillaSoftmaxAttention module defined above. The main difference is the appearance of an energy function and the energy minimization call in the forward pass where the softmax attention used to be. Other differences include the absence of a linear map to \u0026ldquo;values\u0026rdquo; and masking being pushed into the energy function.\nclass EnergyBasedAttention(nn.Module): def __init__( self, query_dim, context_dim=None, heads=1, dim_head=2, scale=None, energy_func=None, ): super().__init__() inner_dim = dim_head * heads context_dim = context_dim if context_dim is not None else query_dim # Linear transformations (queries, keys, output). self.to_q = nn.Linear(query_dim, inner_dim, bias=False) self.to_k = nn.Linear(context_dim, inner_dim, bias=False) self.to_out = nn.Linear(inner_dim, query_dim) self.energy_func = energy_func if energy_func else hopfield_energy self.heads = heads self.scale = scale if scale is not None else dim_head ** -0.5 def forward( self, x, context=None, mask=None, scale=None, bare_attn=False, step_size=1.0, num_steps=1, return_trajs=False, ): # Bare checks. if bare_attn: assert self.heads == 1, \u0026quot;only a single head when bare attention\u0026quot; if context is not None: assert ( x.shape[-1] == context.shape[-1] ), \u0026quot;query_dim/context_dim must match\u0026quot; scale = scale if scale is not None else self.scale context = context if context is not None else x q = x if bare_attn else self.to_q(x) k = context if bare_attn else self.to_k(context) h = self.heads q, k = map(lambda t: rearrange(t, \u0026quot;b n (h d) -\u0026gt; (b h) n d\u0026quot;, h=h), (q, k)) if mask is not None: mask = repeat(mask, \u0026quot;b j -\u0026gt; (b h) () j\u0026quot;, h=h) # Minimize energy with respect to queries. outputs = minimize_energy( partial(self.energy_func, scale=scale), q, k, mask=mask, step_size=step_size, num_steps=num_steps, return_trajs=return_trajs, ) if return_trajs: return outputs out = rearrange(outputs, \u0026quot;(b h) n d -\u0026gt; b n (h d)\u0026quot;, h=h) return out if bare_attn or h == 1 else self.to_out(out)  4. From modern Hopfield networks to multi-head attention Let\u0026rsquo;s start with the simplest possible case: bare attention. We disable all linear mappings to queries/keys/values/output to make sure input and context patterns are processed \u0026ldquo;raw\u0026rdquo; and restrict ourselves to a single attention head. We numerically verify that a \u0026ldquo;bare\u0026rdquo; explicit attention module indeed returns the same result as doing a single, big step of energy minimization with respect to input state patterns. Put differently and more to the point, we merely show that automatic differentiation works.\n4.1. Energy function Consider the energy function of a modern continuous Hopfield network for a set of state patterns $\\boldsymbol{\\Xi}$ and stored patterns $\\boldsymbol{X}$:\n\\begin{equation} E(\\boldsymbol{\\Xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\Xi}^T \\boldsymbol{\\Xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi} \\right),\\label{eq:energy} \\end{equation}\nThink of this model as the scoring function of an associative memory system. For now, we\u0026rsquo;d like to keep the stored patterns fixed as memory slots and wiggle around the state patterns. We can translate this energy function into the following (batched) function:\ndef hopfield_energy(state_patterns, stored_patterns, scale, mask=None): kinetic = 0.5 * einsum(\u0026quot;b i d, b i d -\u0026gt; b i\u0026quot;, state_patterns, state_patterns) scaled_dot_product = scale * einsum( \u0026quot;b i d, b j d -\u0026gt; b i j\u0026quot;, state_patterns, stored_patterns ) if mask is not None: max_neg_value = -torch.finfo(scaled_dot_product.dtype).max scaled_dot_product.masked_fill_(~mask, max_neg_value) potential = -(1.0 / scale) * torch.logsumexp(scaled_dot_product, dim=2) return kinetic + potential  4.2. Verifying the update rule Let\u0026rsquo;s sample some state patterns and stored patterns and enable gradient tracking for the state patterns since we want to take derivatives with respect to these parameters later on.\nlatent_dim = 512 state_patterns = torch.randn(1, 8, latent_dim).requires_grad_(True) stored_patterns = torch.randn(1, 32, latent_dim)  Cross-attention First up is cross-attention. We feed state patterns as input and stored patterns as context into a vanilla softmax attention module.\nsoftmax_attn = VanillaSoftmaxAttention( latent_dim, context_dim=latent_dim, heads=1, dim_head=latent_dim, scale=latent_dim ** -0.5, ) output_bare_softmax_attn = softmax_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), bare_attn=True, )  Now we do the same for an energy-based attention module and tell it to take a single, big gradient update step.\nenergy_attn = EnergyBasedAttention( latent_dim, context_dim=latent_dim, heads=1, dim_head=latent_dim, scale=latent_dim ** -0.5, energy_func=hopfield_energy, ) output_bare_energy_attn = energy_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), step_size=1.0, num_steps=1, bare_attn=True, )  Now let\u0026rsquo;s compare the outputs of the two methods:\ntorch.allclose(output_bare_softmax_attn, output_bare_energy_attn, atol=1e-6)  True  Both tensors are approximately equal: bare softmax attention corresponds to taking a single gradient step of step_size=1.0 with respect to the state patterns using the energy function of modern Hopfield networks as a loss. For more details on this correspondence, we refer to a previous blog post.\nSelf-attention Let\u0026rsquo;s do the same check for self-attention, which boils down to only inputting state patterns. Internally, the modules will consider the state patterns as stored patterns and effectively make the patterns pay attention to themselves.\noutput_bare_softmax_self_attn = softmax_attn( copy_tensor(state_patterns), bare_attn=True ) output_bare_energy_self_attn = energy_attn( copy_tensor(state_patterns), step_size=1.0, num_steps=1, bare_attn=True, ) print( torch.allclose( output_bare_softmax_self_attn, output_bare_energy_self_attn, atol=1e-6 ) ) print( f\u0026quot;Norm between input state patterns and energy-minimized patterns: \u0026quot; f\u0026quot;{torch.norm(state_patterns - output_bare_energy_self_attn)}\u0026quot; )  True Norm between input state patterns and energy-minimized patterns: 5.553587470785715e-06  The pattern update step looks almost like an an identity operation, which is to be expected for \u0026ldquo;bare\u0026rdquo; self-attention3. Without any linear transformations to map state patterns to queries and keys, every state pattern starts off already close to a local minimum since it coincides with itself as a stored pattern. The query starts off close to the key since the query-key mappings are identities. We will visualize this behavior in Section 4 for two-dimensional patterns.\n4.3. Adding queries, keys, and values Let\u0026rsquo;s now move closer to proper vanilla softmax attention by enabling linear transformations which map state patterns to queries and stored patterns to keys (and values). These parameters are able to move patterns around on the energy landscape before (queries, keys) and after (values) paying attention.\nWe recycle the previously instantiated patterns and modules and compare outputs again, making sure the parameters are equal and omitting the bare_attn flag:\noutput_softmax_attn = softmax_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns) ) energy_attn.load_state_dict(softmax_attn.state_dict(), strict=False) output_energy_attn = energy_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), step_size=1.0, num_steps=1, ) torch.allclose(output_softmax_attn, output_energy_attn, atol=1e-6)  False  Why don\u0026rsquo;t the outputs match? We have to make sure we compare apples to apples and be mindful of the fact that the energy minimization step only knows about keys. Indeed, as shown previously in Hopfield Networks is All You Need, the one-step energy minimization, expressed in terms of queries and keys, effectively implements\n\\begin{equation} \\boldsymbol{Q}^{\\text{new}} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\boldsymbol{Q} \\boldsymbol{K}^T \\right) \\boldsymbol{K} \\end{equation}\ninstead of the vanilla softmax attention step\n\\begin{equation} \\boldsymbol{Q}^{\\text{new}} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\boldsymbol{Q} \\boldsymbol{K}^T \\right) \\boldsymbol{V} \\end{equation}\nWe can approximately undo this mapping to make a forced comparison for fixed parameters:\noutput_energy_attn_transformed = softmax_attn.to_v( output_energy_attn @ torch.pinverse(energy_attn.to_k.weight.t()) ) torch.norm(output_softmax_attn - output_energy_attn_transformed)  tensor(0.0005, grad_fn=\u0026lt;CopyBackwards\u0026gt;)  Yet since all these parameters would be optimized in a real-world scenario, we should only care about whether the representational power of the modules is similar. To make the single-head energy-based attention module more expressive, we can always add an output layer, parametrized by weights $W_{O}$, to the module. As long as the composition of linear transformations $W_{K}W_{O}$ doesn\u0026rsquo;t collapse and its rank does not fall below that of the softmax attention\u0026rsquo;s $W_{V}$, things should be okay.\n4.4. Adding masking and multiple attention heads Finally, let us tie up some loose ends and complete the correspondence between vanilla softmax attention and energy-based minimization.\nMasking Since masking boils down to putting restrictions on what patterns in the inputs are allowed to talk to each other, it can just as well be done at the level of the energy function. By filling the tensor inside the logsumexp operator in hopfield_energy with $-\\infty$ values at to-be-masked-out positions, we get the same effect as the masking operation in the forward pass of VanillaSoftmaxAttention. Boolean masks can be passed to the EnergyBasedAttention\u0026rsquo;s forward function and propagate to the energy function.\nMulti-head attention Up to now, we have only considered a single attention head. Essentially, multiple attention heads subdivide the latent space into equal parts and process these subproblems in parallel. The head dimension becomes part of the batch dimension. This translates to having parallel energy minimizations going on for different heads, each acting on their own subspace. Since our hopfield_energy function is already batched, we can use the same machinery of the previous sections, as shown below.\nheads = 8 dim_head = latent_dim // heads scale = dim_head ** -0.5 mha_energy_attn = EnergyBasedAttention( latent_dim, context_dim=latent_dim, heads=heads, dim_head=dim_head, scale=scale, energy_func=hopfield_energy, ) mha_energy_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), step_size=1.0, num_steps=1, )  tensor([[[-0.0514, -0.0353, 0.0243, ..., -0.0335, -0.0060, 0.0243], [-0.1004, -0.0136, -0.0297, ..., 0.0079, 0.0083, 0.0336], [-0.0507, -0.0369, -0.0219, ..., -0.0022, -0.0246, -0.0223], ..., [-0.0388, -0.0217, -0.0470, ..., -0.0067, 0.0020, -0.0139], [-0.0283, -0.0699, -0.0205, ..., -0.0261, -0.0667, 0.0052], [-0.0262, -0.0360, -0.0139, ..., -0.0011, -0.0199, -0.0004]]], grad_fn=\u0026lt;AddBackward0\u0026gt;)  It is hard to compare with the exact output of the equivalent VanillaSoftmaxAttention module for fixed module parameters. For multi-head attention, the updated queries coming out of the separate energy minimization steps will have summed over each heads' keys instead of its values. For a single attention head we could undo the keys' transformation by acting with the inverse of the keys' weights. For multiple attention heads, that is no longer possible.\nAgain, since all these parameters would be optimized in a real-world scenario, we should only care about whether the representational power of the modules is similar. One approach would be to add parameters inside the energy function that take care of mapping to \u0026ldquo;values\u0026rdquo; on the level of the heads.\n5. Attention in flatland: visualizing energy landscapes We now leave the world of high-dimensional latent spaces behind us and focus on the toy model scenario of just two latent space dimensions. We only consider a single attention head because having just two heads, each with dimension one, is just silly. For every two-dimensional token pattern vector, a third dimension will be provided by the value of the scalar energy function at that point.\nLet\u0026rsquo;s sample some tiny toy patterns to play around with.\ntoy_state_patterns = torch.randn(1, 16, 2).requires_grad_(True) toy_stored_patterns = torch.randn(1, 32, 2)  Bare cross-attention Let\u0026rsquo;s plot our tiny toy patterns taking a big gradient step!\nfig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=2 ** -0.5, step_size=1.0, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  In the figure above, the blue open circles correspond to the stored patterns (memory, context, keys, \u0026hellip;), the red circles denote the initial state patterns (inputs, queries, probes, \u0026hellip;) and the red crosses the updated queries obtained after n_steps of energy minimization. The red arrows denote the trajectory in the energy landscape.\nWe will now illustrate some example scenarios.\nSmall steps go nowhere fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=2 ** -0.5, step_size=0.1, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Lots of (big) steps converge near (global) minimum or repeated softmax iterations make all token representations identical fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=2 ** -0.5, step_size=1.0, num_steps=10, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Decreasing the scale (increasing the temperature) makes the landscape smoother and encourages convergence to same (global) minimum fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=0.1 * 2 ** -0.5, step_size=1.0, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Increasing the scale (lowering the temperature) creates \u0026ldquo;disconnected\u0026rdquo; valleys in the energy landscape inhabited by stored patterns which act as attractors for any query that happens to be in its basin of attraction fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=10 * 2 ** -0.5, step_size=1.0, num_steps=5, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Adding linear query-key-value transformations # As commented on before, the value transformation is applied # after the update step so that effectively the product # W_K x W_V is applied to the updated state patterns. to_q = nn.Linear(2, 2, bias=False) to_k = nn.Linear(2, 2, bias=False) to_v = nn.Linear(2, 2, bias=False) fig, ax = simulate_and_plot_patterns( hopfield_energy, to_q(copy_tensor(toy_state_patterns)), context=to_k(copy_tensor(toy_stored_patterns)), scale=2 * 2 ** -0.5, step_size=1.0, num_steps=1, values_post_processing_func=to_v, plot_grid_size=2, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  The yellow arrows point from the final, energy-minimized, query updates to the \u0026ldquo;value-transformed\u0026rdquo; output queries, which are denoted with yellow crosses. Running this cell again in the colab notebook will give different landscapes and trajectories every time since the queries and keys depend on the random linear layers. The differences are more pronounced when increasing the scale (lowering the temperature).\nSince the value transformation is done after the energy minimization, it can and does undo some of the influence of the keys' attractors, e.g. sending updated queries to \u0026ldquo;uphill\u0026rdquo; regions in the energy landscape defined at that that layer. This suggests that the value transformation should not be seen as part of the core attention mechanism but that its role is rather to learn during training how to best hop to different regions in preparation for whatever the next layer needs.\nBare self-attention: on the importance of scale and why multiple heads Since all of the flatland examples so far have been for cross-attention, let\u0026rsquo;s also visualize a self-attention update below:\nfig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), scale=2 ** -0.5, step_size=1.0, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Wait, what? Why did the updated state patterns move from their initialization? Didn\u0026rsquo;t we see before that the norm between inputs and outputs hardly changed at all for bare self-attention?\nTo look into this, let\u0026rsquo;s plot the norm between inputs and outputs in function of the latent dimension, while scaling the scale or inverse temperature relative to the transformer default $\\beta = 1/\\sqrt{\\mathrm{d_k}}$. We sample toy patterns repeatedly for every dimension/scale combination to get an idea of the statistical behavior.\ndims = np.linspace(2.0, 1024, num=100, dtype=np.int32) beta_scales = np.linspace(0.2, 2.0, num=50, dtype=np.float32) norms = np.zeros((len(beta_scales), len(dims))) for i, dim in enumerate(dims): bare_attention = VanillaSoftmaxAttention(dim, heads=1, dim_head=dim) for j, beta_scale in enumerate(beta_scales): inputs = torch.randn(1, 32, dim).requires_grad_(True) outputs = bare_attention(inputs, bare_attn=True, scale=beta_scale * dim ** -0.5) norms[j][i] = torch.norm(inputs - outputs) # Suppresses a warning. norms = np.ma.masked_where(norms \u0026lt;= 0, norms) # Plot data. fig = plt.figure(figsize=(10, 8)) ax = fig.gca() X, Y = np.meshgrid(beta_scales, dims) contourplot = ax.contourf( dims, beta_scales, norms, norm=colors.LogNorm(vmin=1e-5, vmax=1e2), levels=np.logspace(-8, 2, 10), ) ax.set_xlabel(\u0026quot;d_k\u0026quot;) ax.set_ylabel(\u0026quot;scale / sqrt(d_k)\u0026quot;) plt.colorbar(contourplot, format=\u0026quot;%.e\u0026quot;, ticks=ticker.LogLocator(base=10)) ax.axvline(x=2, color=\u0026quot;r\u0026quot;) ax.axvline(x=512, color=\u0026quot;r\u0026quot;) transformer_default_scale = ax.axhline(y=1.0, color=\u0026quot;r\u0026quot;)  In this contour plot, we plot the norm differences between inputs and outputs of a bare self-attention step for a sweep across latent dimensions and inverse temperature scale factors. The horizontal red line corresponds to the scale factor used by default in most transformer implementations. Some comments:\n For a fixed latent dimension, we see that increasing the scale factor corresponds to smaller norm differences, i.e. more pronounced valleys where it\u0026rsquo;s much harder to get out of, especially if you start at the bottom and there is no query-key-value mapping taking you elsewhere. The vertical red line corresponds to the earlier bare self-attention result using a latent dimension of 512. The intersection point indeed corresponds a norm difference of the order we saw previously. The value for a latent dimension of 2 (left border of plot) suggests that patterns do move around quite a bit, confirming our visualization above. Setting the scale for bare multi-head attention proportionally to the (smaller) head dimension instead of the full latent dimension corresponds to moving leftwards along the horizontal red line. The norm difference increases so that, for bare multi-head self-attention, patterns in multiple small heads tend to bounce around more than they would in a single big head. This might be one of the reasons why multiple heads help with training transformers: since the effective temperature is lower in the smaller latent spaces, the topography of the lower-dimensional energy landscapes is more pronounced and individual heads can go explore a bit to find their niche valley.  6. Conclusion Using the tools presented in this blog post, we have shown that it is possible to swap the explicit attention module in a transformer for an implicit energy minimization method. What happens when we start playing around with different energy functions? Can we make patterns interact? Can we make the energy minimization step more efficient by treating it as a fixed-point problem? It remains to be seen whether all of this is a useful thing to do.\nReferences \u0026amp; footnotes If you happen to find this work useful, please consider citing it as:\n@article{bal2021visualizingattention, title = {Attention as Energy Minimization: Visualizing Energy Landscapes}, author = {Bal, Matthias}, year = {2021}, month = {March}, url = {https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/}, }    Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, Hopfield Networks is All You Need (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Caveat: For the special case of bare energy-based self-attention, state patterns actually appear quadratically in the argument of the logsumexp part of the energy function. Taking the derivative using minimize_energy(..) however assumes the context is a different node in the computational graph, which, in this case, where we should be taking the derivative of energy(x, x) instead of energy(x, context), yields a gradient that misses a factor of 2. But ensuring the gradient is \u0026ldquo;correct\u0026rdquo; for this special case would of course screw up the cancellation of the state pattern with itself for step_size=1.0 and num_steps=1 so that the updated query would no longer match the output of bare vanilla softmax attention. Proper treatment of doing multiple steps of bare energy-based self-attention should also include manually setting the context to the updated queries (since the queries themselves change every update step). Luckily no one would seriously consider using bare energy-based self-attention.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1616016977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616103377,"objectID":"e0bbd7f261e7e88f59a52332d9bb7ac9","permalink":"https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/","publishdate":"2021-03-17T22:36:17+01:00","relpermalink":"/post/attention-as-energy-minimization-visualizing-energy-landscapes/","section":"post","summary":"Can we swap softmax attention for energy-based attention?","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Dynamical Systems","Energy-Based Models","Neural Networks","Transformers"],"title":"Attention as Energy Minimization: Visualizing Energy Landscapes","type":"post"},{"authors":[],"categories":[],"content":" ✨ Update (November 2021): Please consider reading Transformers Are Secretly Collectives of Spin Systems for an arguably more comprehensive approach towards understanding transformers from a physics perspective.\n  Introduction Attention from effective energy-based models  Restricted Boltzmann Machines Integrating out hidden units Effective energies and correlations Modern Hopfield networks as mixtures of effective RBMs   Attention as implicit energy minimization  Bending the explicit architecture From explicit architectures to implicit energy minimization Deep implicit layers for attention dynamics   Conclusion References \u0026amp; footnotes   _In your [previous post](https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/), you introduced the energy function of modern Hopfield networks without explanation. Where does it come from? What's up with the logarithm? Is there actually any other interpretation then it being reverse-engineered from the Transformers' attention step? Is this all a desperate attempt to make Hopfield networks cool again? Also, I cannot see the value of looking at attention from an energy-based perspective if it doesn't help me achieve SOTA. Weak reject._ -- 1. Introduction In a previous post, I provided an overview of attention in Transformer models and summarized its connections to modern Hopfield networks. We saw that the energy-based model \\begin{equation} E(\\boldsymbol{\\Xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\Xi}^T \\boldsymbol{\\Xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi} \\right). \\label{eq:mhnenergy} \\end{equation} enables fast pattern storage and retrieval through its simple and robust dynamics, leading to rapid convergence \\begin{align} \\boldsymbol{\\Xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi}_{n}\\right) \\label{eq:mhnupdate} \\end{align} of input queries $\\boldsymbol{\\Xi}_{n}$ to updated queries $\\boldsymbol{\\Xi}_{n+1}$ lying in the convex hull of stored patterns $\\boldsymbol{X}$. I also argued by means of handwaving that optimizing a Transformer looks like meta-learning from the point of view of its attention modules, sculpting energy landscapes to accommodate statistical patterns found in data.\nThe main goal of this post is to build on these insights and highlight how an energy-based perspective can be a useful, complementary approach towards improving attention-based neural network modules. Parallel to scaling compute and making (self-)attention more efficient, it might be worthwhile to try to scale learning itself by experimenting with radically different attention mechanisms.\nTo this end, we will first revisit ancient ideas at the boundary of statistical physics and machine learning and show how vanilla attention looks like a mixture of simple energy-based models. We will then argue how going beyond these simple models could benefit from thinking in terms of implicit instead of explicit attention modules, suggesting opportunities to put ideas from Deep Implicit Layers to work.\n2. Attention from effective energy-based models In this section, we will introduce Restricted Boltzmann Machines as a particular class of energy-based models, focusing on their capacity to capture effective correlations. After identifying classical discrete Hopfield networks and modern discrete Hopfield networks, we will demonstrate a naive way to fit modern continuous Hopfield networks into this framework. Throughout this section, we will rely heavily on the wonderful review A high-bias, low-variance introduction to machine learning for physicists by Mehda et al.1.\nRestricted Boltzmann Machines A Restricted Boltzmann Machine (RBM) is an energy-based model with a bipartite structure imposed on visible and hidden degrees of freedom: visible and hidden degrees of freedom interact with each other but do not interact among themselves (this is the \u0026ldquo;restriction\u0026rdquo;). The energy function looks like\n\\begin{equation} E \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right) = - \\sum_{i} a_{i} (v_{i}) - \\sum_{\\mu} b_{\\mu} (h_{\\mu}) - \\sum_{i \\mu} W_{i \\mu} v_{i} h_{\\mu}, \\end{equation}\nwhere the matrix $W_{i \\mu}$ encodes the coupling between hidden and visible units and where $a_{i} (\\cdot)$ and $b_{\\mu} (\\cdot)$ are functions that can be chosen at will. Popular options are:\n\\begin{align} a_{i} (\\cdot) = \\begin{cases} a_{i} v_{i} \u0026amp; \\text{if $v_{i} \\in {0,1}$ is binary (Bernouilli)}\\\\ \\frac{v_{i}^2}{2\\sigma_{i}^{2}} \u0026amp; \\text{if $v_{i} \\in \\mathbb{R}$ is continuous (Gaussian)}\\ \\end{cases} \\end{align}\nand similar for $b_{\\mu} (\\cdot)$.\n\nWhy hidden units? Introducing hidden or latent variables is a powerful technique to encode interactions between visible units. Complex correlations between visible units can be captured at the cost of introducing new degrees of freedom and letting them interact with visible units in a simpler way. Since this trick often relies on exploiting Gaussian integral identities and physicists like their Gaussians, it shows up in several places across physics, e.g. in the Hubbard-Stratonovich transformation.\n Renormalization group: Rather than trying to fix the interactions in the \u0026ldquo;microscopic theory\u0026rdquo; like is done in the modeling scenario above, physicists are more familiar with the \u0026ldquo;reverse\u0026rdquo; procedure of deducing what effective theory emerges at large scales from a given microscopic theory. Indeed, integrating out degrees of freedom in physical theories can lead to complex, effective interactions between remaining degrees of freedom. This insight crystallized in the development of renormalization group theory in the early 1970s. By focusing on theories defined at different length scales, Kenneth G. Wilson and his contemporaries introduced and unified the notions of flows, fixed points, and universality in theory space to understand the behavior of physical systems under a change of scale.\n As we will see in the next sections, the bipartite structure of RBMs enables pairwise and higher-order correlations to emerge between visible units after integrating out hidden units. Additionally, the conditional independence of visible and hidden units enables tractable training methods like (block) Gibbs sampling and contrastive divergence1. We will not consider explicitly training RBMs in this post but will instead reflect on the idea of implicitly training these models, which is what seems to be happening inside Transformers.\nEffective energies and correlations Let us now consider what kind of correlations between visible degrees of freedom are supported by RBMs. The distribution of the visible degrees of freedom can be obtained by marginalizing over the hidden degrees of freedom:\n\\begin{equation} p \\left( \\boldsymbol{v} \\right) = \\int \\mathrm{d} \\boldsymbol{h} \\ p \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right) = \\int \\mathrm{d} \\boldsymbol{h} \\ \\frac{\\mathrm{e}^{- E \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right)}}{Z} \\end{equation}\nWe try to find an expression for the marginalized energy $E (\\boldsymbol{v})$ by defining\n\\begin{equation} p \\left( \\boldsymbol{v} \\right) = \\frac{\\mathrm{e}^{- E (\\boldsymbol{v})}}{Z} \\end{equation}\nso that we can identify\n\\begin{align} E \\left( \\boldsymbol{v} \\right) \u0026amp;= - \\mathrm{log} \\int \\mathrm{d} \\boldsymbol{h} \\ \\mathrm{e}^{- E \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right)} \\\\ \u0026amp;= - \\sum_{i} a_{i} (v_{i}) - \\sum_{\\mu} \\log \\int \\mathrm{d} h_{\\mu}\\ \\mathrm{e}^{b_{\\mu}(h_{\\mu}) + \\sum_{i} W_{i\\mu} v_{i} h_{\\mu}} \\label{eq:effvisenergy} \\end{align}\nFollowing Mehda et al., we can try to better understand the correlations in $p(\\boldsymbol{v})$ by introducing the (prior) distribution\n\\begin{equation} q_{\\mu} \\left( h_{\\mu} \\right) = \\frac{\\mathrm{e}^{b_{\\mu} (h_{\\mu})}}{Z} \\end{equation}\nfor the hidden units $h_{\\mu}$, ignoring the interactions between $\\boldsymbol{v}$ and $\\boldsymbol{h}$. Additionally, we can introduce the hidden unit\u0026rsquo;s distribution\u0026rsquo;s cumulant generating function\n\\begin{align} K_{\\mu} (t) \u0026amp;= \\mathrm{log}\\ \\mathbb{E} \\left[ \\mathrm{e}^{t h_{\\mu}} \\right] \\\\ \u0026amp;= \\mathrm{log} \\int \\mathrm{d} h_{\\mu} \\ q_{\\mu} \\left( h_{\\mu} \\right) \\mathrm{e}^{t h_{\\mu}}\\\\ \u0026amp;= \\sum_{n=1}^{\\infty} \\kappa_{\\mu}^{(n)} \\frac{t^{n}}{n!}, \\end{align}\nwhich is defined such that the $n^{\\mathrm{th}}$ cumulant $\\kappa_{\\mu}^{(n)}$ of $q_{\\mu} \\left( h_{\\mu} \\right)$ can be obtained by taking derivatives $\\kappa_{\\mu}^{(n)} = \\partial_{t}^{n} K_{\\mu} \\rvert_{t=0}$.\nLooking back at the effective energy function \\eqref{eq:effvisenergy} for the visible units, we find that the effective energy can be expressed in terms of cumulants:\n\\begin{align} E \\left( \\boldsymbol{v} \\right) \u0026amp;= - \\sum_{i} a_{i} \\left(v_{i}\\right) - \\sum_{\\mu} K_{\\mu} \\left( \\sum_{i} W_{i\\mu} v_{i} \\right) \\\\ \u0026amp;= - \\sum_{i} a_{i} \\left(v_{i}\\right) - \\sum_{\\mu} \\sum_{n=1}^{\\infty} \\kappa_{\\mu}^{(n)} \\frac{\\left( \\sum_{i} W_{i\\mu} v_{i} \\right)^{n}}{n!} \\\\ \u0026amp;= - \\sum_{i} a_{i} \\left(v_{i}\\right) - \\sum_{i} \\left( \\sum_{\\mu} \\kappa_{\\mu}^{(1)} W_{i\\mu} \\right) v_{i} \\\\ \u0026amp;\\ \\ \\ \\ \\ - \\frac{1}{2} \\sum_{ij} \\left( \\sum_{\\mu} \\kappa_{\\mu}^{(2)} W_{i\\mu} W_{j\\mu} \\right) v_{i} v_{j} + \\ldots \\label{eq:effectivenergy} \\end{align}\nWe see that the auxiliary, hidden degrees of freedom induce effective pairwise and higher-order correlations among visible degrees of freedom. Each hidden unit $h_{\\mu}$ can encode interactions of arbitrarily high order, with the $n$-th order cumulants of $q_{\\mu} \\left( h_{\\mu} \\right)$ weighting the $n$-th order interactions. By combining many hidden units and/or stacking layers, RBMs can in principle encode complex interactions at all orders and learn them from data.\nLet us now recover some known models by picking a suitable prior distribution for the hidden units:\n  Classical discrete Hopfield networks: Consider a Bernouilli distribution for the visible units and a standard Gaussian distribution for the hidden units. For a standard Gaussian, the mean $\\kappa_{\\mu}^{(1)} = 0$, the variance $\\kappa_{\\mu}^{(2)} = 1$, and $\\kappa_{\\mu}^{(n)} = 0$, $\\forall n\\geq 3$, leading to the quadratic energy function of Hopfield networks: \\begin{align} E \\left( \\boldsymbol{v} \\right) = - \\sum_{i} a_{i} v_{i} - \\frac{1}{2} \\sum_{ij} \\left( \\sum_{\\mu} W_{i\\mu} W_{j\\mu} \\right) v_{i} v_{j} \\end{align}\n  Modern discrete Hopfield networks: Consider a Bernouilli distribution for the visible units. Since it can be shown that the normal distribution is the only distribution whose cumulant generating function is a polynomial, i.e. the only distribution having a finite number of non-zero cumulants2, it looks like we cannot model a finite amount of polynomial interactions in this framework. But we can model an exponential interaction by considering a Poisson distribution $\\mathrm{Pois}(\\lambda)$ with rate $\\lambda=1$ for the hidden units, whose cumulants are all equal to the rate, i.e. $\\kappa_{\\mu}^{(n)} = 1$, $\\forall n\\geq 1$. Up to a constant, we then obtain an exponential interaction \\begin{align} E \\left( \\boldsymbol{v} \\right) = - \\sum_{i} a_{i} v_{i} - \\sum_{\\mu} \\exp \\left( \\sum_{i} W_{i\\mu} v_{i} \\right) \\end{align}\n  Other kinds of effective interactions can be obtained by substituting the cumulants of your favorite probability distribution. The cumulants of hidden Bernouilli units induce interactions of all orders. Considering exponential or Laplacian distributions where $\\kappa^{(n)} \\sim (n-1)!$ seems to lead to funky logarithmic interactions.\nModern Hopfield networks as mixtures of effective RBMs Let us now turn to the energy function of modern Hopfield networks for a single query $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ and $N$ stored patterns encoded by $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$, \\begin{equation} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right), \\end{equation} which we can transform into the RBM notation of the previous section by changing the names of variables and transposing the stored pattern matrix, \\begin{equation} E(\\boldsymbol{v}; W) = \\frac{1}{2} \\sum_{i} v_{i}^{2} -\\log \\left( \\sum_{\\mu} \\exp \\left( \\sum_{i} W_{\\mu i} v_{i} \\right) \\right). \\end{equation}\nIs there a simple way to interpret this energy function in terms of (effective) RBMs? Let\u0026rsquo;s imagine this energy to be an effective energy $E(\\boldsymbol{v})$ for the visible units with probability distribution \\begin{equation} p(\\boldsymbol{v}) = \\frac{\\mathrm{e}^{-E(\\boldsymbol{v})}}{Z} = \\frac{1}{Z} \\sum_{\\mu} \\mathrm{e}^{-\\frac{1}{2} \\sum_{i} v_{i}^{2} + \\sum_{i} W_{\\mu i} v_{i}}, \\end{equation} where the partition function $Z$ follows from doing a Gaussian integral \\begin{equation} Z = (2\\pi)^{n/2} \\sum_{\\mu} Z_{\\mu} = (2\\pi)^{n/2} \\sum_{\\mu} \\mathrm{e}^{\\frac{1}{2} \\sum_{i} W_{\\mu i} W_{i\\mu}} \\end{equation}\nWe can then identify the probability distribution $p(\\boldsymbol{v})$ with a mixture of effective energy-based models3 \\begin{equation} p(\\boldsymbol{v}) = \\sum_{\\mu} w_{\\mu} \\frac{\\mathrm{e}^{-\\frac{1}{2} \\sum_{i} v_{i}^{2} + \\sum_{i} \\mathbf{W}_{\\mu i} v_{i}}}{Z_{\\mu}} = \\sum_{\\mu} w_{\\mu} \\frac{ \\mathrm{e}^{ -E_{\\mu}(\\boldsymbol{v}) }}{Z_{\\mu}} \\end{equation} where $w_{\\mu} = Z_{\\mu} / Z$ so that $\\sum_{\\mu} w_{\\mu} = 1$. During training, the model can control prior weights $w_{\\mu}$ by adjusting relative norms of patterns. If the difference in norms between the stored patterns is not too wild, $w_{\\mu} \\approx 1/N$.\nA single model in the mixture has an effective energy function derived from a joint energy function with just a single hidden unit,\n\\begin{equation} E_{\\mu} \\left( \\boldsymbol{v}, h_{\\mu} \\right) = - \\sum_{i} a_{i} (v_{i}) - b_{\\mu} (h_{\\mu}) - \\sum_{i} W_{i \\mu} v_{i} h_{\\mu} \\end{equation}\nLooking back at \\eqref{eq:effectivenergy}, we see that we can recover $E_{\\mu}(\\boldsymbol{v})$ by picking a hidden prior distribution that is a constant random variable so that $\\kappa_{\\mu}^{(1)}=1$ is the only non-zero cumulant. This frozen property of hidden units seems to agree with the fast dynamics of memory neurons in the dynamical systems model proposed in Krotov and Hopfield (2020)4.\nIn conclusion, the energy-based model underlying vanilla Transformer attention is not terribly exciting.\n3. Attention as implicit energy minimization Let\u0026rsquo;s finish this post with some comments on how one could leverage the idea of implicit energy minimization to develop novel attention mechanisms.\nBending the explicit architecture A lot of work on post-vanilla Transformer architectures tries to improve softmax-attention by making it more efficient through approximations and/or modifications at the level of the architecture. Kernel-based approaches like Rethinking Attention with Performers have shown not only that softmax attention can be efficiently approximated by a generalized attention mechanism but also that generalized ReLU-based attention performed better in practice. Papers like Normalized Attention Without Probability Cage show how we can replace the softmax non-linearity in \\eqref{eq:mhnupdate} with pure normalization and still end up with a competitive algorithm, noting that the updated query being restricted to lie in the convex hull of the stored patterns is a bias we might want to question.\nFrom the above examples, it seems like at least a part of current research on attention is trying to break away from the confines of existing, explicit attention architectures but doesn\u0026rsquo;t quite know how to do so in a principled way. Does an energy-based perspective help to understand these developments?\nFrom explicit architectures to implicit energy minimization We have seen in this post that the energy function behind the softmax attention mechanism can be understood as a mixture of simple energy-based models. But what can we actually do with this information? Especially since we know from language modeling experiments that \u0026ldquo;just scaling\u0026rdquo; these simple models to billions of parameters enables them to store enough patterns to be useful. Despite huge progress, there however remain important challenges in terms of efficiency and generalizability. Considering slightly less trivial energy-based models might address both by adding interactions in such a way that attention modules are able to return a collective response rather than a sum of decoupled contributions.\nTo some extent, the additional linear transformations on the input patterns in the query-key-value formulation of Transformer self-attention already try to address this: \\begin{equation} \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V} \\label{eq:vanilla-attention} \\end{equation} These linear transformations slightly generalize the \u0026ldquo;naked\u0026rdquo; explicit gradient step of \\eqref{eq:mhnupdate} and can in principle learn to cluster and direct patterns to neighborhoods in the energy landscape, parametrizing the energy function. But why stop there?\nDeep implicit layers for attention dynamics An interesting way forward might be to integrate attention with deep implicit layers. Funnily enough, the authors of the NeurIPS 2020 tutorial on Deep Implicit Layers list self-attention as a prime example of an explicit layer in their introductory notebook. Approaches like Deep Equilibrium Models implicitly train DEQ-Transformers but still consider the attention module itself an explicit function.\nYet we have seen in a previous post that self-attention can \u0026mdash; and perhaps should \u0026mdash; actually be considered an implicit layer solving for a fixed point query. Because of the lack of dynamics of the current generation of attention mechanisms, this can be done in a single big gradient step, removing the need to iterate. Attention models with more complicated dynamics might benefit from a differentiable solver to find a fixed point and return the most appropriate result in a given context.\nCompared to modifying explicit architectures, the implicit-layer perspective seems to act on a different \u0026ldquo;conceptual level\u0026rdquo; of neural network architecture design. This raises a lot of questions. Which families of attention architectures can be expressed in terms of implicit energy functions like softmax-attention? How many of these have efficient minimization properties with closed-form gradients? Beyond closed-form gradients, how far can we go in parametrizing more general energy-based attention models and still end up with an efficient algorithm? What does the trade-off look like between an attention model\u0026rsquo;s complexity and it still being implicitly trainable?\n4. Conclusion Looking back and reversing causation, one could argue that the now-famous dot-product attention module introduced in Attention Is All You Need5 could only have been arrived at because of the properties of its implicit energy function \\eqref{eq:mhnenergy}. Indeed, it is only because of the associative memory\u0026rsquo;s decoupled and rather crude way of storing patterns in isolated, high-dimensional valleys that expensive, implicit energy minimization steps can be traded for a cheap, explicit one-step gradient update like \\eqref{eq:mhnupdate}.\nThe obvious pitfall of continuing to hold on to the conceptual framework introduced by this shortcut is that a potentially far richer picture of (sparse) attention dynamics remains obscured. Rather than perpetually rethinking what is all you really need within the confines of existing, explicit attention modules, why not opt for implicit modules built on top of an energy-based perspective to try to push things forward?\nReferences \u0026amp; footnotes If you happen to find this work useful, please consider citing it as:\n@article{bal2020attentionrbms, title = {Transformer Attention as an Implicit Mixture of Effective Energy-Based Models}, author = {Bal, Matthias}, year = {2020}, month = {December}, url = {https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/}, }    Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, David J. Schwab, A high-bias, low-variance introduction to Machine Learning for physicists (2019)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Proof by Marcinkiewicz (1935) according to http://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n We are aware that this identification might be tremendously trivial when considering prior work on Implicit Mixtures of Restricted Boltzmann Machines or, more generally, mixture models in the context of expectation-minimization optimization.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention Is All You Need (2017)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1608627797,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609320857,"objectID":"6710ba4e2ffbeaf0ef110d190c98e7ed","permalink":"https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/","publishdate":"2020-12-22T10:03:17+01:00","relpermalink":"/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/","section":"post","summary":"Where does the energy function behind Transformers' attention mechanism come from?","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Energy-Based Models","Neural Networks","Renormalization Group","Restricted Boltzmann Machine","Statistical Physics","Transformers"],"title":"Transformer Attention as an Implicit Mixture of Effective Energy-Based Models","type":"post"},{"authors":[],"categories":[],"content":"XKCD 793: A physicist encountering machine learning for the first time  ✨ Update (November 2021): Please consider reading Transformers Are Secretly Collectives of Spin Systems for an arguably more comprehensive approach towards understanding transformers from a physics perspective.\n  Introduction A growing zoo of Transformers  Vanilla Transformers Beyond vanilla: confronting quadratic scaling   From Hopfield networks to Transformers  Classical discrete Hopfield networks Modern discrete Hopfield networks Modern continuous Hopfield networks Modern continuous Hopfield Networks as energy-based models Transformers store and retrieve context-dependent patterns Where are patterns stored in a Transformer?   Training Transformers  Pretraining loss functions Stepping through the Transformer: implicit energy minimization Meta-learning and few-shot inference   Beyond dot-product attention  Attention dynamics: embracing collective phenomena Why very long sequences should not be needed   Conclusion References \u0026amp; footnotes   1. Introduction In 2017, Attention Is All You Need 1 demonstrated state-of-the-art performance in neural machine translation by stacking only (self-)attention layers. Compared to recurrent neural networks, Transformer models exhibit efficient parallel processing of tokens, leading to better modeling of long-range correlations and, most importantly, favorable scaling in terms of data and compute. Since then, Transformers seem to have taken over natural language processing. Widespread adoption of attention-based architectures seems likely given recent work like An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale and the flurry of developments addressing the architecture\u0026rsquo;s quadratic scaling bottlenecks.\nRecently, the papers Hopfield Networks is All You Need 2 3 4 and Large Associative Memory Problem in Neurobiology and Machine Learning 5 provided complementary post-facto explanations of some of the success of Transformers from the perspective of energy-based models. In this post, I provide a biased overview of (self-)attention in Transformers and summarize its connections to modern Hopfield networks. Along the way, I look for intuition from physics and indulge in hand-wavy arguments on how an energy-based perspective can shed light on training and improving Transformer models.\n2. A growing zoo of Transformers Let\u0026rsquo;s start off with an overview of the components in a vanilla Transformer model. Since our focus is on (self-)attention, I am going to assume some prior knowledge6 and skip comprehensive architecture descriptions and experimental results. In Section 3, we will start from scratch and use Hopfield networks to build back up to the attention module described below.\nVanilla Transformers The proto-Transformer was introduced in an encoder-decoder context for machine translation in Attention Is All You Need. The original motivation seems to have been mostly driven by engineering efforts to model long-range correlations in sequence data and the recent successes of attention mechanisms stacked on top of recurrent neural networks. The main contribution and selling point of the paper was making an attention-only approach to sequence modeling work.\nLet\u0026rsquo;s focus on the encoder on the left and ignore the decoder on the right. Transformer models accept (batches of) sets of vectors, which covers most inputs people care about in machine learning. Text can be modelled as a sequence of embedded tokens. Images can be viewed as a snaky sequence of embedded pixels or embedded patches of pixels. Since sets have no notion of ordering, learned or fixed positional information needs to be explicitly added to the input vectors.\nThe main module in the Transformer encoder block is the multi-head self-attention, which is based on a (scaled) dot-product attention mechanism acting on a set of $d$-dimensional vectors:\n\\begin{equation} \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V} \\label{eq:vanilla-attention} \\end{equation}\nHere, queries $\\mathbf{Q}$, keys $\\mathbf{K}$, and values $\\mathbf{V}$ are matrices obtained from acting with different linear transformations \u0026mdash; parametrized respectively by weights $\\mathbf{W}_{\\mathbf{Q}}$, $\\mathbf{W}_{\\mathbf{K}}$, and $\\mathbf{W}_{\\mathbf{V}}$ \u0026mdash; on the same set of $d$-dimensional inputs. Cross-attention takes the inputs for its queries from a different source than for its keys and values, as can be glimpsed from the decoder part of the architecture on the right.\nFor every input query, the updated output query of \\eqref{eq:vanilla-attention} is a linear combination of values weighted by an attention vector quantifying the overlap of the input query with the keys corresponding to these values. Stacking input query attention vectors leads to an attention matrix. Since all objects are vectors and the attention mechanism is just a dot product between vectors, we can think of the attention module as matching query vectors to their \u0026ldquo;closest\u0026rdquo; key vectors in latent space and summing up contributions from value vectors, weighted by the \u0026ldquo;closeness\u0026rdquo; of their keys to the queries.\nThe remaining components of the Transformer encoder block are needed to make the module work properly in practice:\n The multi-headedness of the attention module refers to chunking up the dimension of the vector space and having multiple attention operations running in parallel in the same module, yet with each acting on a lower-dimensional segment of the full space. This is a trick to (1) get around the fact that every input vector only couples to one query at a time to calculate its attention coefficient, and (2) provide multiple starting points in the subspaces for the queries, which might help to avoid bad local minima in parameter space during optimization. A positional feed-forward network, made up of two linear layers with a non-linearity in between, is inserted at the end of the module. Folklore wisdom tells us that the feed-forward layer needs to blow up the dimension of the latent space by a factor of four for it to be able to \u0026ldquo;disentangle\u0026rdquo; the represention. More likely though, it\u0026rsquo;s a way to increase model capacity and warp latent spaces since the attention modules on their own are pretty much linear apart from the $\\mathrm{softmax}$-operator used to obtain the normalized attention coefficients. Residual connections are added to control the flow of gradients. Layer normalisation is used to control learning dynamics and keep vector norms from exploding.  Beyond vanilla: confronting quadratic scaling Most architectural variations of the vanilla Transformer are targeted at the attention module, which scales poorly with respect to the input sequence length $N$. Since the overlap of all queries with all keys is required, calculating a dense attention matrix scales like $\\mathcal{O}(N^2)$ in time and space. Limits on the context window of the attention mechanism during training prevent the model from learning how to deal with long sequences and long-range correlations. The majority of post-vanilla Transformer species can be classified into one of the following buckets6:\n Low-rank approximations: truncate the matrix product $\\mathbf{Q} \\mathbf{K}^T$ since it\u0026rsquo;s likely not full rank for structured data Sparsification: reduce the attention calculation from all query-key pairs to a subset because not all of them feel the need to talk to each other Recurrence: keep track of a (compressed) history of context Kernels: approximate the attention operation with kernel methods  For the remainder of our discussion, we will focus on vanilla Transformers. One of the goals of this blog post is to explore how a different perspective on the function of attention-based algorithms might lead to qualitatively different improvements beyond what is possible by relying on scaling and reducing computational complexity alone.\n3. From Hopfield networks to Transformers In this section, we provide a short history of Hopfield networks and gradually build up intuition until we can recognize the Transformer self-attention mechanism for what it really is. We refer to the blog post accompanying Hopfield Networks is All You Need for more details and insightful visualizations of pattern storage and retrieval.\nClassical discrete Hopfield networks A Hopfield network is a simple model for associative memory popularized by John Hopfield in his 1982 paper Neural Networks and Physical Systems with Emergent Collective Computational Abilities7. The task of an associative memory is to store and retrieve patterns, preferably in a way that allows one to recover stored patterns quickly with a low error rate.\nThe basic idea of the Hopfield network \u0026mdash; and other energy-based models like Boltzmann machines \u0026mdash; is to construct an energy function which defines an energy landscape containing basins of attraction around patterns we want to store. Starting at any pattern, we want to have an update rule pointing towards the closest stored pattern, guided by a scalar \u0026ldquo;closeness\u0026rdquo; score provided by the energy function.\n\nLet\u0026rsquo;s make this a bit more formal but not too formal. Consider trying to store a set of $N$ binary patterns $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$ where each pattern $\\boldsymbol{x}_{i}$ is a $d$-dimensional vector whose entries are either $-1$ or $1$. For example, in the case of storing black-and-white images, every image would correspond to a string of pixel values, a binary pattern $\\boldsymbol{x}_{i}$.\nFor any query $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$, or state pattern, we want to find a way to retrieve the closest stored pattern. In his paper, Hopfield considered the energy function\n\\begin{equation} E = - \\frac{1}{2} \\boldsymbol{\\xi}^{T} \\boldsymbol{W} \\boldsymbol{\\xi} + \\boldsymbol{\\xi}^{T} \\boldsymbol{b} = - \\frac{1}{2} \\sum_{i=1}^{d} \\sum_{j=1}^{d} w_{ij} \\xi_{i} \\xi_{j} + \\sum_{i=1}^{d} b_{i} \\xi_{i} , \\label{eq:ising} \\end{equation}\nwhere $\\boldsymbol{b} \\in \\mathbb{R}^{d}$ denotes a bias vector and the weights $\\boldsymbol{W} \\in \\mathbb{R}^{d \\times d}$ are set to the sum of the outer products of the patterns we want to store\n\\begin{equation} \\boldsymbol{W} = \\sum_{i=1}^{N} \\boldsymbol{x}_{i} \\otimes \\boldsymbol{x}_{i}^{T}. \\end{equation}\nThe state pattern update rule is given by the sign of the gradient of \\eqref{eq:ising} with respect to $\\boldsymbol{\\xi}$ and can be done in one step (synchronously) or separately for every component of the vector (asynchronously):\n\\begin{equation} \\boldsymbol{\\xi}_{n+1} = \\mathrm{sgn} \\left( \\boldsymbol{W} \\boldsymbol{\\xi}_{n} - \\boldsymbol{b} \\right). \\end{equation}\nThe storage capacity of this system for retrieval of patterns with a small amount of errors can be shown to be $C \\cong 0.14 d$, scaling linearly with the dimension of the pattern vector.\nPhysical intuition Physicists immediately recognize the energy function \\eqref{eq:ising} as an incarnation of the Ising model. Spin degree of freedoms $\\xi_{i}$ are grouped into patterns $\\boldsymbol{\\xi}$ that are equivalent to spin configurations of $d$ spins. The weight matrix is a sum of stored-pattern spin configurations, serving as attractors for the state-pattern spin configuration. The couplings $w_{ij}$ can be regarded a sum of samples of an underlying pattern data distribution. They are not restricted to (nearest-)neighbors and their values are neither uniform like in exactly solvable models nor totally random like in spin glass models.\n Neural networks and spin glasses: There is some literature on connections between spin glasses and neural networks. Spin glasses are phases of matter describing disordered magnetic systems exhibiting both quenched disorder and frustratation. Spin glasses were a major inspiration for Hopfield networks, as beautifully explained by the condensed matter physicist Philip W. Anderson in a column series for Physics Today (1988-1990). However, apart from Efficient training of energy-based models via spin-glass control 8, I could not find any recent papers that point to a productive research direction beyond qualitative statements like \u0026ldquo;here\u0026rsquo;s two hard problems where symmetry and order will not help you solve them\u0026rdquo;.\n Modern discrete Hopfield networks Modern discrete Hopfield networks (or dense associative memories) introduced the following family of energy functions to improve pattern storage capacity and pattern separation capabilities 9 10\n\\begin{equation} E = - \\sum_{i=1}^{N} F \\left( \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{\\xi} \\right) \\end{equation}\nCompared to the classical discrete Hopfield network energy function \\eqref{eq:ising}, the explicit weight matrix is gone and the energy has been reduced to a sum of a function of dot products between the state pattern $\\boldsymbol{\\xi}$ and every stored pattern $\\boldsymbol{x}_i$. For a polynomial interaction function $F(x) = x^{a}$, low-error storage capacity is $C \\cong d^{a-1}$. The quadratic, classical discrete Hopfield network is recovered by setting $a=2$.\nEssentially, the role of $F(x)$ is to separate close patterns by blowing up differences in dot product values. Few things blow up better than exponentials, so we can generalize the energy to\n\\begin{equation} E = - \\sum_{i=1}^{N} \\exp \\left( \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{\\xi} \\right) \\end{equation}\nwith storage capacity $C \\cong 2^{d/2}$. The corresponding update rules for modern discrete Hopfield networks can be shown to converge quickly with high probability10.\nModern continuous Hopfield networks Most machine learning applications are tailored to work with continuous embeddings (vector representations) rather than discrete patterns. Is there a way to generalize modern Hopfield networks to continuous data? Recently, Hopfield Networks is All You Need proposed the following energy function to deal with continuous $d$-dimensional patterns11:\n\\begin{equation} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right), \\label{eq:energyfunc} \\end{equation}\nwhich we consider to be a function of the state pattern $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ and parametrized by $N$ stored patterns $\\boldsymbol{X} = (\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{N}) \\in \\mathbb{R}^{d \\times N}$. From the point of view of restricted Boltzmann machines, the stored patterns $\\boldsymbol{X}^T$ can also be interpreted as weights mapping $\\boldsymbol{\\xi}$ to hidden units5.\n Smoothly taking a maximum: The $\\mathrm{logsumexp}$ operator is defined for vectors $\\mathbf{x}$ as \\begin{equation} \\mathrm{logsumexp} \\left( \\mathbf{x} \\right) = \\log \\left( \\sum_{i=1}^{N} \\mathrm{e}^{x_i} \\right) \\end{equation} while for matrix arguments (like a batch of vectors), the $\\mathrm{sumexp}$ is understood to apply to just one dimension after which the $\\log$ acts element-wise on the resulting vector.\n Physical intuition We assume that the stored patterns equilibrate much quicker than those of the state pattern so that the former can effectively be considered \u0026ldquo;frozen\u0026rdquo;. The energy function \\eqref{eq:energyfunc} looks deceptively simple: there is a single state pattern and there are no interactions among stored patterns. The first term takes care of making sure the norm of the input state pattern is finite, while the second term scores the query\u0026rsquo;s overlap based on its individual alignment with every stored pattern. The exponential function in the term\n\\begin{equation} \\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) = \\log \\left( \\sum_{i=1}^{N} \\mathrm{e}^{\\mathbf{x}_i \\cdot \\boldsymbol{\\xi}} \\right) \\end{equation}\nis used to pull apart close patterns by blowing up differences in the dot product between state pattern and stored patterns. From the perspective of the query, it is not so much an interaction term but rather a measure of the alignment of the query to external \u0026ldquo;magnetic fields\u0026rdquo; generated by the stored patterns.\nDeriving the update rule In the spirit of hand-waving, let us refuse to resort to of the dynamical systems machinery used in the original references 2 5 and rather derive the update rule for the state pattern $\\boldsymbol{\\xi}$ by taking the derivative of the energy function \\eqref{eq:energyfunc} with respect to $\\boldsymbol{\\xi}$\n\\begin{equation} \\nabla_{\\boldsymbol{\\xi}} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\boldsymbol{\\xi} - \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right). \\end{equation}\nA gradient descent update with step size $\\gamma$ looks like\n\\begin{equation} \\boldsymbol{\\xi}_{n+1} = \\boldsymbol{\\xi}_{n} - \\gamma \\left( \\boldsymbol{\\xi}_{n} - \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi}_{n}\\right) \\right). \\label{eq:conthopfupdate} \\end{equation}\nWe are very confident that the topography of the energy landscape allows us to take big steps and boldly set $\\gamma = 1$ to recover the familiar update rule\n\\begin{align} \\boldsymbol{\\xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi}_{n}\\right) . \\end{align}\nThe updated vector is a linear combination of all stored patterns, weighted by an attention vector quantifying the overlap with the input pattern.\nModern continuous Hopfield Networks as energy-based models Let\u0026rsquo;s now try to connect the system defined by the energy function \\eqref{eq:energyfunc} to the statistical mechanics framework of energy-based models 12 13.\nEnergy-based models: a gentle introduction Energy-based models learn a parametrized energy function $E_{\\theta}$ which maps data points $\\boldsymbol{x}$ to real, scalar energy values $E_{\\theta}(\\boldsymbol{x})$. The data distribution is modeled by the Boltzmann distribution, \\begin{equation} p_{\\theta}(\\boldsymbol{x}) = \\frac{\\mathrm{e}^{ - E_{\\theta}(\\boldsymbol{x}) }}{Z(\\theta)}, \\label{eq:boltzmann} \\end{equation} where $Z(\\theta) = \\int \\mathrm{d} \\boldsymbol{x} \\ \\mathrm{e}^{-E(\\boldsymbol{x})}$ denotes the system\u0026rsquo;s partition function. Configurations $\\boldsymbol{x}$ with low energies $E_{\\theta}(\\boldsymbol{x})$ are considered more likely and their weight contributes more strongly to the partition function.\nTo steer the model distribution $p_{\\theta}$ towards a target data distribution $p_{\\mathrm{data}}$, we can try to minimize the likelihood loss function\n\\begin{equation} \\mathcal{L}_{\\mathrm{ML}} (\\theta) = \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\mathrm{data}}} \\left[ -\\log p_{\\theta} (\\boldsymbol{x}) \\right], \\label{eq:nll} \\end{equation}\nwhere the negative log-likelihood equals\n\\begin{equation} -\\log p_{\\theta} (\\boldsymbol{x}) = E_{\\theta} (\\boldsymbol{x}) + \\log Z (\\theta). \\end{equation}\nThis is a hard optimization problem because calculating $\\log Z (\\theta)$ is hard for the vast majority of high-dimensional data distributions we care about. In practice, people resort to approximations like contrastive divergence to push the energy down on \u0026ldquo;positive examples\u0026rdquo; drawn from the data distribution while pushing up on \u0026ldquo;negative examples\u0026rdquo; obtained from sampling the model distribution. Even though sampling from \\eqref{eq:boltzmann} can be done with methods like Markov Chain Monte Carlo, it is computationally expensive to do so, especially as part of an inner-loop optimization step14.\nExactly optimizing modern continuous Hopfield networks So what about the system defined by the energy function \\eqref{eq:energyfunc}? Let\u0026rsquo;s consider the stored patterns $\\mathbf{X} \\in \\mathbb{R}^{d \\times N}$ as the model parameters we want to optimise. The task for the model is then to try to memorise incoming state patterns $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ drawn from some data distribution $p_{\\mathrm{data}}$ by deciding what kind of patterns to store. The partition function looks like\n\\begin{equation} Z = \\int \\mathrm{d} \\boldsymbol{\\xi} \\ \\mathrm{e}^{-E(\\boldsymbol{\\xi})} = \\int \\mathrm{d} \\boldsymbol{\\xi} \\ \\mathrm{e}^{-\\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi}} \\left( \\sum_{i=1}^{N} \\mathrm{e}^{ \\boldsymbol{x}^{T}_{i} \\cdot \\boldsymbol{\\xi} } \\right) \\label{eq:zforcontinuoushopfield} \\end{equation}\nwhich, because of the $\\log$ in the \u0026ldquo;interaction term\u0026rdquo;, boils down to a sum of $n$-dimensional Gaussian integrals\n\\begin{aligned} Z = (2\\pi)^{n/2} \\sum_{i=1}^{N} \\mathrm{e}^{ \\frac{1}{2} \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{x}_{i} } \\end{aligned}\nAfter taking the logarithm, we end up with the $\\mathrm{logsumexp}$ operator:\n\\begin{equation} \\log Z = \\frac{n}{2} \\log \\left( 2\\pi \\right) + \\mathrm{logsumexp} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) \\end{equation}\nwhere the $\\mathrm{diag}$ operator is understood to turn the diagonal of its matrix argument into a vector. Plugging this expression into \\eqref{eq:nll} leads to the following loss function for the matrix of stored patterns\n\\begin{align} \\mathcal{L}_{\\mathrm{ML}} (\\mathbf{X}) = \u0026amp; \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\ \u0026amp; + \\mathrm{logsumexp} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) + \\frac{n}{2} \\log \\left( 2\\pi \\right) \\end{align}\nand a gradient\n\\begin{align} \\nabla_{\\mathbf{X}} \\mathcal{L}_{\\mathrm{ML}} (\\mathbf{X}) = \u0026amp; - \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\boldsymbol{\\xi} \\otimes \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\ \u0026amp; + \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) \\end{align}\nand an update with step size $\\gamma$\n\\begin{align} \\mathbf{X}_{n+1} = \\ \\mathbf{X}_{n} \u0026amp;+ \\gamma \\ \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\boldsymbol{\\xi} \\otimes \\mathrm{softmax} \\left( \\boldsymbol{X}^T_{n} \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\ \u0026amp; - \\gamma \\ \\mathbf{X}_{n} \\ \\mathrm{softmax} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T}_{n} \\boldsymbol{X}_{n} \\right) \\right) \\end{align}\nLet\u0026rsquo;s try to guess what this means for a single input state pattern. The first gradient term pushes all stored patterns towards the sample but weighted by a dot-product attention vector quantifying their overlap with the input pattern, similar to \\eqref{eq:conthopfupdate} but in the other direction. The second gradient term comes from the partition function and acts as a regularizer by keeping the norms of the stored patterns in check. Regularization keeps pattern values within a reasonable range and pushes the system towards regions in parameter space with non-trivial small dot-product values.\nTransformers store and retrieve context-dependent patterns Making the leap from modern continous Hopfield networks to the vanilla Transformer (self-)attention mechanism we encountered in Section 2 requires a few additional steps, as explained in detail in the blog post accompanying Hopfield Networks is All You Need.\n We want to act on multipe $d$-dimensional state patterns at the same time in order to retrieve multiple updated patterns in parallel: \\begin{align} \\boldsymbol{\\xi} \\in \\mathbb{R}^{d} \\to \\boldsymbol{\\Xi} = (\\boldsymbol{\\xi}_{1}, \\ldots, \\boldsymbol{\\xi}_{S}) \\in \\mathbb{R}^{d \\times S} \\end{align} so that \\begin{align} \\boldsymbol{\\Xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi}_{n}\\right) . \\end{align} In practice, the number of state patterns $S$ is often taken to be equal to the number of stored patterns $N$. We want to map stored patterns $\\mathbf{X}$ and state patterns $\\boldsymbol{\\Xi}$ respectively to keys $\\mathbf{K} \\in \\mathbb{R}^{N \\times d}$ and queries $\\mathbf{Q} \\in \\mathbb{R}^{S \\times d}$ in a common feature space using linear transformations $\\mathbf{W_{K}}$ and $\\mathbf{W_{Q}}$. We want introduce another linear transformation $\\mathbf{W_{V}}$ on stored patterns to transform them into values $\\mathbf{V} \\in \\mathbb{R}^{N \\times d}$ appropriate for the keys' content. We want to modify the learning dynamics by decreasing the inverse temperature to $\\beta = 1 / \\sqrt{d}$, effectively making the $\\mathrm{softmax}$ softer by increasing the temperature of the system15. Physically, this might correspond to warming up the system just enough to get out of the spin-glass phase while not introducing too much thermal noise8.  The result is the update rule we stated without explanation in Section 2: \\begin{equation} \\mathbf{Q}^{\\mathrm{updated}} = \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V}, \\label{eq:transformerattnupdate} \\end{equation} where the $\\mathrm{softmax}$ acts row-wise. In practice, the vanilla Transformer module additionally wraps the above attention module in (1) residual connections to control the flow of gradients, (2) layer norms to control pattern normalisations and learning dynamics, and (3) a positional feed-forward network for additional model capacity.\nWhere are patterns stored in a Transformer? Let\u0026rsquo;s try to digest the implications of these quite substantial changes. It\u0026rsquo;s useful to think of Transformer (self-)attention modules as dynamic pattern storage and retrieval systems. In modern continuous Hopfield networks, stored patterns are considered a given. However, in the Transformer (self-)attenton module, patterns to be matched and retrieved are dependent on inputs and implicitly stored in the weights $\\mathbf{W_{Q}}$, $\\mathbf{W_{K}}$, and $\\mathbf{W_{V}}$ of the linear transformations. In every layer, the module needs to learn how to map a set of inputs to patterns it wants to store (keys and values) as well as how to best retrieve them (queries). Within the same layer, dynamically generated queries are matched to keys within the same latent space. Between attention modules of neighboring layers, the non-linear activation function in the positional feed-forward network warps latent spaces.\n4. Training Transformers Now that we are aware of an energy-based interpretation of dot-product (self-)attention, we can start hand-waving about what could be going on during the supervised training procedure of Transformer models and how energy-based models suggest a qualitatively different approach to improving attention mechanisms.\nPretraining loss functions The goal of pretraining loss functions is to induce useful data-dependent pattern storage and retrieval behavior. Pretraining strategies for Transformer-based language models rely on loss functions derived from auxiliary tasks to learn statistical patterns in natural language. Starting from almost identical model architectures, autoregressive models like GPT-3 leverage all their parameters to predict the next token in a sequence given previous tokens while autoencoding models like BERT try to reconstruct corrupted tokens. In both cases, the loss function is a cross-entropy loss involving predictions in the space of the model\u0026rsquo;s token vocabulary.\nStepping through the Transformer: implicit energy minimization Although no energy function is explicitly optimized during training16, let\u0026rsquo;s see how far we can push hand-wavy energy-based arguments by stepping through the forward and backward pass of a Transformer model. We have learned that the attention update \\eqref{eq:transformerattnupdate} in every Transformer layer is actually a hidden gradient step. This trivial insight leads to a trio of trivial observations.\nTrivial Observation #1: During training, the update step \\eqref{eq:transformerattnupdate} of the attention mechanism in a Transformer layer acts as an inner-loop optimization step, minimizing an implicit energy function determined by the queries, keys, and values constructed from the output of the previous layer.\nTrivial Observation #2: During the forward pass of a deep Transformer model, a nested hierarchy of energy functions is minimized.\nTrivial Observation #3: During the backward pass of a deep Transformer model, the parameters of its attention modules get updated such that the inner-loop optimization steps conspire to pattern match queries to keys in such a way that the sequentially-updated final latent representations are useful for improving the loss.\nMeta-learning and few-shot inference Squinting our eyes, we can see traces of a meta-learning problem: how to tune model parameters \u0026mdash; in particular the attention mechanisms' linear transformation matrices \u0026mdash; such that applying a sequence of one-step attention updates to sets of input patterns converges to representations useful for minimizing the (meta-)loss function. Learnable modules of a differentiable program can of course often be considered part of a larger meta-learning setup. But what this point of view suggests is that confining the one-step inner-loop update to a simple associative memory pattern lookup might be quite restrictive.\nYet even with with a simple dense associative memory, OpenAI\u0026rsquo;s paper Language Models are Few-Shot Learners showed that large-capacity models like GPT-3 already exhibit quite impressive meta-learning capabilities. The energy-based perspective provides a naive yet attractive explanation for this phenomenon. At inference time, the few-shot demonstrations, which make up the initial part of a few-shot learning query, condition the sequential generation process by providing basins of attraction in the energy landscape for other energy minimization steps to be pulled towards. The GPT-3 model is memorizing to the extent the demonstrations match patterns seen during training and generalizing within the possibilities of the rudimentary attention dynamics of the simple underlying energy functions.\n5. Beyond dot-product attention Let\u0026rsquo;s conclude this post with two related thoughts inspired by an energy-based perspective on current attention architectures: attention dynamics and modeling very long sequences.\nAttention dynamics: embracing collective phenomena We have seen that the energy function of a modern continuous Hopfield network \\eqref{eq:energyfunc} is rather uninspiring from a physics perspective. Theoretically, the exponential storage and efficient retrieval of patterns is obtained by burning deep valleys into the energy landscape around stored patterns (keys) for neighbouring state patterns (queries) to quickly roll into. In practice, the authors of Hopfield Networks is All You Need observed three kinds of fixed-point behavior in a pretrained BERT model: (1) global fixed points averaging over all stored patterns, (2) metastable states averaging over a subset of stored patterns, and (3) fixed points returning a single, well-separated stored pattern.\nWhat does this tell us? Assuming the attention updates converge faithfully during training, the linear maps turning input vectors into queries, keys, and values can become bottlenecks in terms of being able to separate patterns and organise the energy landscape. Additionally, the lack of interactions among patterns and the decoupled dot-product overlap between queries and keys puts considerable limits on how the network can process information. In practice, this is being partially addressed by using multiple attention heads (see Section 2), but this solution does not feel satisfactory.\nWhy very long sequences should not be needed Recurrent neural networks try to compress patterns in a single hidden state via sequential propagation but often fail to do so and forget stuff along the way. Transformers bake patterns into a hierarchical energy landscape but focus on a fixed-length context window to store and retrieve patterns. As we\u0026rsquo;ve seen in Section 2, a lot of research on improving Transformers focuses on alleviating the $\\mathcal{O}(N^2)$ bottleneck of the attention computation with the implicit goal of scaling to longer sequences and enabling larger context windows.\nBut very long sequences should not be needed if patterns are allowed to talk to each other. A model should not need all of the world as context if patterns and emergent concepts can be connected. It\u0026rsquo;s definitely worthwhile to try to reduce the computational complexity of current attention architectures, but it might be far more valuable to swap the simple energy-based model \\eqref{eq:energyfunc} for more interesting energy-based models. Why not dust off the old unrestricted Boltzmann machine once again? Or experiment with any one of a century\u0026rsquo;s worth of physics models? Not to train them explicitly, but have them serve as implicit models underlying more intricate attention mechanisms, mediated by (local) interactions among patterns. Naturally, after so much hand-waving, our journey has to end here.\n6. Conclusion Even if attention turns out to not be all we need, (self-)attention modules have established themselves as highly parallelizable neural network building blocks capable of dynamically routing information based on context. We have seen that dot-product attention modules in Transformer models work by encoding high-dimensional patterns into the landscapes of simple energy functions, enabling fast pattern storage and retrieval. During training, these landscapes are sculpted to accommodate statistical patterns found in data by hierarchically matching and combining latent pattern representations through a sequence of implicit energy function minimizations.\nWe argued that an energy-based perspective on attention provides an intuitive explanation of meta-learning capabilities of large-capacity language models and encourages the exploration of qualitatively different attention mechanisms for pattern storage and retrievel. Rather than naively scaling the current generation of Transformers, it might be more rewarding to scale learning itself by exploring more powerful, expressive, and computationally efficient attention mechanisms, guided by energy-based models. Perhaps we should consider looking at neural networks again like John Hopfield already did in 1982: physical systems with emergent collective computational abilities.\nReferences \u0026amp; footnotes If you happen to find this work useful, please consider citing it as:\n@article{bal2020energyattention, title = {An Energy-Based Perspective on Attention Mechanisms in Transformers}, author = {Bal, Matthias}, year = {2020}, month = {December}, url = {https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/}, }    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention Is All You Need (2017)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, Hopfield Networks is All You Need (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Johannes Brandstetter, https://ml-jku.github.io/hopfield-layers/ (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Johannes Brandstetter and Hubert Ramsauer, https://ml-jku.github.io/blog-post-performer/ (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n If you have only just joined the attention revolution, there are a lot of great resources out there to get you started. Yannic Kilcher provides a great introduction in his video on Attention is All You Need. The High Performance NLP tutorial slides presented at EMNLP 2020 contain a thorough and visually appealing introduction to attention-based models. Because code is usually more to the point than papers that need to sell themselves, I highly recommend Phil Wang\u0026rsquo;s excellent collection of self-contained repositories showcasing some of the latest models and techniques.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n John Hopfield, Neural Networks and Physical Systems with Emergent Collective Computational Abilities (1982)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Alejandro Pozas-Kerstjens, Gorka Muñoz-Gil, Miguel Ángel García-March, Antonio Acín, Maciej Lewenstein, Przemysław R. Grzybowski, Efficient training of energy-based models via spin-glass control (2019)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Dense Associative Memory for Pattern Recognition (2016)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Mete Demircigil, Judith Heusel, Matthias Löwe, Sven Upgang, and Franck Vermet, On a Model of Associative Memory with Huge Storage Capacity (2017)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n A physicist might consider these continuous patterns spin configurations of the degrees of freedom in a vector spin model where the internal dimension $D \\sim 10^2-10^4$ is much bigger than familiar small-$D$ cases like the XY model or the Heisenberg model but much smaller than infinity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Yann LeCun, Sumit Chopra, Raia Hadsell, Marc\u0026rsquo;Aurelio Ranzato, and Fu Jie Huang, A Tutorial on Energy-Based Learning (2006) and Yann LeCun and Alfredo Canziani, Deep Learning DS-GA 1008 course (2020)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, and David J. Schwab, A high-bias, low-variance introduction to Machine Learning for physicists (2019)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The generator in a Generative Adverserial Network (GAN) setup can be considered a clever way to generate negative samples for the implicit energy function optimization taking place in the discriminator.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n As we have seen in Section 2, the naive interpretation of $\\beta$ as the effective inverse temperature is tenuous in practice given the influence of the surrounding layer normalisation modules.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The implicitly defined energy functions in Tranformer layers are not optimized directly because they arguably do not provide a meaningful training signal on their own. Verifying whether this is true or not could make for an interesting experiment.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":1606557261,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606995021,"objectID":"8e7322e243b34c70767e389210fb4d59","permalink":"https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/","publishdate":"2020-11-28T10:54:21+01:00","relpermalink":"/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/","section":"post","summary":"Can an energy-based perspective shed light on training and improving Transformer models?","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Energy-Based Models","Hopfield Networks","Neural Networks","Statistical Physics","Transformers"],"title":"An Energy-Based Perspective on Attention Mechanisms in Transformers","type":"post"}]