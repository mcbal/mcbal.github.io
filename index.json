[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mcbal.github.io/author/matthias-bal/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matthias-bal/","section":"authors","summary":"","tags":null,"title":"Matthias Bal","type":"authors"},{"authors":[],"categories":[],"content":"  Introduction Mean-field theory for disordered systems  Random Ising models (or Boltzmann machines or \u0026hellip;) Adaptive Thouless\u0026ndash;Anderson\u0026ndash;Palmer mean-field theory   Attention as a fixed-point method  Generalizing spin models to vector degrees of freedom Deep implicit attention: attention as a collective response Slow and explicit: solving the adaptive TAP equations Fast and neural: parametrizing the Onsager self-correction term   A mean-field theory perspective on transformers  Parametrizing the couplings: sparse graph structure from inputs Softmax attention does a single, naive mean-field update step Feed-forward layer corrects naive mean-field update   Conclusion and outlook Related work   1. Introduction To explore progress beyond the cage of softmax attention, we have previously looked at energy-based perspectives on attention mechanisms:\n An Energy-Based Perspective on Attention Mechanisms in Transformers Transformer Attention as an Implicit Mixture of Effective Energy-Based Models Attention as Energy Minimization: Visualizing Energy Landscapes  The main take-away so far has been that you can think of softmax attention as implementing a single, big gradient step of some energy function and that training transformers is akin to meta-learning how to best tune a stack of attention and feed-forward modules to perform well on some auxiliary (meta-)task(s). But what can an energy-based perspective actually provide beyond quaint and hand-wavy statements like implicit energy landscapes are sculpted every time you train a transformer?\nIn this post, we approach attention in terms of the collective response of a statistical-mechanical system. Attention is interpreted as an inner-loop fixed-point optimization step which returns the approximate response of a system being probed by data. This response is a differentiable compromise between the system\u0026rsquo;s internal dynamics and the data it\u0026rsquo;s being exposed to. To better respond to incoming data, outer-loop optimization steps can nudge the interactions and the self-organizing behaviour of the system.\nTo implement our proposal, we combine old ideas and new technology to construct a family of attention mechanisms based on fixed points. We use deep equilibrium models to solve a set of self-consistent mean-field equations of a vector generalization of the random Ising spin-model. By approximating these equations, we arrive at simplified update steps which mirror the vanilla transformer architecture. We conclude by showing how transformers can be understood from a mean-field theory perspective.\n ✨ Code: A reference PyTorch implementation of the ideas outlined in this blog post is available in the repository deep-implicit-attention. Comments welcome.\n 2. Mean-field theory for disordered systems In physics, mean-field theory is an approximation method to study models made up of many individual degrees of freedom that interact with each other. Mean-field theory approximates the effect of the environment on any given individual degree of freedom by a single, averaged effect, and thus reduces a many-body problem to an (effective) one-body problem. This is a drastic approximation. Whether mean-field theory a sensible thing to do depends on the problem and the properties of your variational ansatz.\n Mean-field theory \u0026amp; variational methods: From the point of view of variational methods, mean-field theory tries to approximate a complicated object (like a partition function of a statistical-mechanical system) by wiggling around the parameters of a tractable variational ansatz to get as close as possible to the real thing. You can picture this process as projecting down a complicated object living in a high-dimensional space to its shadow in an easier-to-handle subspace (I can hear a mathematician fainting in the background). This effectively reduces the problem to optimizing for the best possible approximation within your variational class. A lot of mean-field machinery also shows up in probability theory, statistics, and machine learning where it appears in belief propagation, approximate variational inference, expectation propagation, etc.\n In the next two subsections, we introduce random Ising models and sketch a physics-inspired approach to deal with disordered models using mean-field theory. In Section 3 we will then generalize these results to vector spin degrees of freedom and propose two flavours of attention models.\n2.1. Random Ising models (or Boltzmann machines or \u0026hellip;) The random Ising model is a prototypical model in the study of spin glasses and disordered random systems, where it is often referred to as the Sherrington–Kirkpatrick model, famous for its replica-method solution by Giorgio Parisi in 1979. Its energy function with external field for $N$ classical, binary spin variables looks like\n\\begin{equation} E = \\sum_{i,j} J_{ij} S_{i} S_{j} + \\sum_{i} x_{i} S_{i}, \\label{eq:randomising} \\end{equation}\nwhere the couplings $J_{ij}$ between degrees of freedom are randomly distributed according to some probability distribution and self-interactions are absent ($J_{ii} = 0$). The external magnetic fields $x_{i}$ provide a preferential direction of alignment at every local site. Since the elements in the coupling matrix can have both negative and positive signs, the system is said to have both frustrated ferro- as well as antiferromagnetic couplings. The model defined by \\eqref{eq:randomising} is also known as a Boltzmann machine or a Hopfield network.\nIn contrast with disordered systems, we expect the couplings in the context of artificial neural networks to no longer be randomly drawn from a distribution but to reflect structure and organization between spins after being exposed to data. The system should self-organize in order to better respond to incoming data.\nA cartoon of a spin configuration of a 7-spin system looks something like where we have only drawn the connections strongest in absolute value. It\u0026rsquo;s helpful to think of classical spin degrees of freedom as arrows. For vector spins, we can imagine lifting the up/down restriction and letting the arrows rotate freely.\n2.2. Adaptive Thouless\u0026ndash;Anderson\u0026ndash;Palmer mean-field theory One of the approaches physicists have come up with to tackle disordered random systems with pairwise interactions like those in Eq. \\eqref{eq:randomising} is Thouless\u0026ndash;Anderson\u0026ndash;Palmer (TAP) mean-field theory (1977). The TAP equations improve mean-field theory results by adding a so-called Onsager self-correction term calculated from the couplings' distribution.\nOpper and Winther (2001) adapted this method to probabilisic modeling to be able to deal with scenarios where the distribution of the couplings between spins is not known a priori. To compensate for the lack of knowledge of the couplings distribution, they introduced a self-consistent computation to adapt the Onsager correction to the actual couplings using the cavity method and linear response relations. We will sketch the adaptive TAP approach below but refer to Opper and Winther (2001) and Raymond, Manoel, and Opper (2014) for more details and derivations.\nSingle-site partition function from cavity method The adaptive TAP equations can be derived using the cavity method, where a cavity field distribution is introduced to rewrite the marginal distributions of the spins. The cavity corresponds to the \u0026ldquo;hole\u0026rdquo; left by removing a single spin. By assuming a Gaussian cavity distribution in the large connectivity limit, one can show that the single-site partition function looks like\n\\begin{equation} Z_{0}^{(i)} = \\int \\mathrm{d} S \\ \\rho_{i}\\left(S\\right) \\exp \\left[ S \\left( a_{i} + x_{i} \\right) + \\frac{V_{i} S^2}{2} \\right] \\end{equation}\nwhere the $a_i$ denote cavity means and the $V_i$ cavity variances. The single-site partition function can be integrated to yield an explicit expression after choosing well-behaved priors $\\rho_{i}(S)$ for the spins. For binary spins $S=\\pm 1$, we can pick $\\rho_{i}(S)=\\frac{1}{2}\\left( \\delta(S-1) + \\delta(S+1) \\right)$ to find\n\\begin{equation} Z_{0}^{(i)} = \\cosh \\left( a_{i} + x_{i} \\right). \\label{eq:partfunbinaryspins} \\end{equation}\nCavity means and Onsager correction term The cavity means can be shown to be given by \\begin{equation} a_{i} = \\sum_{j} J_{ij} \\langle S_{j} \\rangle - V_{i} \\langle S_{i} \\rangle. \\label{eq:cavitymean} \\end{equation}\nwhere the last term is the Onsager correction term, a self-correction term for every spin which depends on the cavity variances.\nCavity variances and linear response The cavity variances are determined self-consistently, i.e. by calculating the same quantity in two different ways and demanding the obtained expressions to be equal. To do this, we introduce the matrix of susceptibilities\n\\begin{equation} \\chi_{ij} = \\langle S_{i} S_{j} \\rangle - \\langle S_{i} \\rangle \\langle S_{j} \\rangle = \\frac{\\partial^2}{\\partial x_{i}\\partial x_{j}} \\log Z_{0}^{(i)} \\end{equation}\n The susceptibility matrix $\\chi_{ij}$ is a covariance matrix and should thus be positive semi-definite, which is criterion for the mean-field solution be consistent. As soon this property is lost, the fixed-point procedure will no longer be stable.\n Its diagonal elements $\\chi_{ii}$ can be obtained both from the explicit calculation of the spin variances from the partition function\n\\begin{equation} \\chi_{ii} = \\langle S_{i}^2 \\rangle - \\langle S_{i} \\rangle^2 = \\frac{\\partial^2}{\\partial x_{i}^2} \\log Z_{0}^{(i)} \\label{eq:chiii} \\end{equation}\nbut also from a linear response calculation assuming fixed $V_i$,\n\\begin{align} \\chi_{ij} = \\frac{\\partial \\langle S_{i} \\rangle}{\\partial x_{j}} = \\frac{\\partial \\langle S_{i} \\rangle}{\\partial x_{i}} \\left( \\delta_{ij} + \\sum_{k} \\left( J_{ik} - V_{k} \\delta_{ik} \\right) \\chi_{kj} \\right) \\label{eq:chiijlinrespexp} \\end{align}\nwhich can be solved for $\\chi_{ij}$ to yield \\begin{equation} \\chi_{ij} = \\left[ \\left( \\boldsymbol{\\Lambda} - \\boldsymbol{J} \\right)^{-1} \\right]_{ij} \\label{eq:chiijlinresp} \\end{equation} where \\begin{align} \\boldsymbol{\\Lambda} = \\mathrm{diag} \\left( \\Lambda_1, \\ldots, \\Lambda_{N} \\right),\\\\\n\\Lambda_i = V_i + \\left( \\frac{\\partial \\langle S_{i} \\rangle}{\\partial x_{i}} \\right)^{-1}. \\end{align}\nThe cavity variances $V_i$ are then determined by equating \\eqref{eq:chiii} to the diagonal elements of \\eqref{eq:chiijlinresp} and solving the following consistency condition for $V_i$ \\begin{equation} \\frac{1}{\\Lambda_i - V_i} = \\left[ \\left( \\boldsymbol{\\Lambda} - \\boldsymbol{J} \\right)^{-1} \\right]_{ii}. \\label{eq:viselfcons} \\end{equation}\nGiven updated values for the cavity means $a_i$ and the cavity variances $V_i$, spin means and spin variances can then be updated as follows:\n\\begin{align} \\langle S_{i} \\rangle \u0026amp;= \\frac{\\partial}{\\partial x_{i}} \\log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}),\\\\\n\\langle S_{i}^2 \\rangle - \\langle S_{i} \\rangle^2 \u0026amp;= \\frac{\\partial^2}{\\partial x_{i}^2} \\log Z_{0}^{(i)} (x_{i}, a_{i}, V_{i}), \\end{align}\nThese equations reduce to explicit expressions given an explicit expression for $Z_{0}^{(i)}$. For the binary-spin partition function \\eqref{eq:partfunbinaryspins} where $S=\\pm 1$, we get a set of fixed-point equations for the spin means that look like\n\\begin{equation} \\langle S_{i} \\rangle = \\tanh \\left( \\sum_{j} J_{ij} \\langle S_{j} \\rangle - V_{i} \\langle S_{i} \\rangle + x_{i} \\right) \\end{equation}\nwith spin variances $\\chi_{ii} = 1 - \\langle S_{i} \\rangle^2$.\n3. Attention as a fixed-point method In this section, we attempt to generalize the mean-field equations obtained in the previous section to random Ising-like models with vector spin degrees of freedom. We then recognize the physical system as an attention model and provide both a slow, explicit implementation and a faster, neural one.\n ✨ Code: A reference PyTorch implementation of the models outlined below is available in the repository deep-implicit-attention.\n 3.1. Generalizing spin models to vector degrees of freedom Let\u0026rsquo;s return to our Ising model cartoon and replace the scalar spin degrees of freedom $S_i$ at every site with vectors $\\boldsymbol{S}_i \\in \\mathbb{R}^d$, which we visualize using arrows below\nLet\u0026rsquo;s consider a system of $N$ $d$-dimensional spins and let\u0026rsquo;s label site indices with $i,j,\\ldots$ and internal vector-space indices with Greek letters $\\alpha,\\beta,\\ldots$. We let the coupling weight matrix become a tensor $\\boldsymbol{J}_{ij} = J_{ij}^{\\alpha\\beta}$ (matrices coupling every pair of sites) and remove self-couplings by enforcing the couplings' block-diagonal to be zero. Additionally, we can symmetrize both the internal dimension and the sites to end up with $N(N-1)/2$ times $d(d+1)/2$ effective free parameters for the couplings. If we also turn the external fields into vectors, we obtain a vector generalization of Eq. \\eqref{eq:randomising}:\n\\begin{equation} E = \\sum_{i,j} \\boldsymbol{S}_{i}^{T} \\boldsymbol{J}_{ij} \\boldsymbol{S}_{j} + \\sum_{i} \\boldsymbol{X}_{i} \\cdot \\boldsymbol{S}_{i}. \\label{eq:vectrandomising} \\end{equation}\n3.2. Deep implicit attention: attention as a collective response Remember that our goal is to understand attention as the collective response of a statistical-mechanical system. Let\u0026rsquo;s now relate vector models like Eq. \\eqref{eq:vectrandomising} to attention models by treating the external magnetic fields $\\boldsymbol{X}_{i}$ as input data. Batches of sequences applied to every site act as probes for the system, pushing its behaviour into a certain direction. The system\u0026rsquo;s mean-field average magnetizations $\\langle \\boldsymbol{S}_{i} \\rangle$ are an approximation of the collective response at every site: what is the expected value of this particular vector spin? We interpret solving mean-field equations for $\\langle \\boldsymbol{S}_{i} \\rangle$ in the presence of input injections $\\boldsymbol{X}_{i}$ as an attention operation. If the whole system is differentiable, we can tune the couplings $\\boldsymbol{J}_{ij}$ in an outer-loop optimization to steer the system\u0026rsquo;s behaviour to better1 respond to future incoming data.\n3.3. Slow and explicit: solving the adaptive TAP equations What changes do we have to make to the adaptive TAP mean-field equations to turn them into a vector-based attention module and how can we implement them? Let\u0026rsquo;s explicitly enumerate the objects introduced in Section 2.2 together with their (generalized) tensor shapes:\n  Iteratively determined fixed-point variables\n Spin means $\\langle \\boldsymbol{S}_{i} \\rangle = \\left[ \\langle \\boldsymbol{S}_{i} \\rangle \\right]^{\\alpha}$ (batch_size, N, d) Cavity variances $\\boldsymbol{V}_{i} = V_{i}^{\\alpha\\beta}$ (N, d, d)    Other variables calculated during fixed-point iteration\n Cavity means $\\boldsymbol{a}_{i} = a_{i}^{\\alpha}$ (batch_size, N, d) Spin variances $\\langle \\boldsymbol{S}_{i}^2 \\rangle - \\langle \\boldsymbol{S}_{i} \\rangle^2 = \\boldsymbol{\\chi}_{ii} = \\chi_{ii}^{\\alpha\\beta}$ (N, d, d)    For every site, the scalar spin and cavity variances have turned into $d \\times d$ (inverse) covariance matrices on the level of the local dimension. Note that the \u0026ldquo;system properties\u0026rdquo; in the above list have no batch size: their values are identical across all examples and capture the properties of the system irrespective of the input injections $\\boldsymbol{x}_i$.\nThe vector translation of the single-site partition function looks like\n\\begin{equation} Z_{0}^{(i)} = \\int \\mathrm{d}^{d} \\boldsymbol{S} \\ \\rho_{i}\\left(\\boldsymbol{S}\\right) \\exp \\left[ \\boldsymbol{S} \\cdot \\left( \\boldsymbol{a}_{i} + \\boldsymbol{X}_{i} \\right) + \\frac{1}{2} \\boldsymbol{S}^T \\boldsymbol{V}_{i} \\boldsymbol{S} \\right] \\end{equation}\nwhere\n\\begin{equation} \\boldsymbol{a}_{i} = \\sum_{j} \\boldsymbol{J}_{ij} \\langle \\boldsymbol{S}_{j} \\rangle - \\boldsymbol{V}_{i}\\langle \\boldsymbol{S}_{i} \\rangle. \\label{eq:veccavmeans} \\end{equation}\nSpin means and variances are then computed from\n\\begin{equation} \\langle \\boldsymbol{S}_{i} \\rangle = \\frac{\\partial}{\\partial\\boldsymbol{X}_{i}} \\log Z_{0}^{(i)} (\\boldsymbol{X}_{i}, \\boldsymbol{a}_{i}, \\boldsymbol{V}_{i}) \\end{equation}\n\\begin{equation} \\langle \\boldsymbol{S}_{i}^2 \\rangle - \\langle \\boldsymbol{S}_{i} \\rangle^2 = \\frac{\\partial^2}{\\partial\\boldsymbol{X}_{i}^2} \\log Z_{0}^{(i)} (\\boldsymbol{X}_{i}, \\boldsymbol{a}_{i}, \\boldsymbol{V}_{i}) \\end{equation}\nAs a spin prior $\\rho_{i}\\left(\\boldsymbol{S}\\right)$, we pick a simple diagonal multivariate Gaussian $\\mathcal{N} \\left( \\boldsymbol{\\mu} = \\boldsymbol{0}_{d}, \\boldsymbol{\\Sigma}= \\boldsymbol{1}_{d \\times d} \\right)$ at every site, leading to the explicit equations:\n\\begin{equation} \\langle \\boldsymbol{S}_{i} \\rangle = \\left( \\boldsymbol{\\Sigma}^{-1} - \\boldsymbol{V}_{i} \\right)^{-1} \\left( \\boldsymbol{a}_{i} + \\boldsymbol{X}_{i} \\right) \\end{equation}\n\\begin{equation} \\langle \\boldsymbol{S}_{i}^2 \\rangle - \\langle \\boldsymbol{S}_{i} \\rangle^2 = \\left( \\boldsymbol{\\Sigma}^{-1} - \\boldsymbol{V}_{i} \\right)^{-1} \\end{equation}\nGeneralizing the cavity variance calculation The cavity variance computation can be done by generalizing Eqs. \\eqref{eq:chiijlinrespexp}\u0026ndash;\\eqref{eq:chiijlinresp} and solving the following system of equations for $\\boldsymbol{\\chi}_{ij}$,\n\\begin{equation} \\left( \\delta_{ik} \\otimes \\boldsymbol{1}_{d} - \\boldsymbol{\\Sigma}_{i} \\boldsymbol{J}_{ik} + \\boldsymbol{\\Sigma}_{i} \\boldsymbol{V}_{i} \\delta_{ik} \\right)\\boldsymbol{\\chi}_{kj} = \\boldsymbol{\\Sigma}_{i} \\delta_{ij} \\end{equation}\nThe generalization of the self-consistency condition Eq \\eqref{eq:viselfcons} is then obtained by solving $\\boldsymbol{\\chi}_{ii} \\boldsymbol{V}_{i} = \\boldsymbol{\\chi}_{ii} \\boldsymbol{\\Lambda}_{i} - \\boldsymbol{1}_{N \\times d \\times d}$ for $\\boldsymbol{V}_{i}$, where $ \\boldsymbol{\\Lambda}_{i} = \\boldsymbol{V}_{i} + \\boldsymbol{\\Sigma}^{-1}$ is computed using the current values of $\\boldsymbol{V}_{i}$. The price to pay for this added complexity is a computational cost of $O(N^3d^3)$ and an excruciatingly slow backward pass. The algorithm works, but it ain\u0026rsquo;t pretty.\n Implementation: To avoid torch.solve crashing on singular matrices during the fixed-point calculation, we found it crucial for stability and learning behaviour to initialize the couplings $J_{ij}^{\\alpha\\beta} \\sim \\mathcal{N}(0, \\sigma^2)$ with small values $\\sigma^2 = 1 / (N*d^2)$ to ensure $|J| \\sim \\mathcal{O}(1)$. It\u0026rsquo;s also beneficial if the sources satisfy $|\\boldsymbol{X}_{i}| \\sim \\mathcal{O}(1)$ so that terms are balanced in the update step, all together adding up to $\\mathcal{O}(1)$.\n 3.4. Fast and neural: parametrizing the Onsager self-correction term Can we somehow approximate the slow and explicit calculation of the cavity variances? Since $\\boldsymbol{z}^{*} = \\left( \\langle \\boldsymbol{S}_{i}^{*} \\rangle, \\boldsymbol{V}_{i}^{*} \\right)$ at the fixed point, the Onsager self-correction term in Eq. \\eqref{eq:veccavmeans} converges to a constant vector $\\boldsymbol{V}_{i}^{*}\\langle \\boldsymbol{S}_{i}^{*} \\rangle$ for every site. We propose to make a bold move by getting rid of the cavity variables altogether and reducing the equations for the fixed-point update step to\n\\begin{equation} \\langle \\boldsymbol{S}_{i} \\rangle = \\sum_{j} \\boldsymbol{J}_{ij} \\langle \\boldsymbol{S}_{j} \\rangle - f_{\\theta} \\left( \\langle \\boldsymbol{S}_{i} \\rangle \\right) + \\boldsymbol{X}_{i}, \\label{eq:diaupdate} \\end{equation}\nwhere $f_{\\theta}$ is a neural network parametrizing the action of the cavity variances on the spin means. Since the parameters $\\theta$ stay fixed during the inner-loop fixed-point calculation, we have effectively lifted the optimization of the self-correction term to the outer-loop, which also optimizes the weights $\\boldsymbol{J}_{ij}$.\nAll of this starts to look an awful lot like a transformer module. Before discussing an explicit comparison in Section 4, let\u0026rsquo;s finish this section with a simple example model.\nSimple example: MNIST A simple image classification model for MNIST using a convolutional feature extractor and a deep implicit attention layer could look something like\nclass MNISTNet(nn.Module): def __init__(self, dim=10, dim_conv=32, num_spins=16): super(MNISTNet, self).__init__() self.to_patch_embedding = nn.Sequential( nn.Conv2d(1, dim_conv, kernel_size=3), # -\u0026gt; 26 x 26 nn.ReLU(), nn.MaxPool2d(3, stride=2), # -\u0026gt; 12 x 12 nn.Conv2d(dim_conv, dim_conv, kernel_size=3), # -\u0026gt; 10 x 10 nn.ReLU(), nn.MaxPool2d(3, stride=2), # -\u0026gt; 4 x 4 Rearrange( 'b c h w -\u0026gt; b (h w) c' ), nn.Linear(dim_conv, dim) ) self.cls_token = nn.Parameter(torch.randn(1, 1, dim)) self.deq_atn = nn.Sequential( DEQFixedPoint( DEQMeanFieldAttention( num_spins=num_spins+1, dim=dim, weight_sym_internal=True, weight_sym_sites=False, lin_response=True, ), anderson, solver_fwd_max_iter=40, solver_fwd_tol=1e-4, solver_bwd_max_iter=40, solver_bwd_tol=1e-4, ), ) self.final = nn.Linear(dim, 10) def forward(self, x): x = self.to_patch_embedding(x) cls_tokens = self.cls_token.repeat(x.shape[0], 1, 1) x = torch.cat((cls_tokens, x), dim=1) x = self.deq_atn(x) return self.final(x[:, 0, :])  The ViT-style classification token is interpreted as an additional site in the system, which is probed with a learnable input injection that is shared across examples. The model uses the classification token\u0026rsquo;s output response to do the final classification. The system has to self-organize its behaviour so that the classification token gets all the information it needs.\nYou can train this small model (26k parameters) on MNIST to find a test set accuracy hovering around 99.1%. The animation above shows a graph reflecting the (directed) connection strengths between spins during training as measured by the Frobenius norms of the matrices $\\boldsymbol{J}_{ij}$. Almost all major organization of connections is seen to happen in the first few iterations. One imagines the model getting frustrated at zeros which really look like nines and just flat-out refusing to remember edge cases out of spite.\n4. A mean-field theory perspective on transformers Let\u0026rsquo;s conclude this post by applying the mean-field theory perspective on attention to the transformer architecture. Schematically, a vanilla transformer module looks like\nwhich consists of an attention module acting on all vectors in the sequence input followed by a feed-forward layer acting \u0026ldquo;locally\u0026rdquo; across individual vectors in the sequence, mixed with some residual connections and layer normalizations.\n4.1. Parametrizing the couplings: sparse graph structure from inputs Transformers can be interpreted as fully-connected graph neural networks acting on sets of vectors. Inside an attention module, the row-stochastic attention matrix corresponds to a particular parametrization of the couplings\n\\begin{equation} J_{ij} = \\left[\\mathrm{softmax}\\left( \\frac{\\boldsymbol{X} \\boldsymbol{W}_{\\boldsymbol{Q}} \\boldsymbol{W}_{\\boldsymbol{K}}^{T} \\boldsymbol{X}^{T}}{\\sqrt{d}} \\right)\\right]_{ij}. \\label{eq:softmaxcouplings} \\end{equation}\nwhich swaps storing explicit coupling weights for parameters of linear query-key transformations. By dynamically determining the connectivity of the sites based on the inputs $\\boldsymbol{X}$ according to Eq. \\eqref{eq:softmaxcouplings}, the coupling weights are no longer completely free parameters. The introduction of queries and keys can be seen as a neural network approach to \u0026ldquo;amortizing\u0026rdquo; the coupling tensor while the softmax temperature promotes sparsity. Multiple attention heads correspond to imposing a block-diagonal structure in the hidden dimensions of the couplings: the dot product gets cut into disjoint pieces, one for each attention head.\n4.2. Softmax attention does a single, naive mean-field update step Looking at the update step \\eqref{eq:diaupdate} and the softmax couplings \\eqref{eq:softmaxcouplings}, we observe that the softmax attention module does a single, naive mean-field update step without a self-correction term. Ignoring layer normalizations, the attention update step for every input vector looks like\n\\begin{equation} \\boldsymbol{X}'_{i} = \\sum_{j} \\left[ \\mathrm{softmax} \\left( \\frac{\\boldsymbol{X} \\boldsymbol{W}_{\\boldsymbol{Q}} \\boldsymbol{W}_{\\boldsymbol{K}}^{T} \\boldsymbol{X}^{T}}{\\sqrt{d}} \\right) \\right]_{ij} \\left[ \\boldsymbol{X} \\boldsymbol{W}_{\\boldsymbol{V}} \\right]_{j} + \\boldsymbol{X}_{i}, \\nonumber \\label{eq:vanilla-attention} \\end{equation}\nwhere, crucially, the residual connection is responsible for adding the source term to the update step. Without a residual connection, the applied magnetic field is effectively turned off and the signal would only be able to propagate via the coupling term.\n4.3. Feed-forward layer corrects naive mean-field update Looking at the Onsager self-correction term $f_{\\theta} \\left( \\langle \\boldsymbol{S}_{i} \\rangle \\right)$ in the update step \\eqref{eq:diaupdate}, we observe that the full transformer attention module emerges when we substitute $\\langle \\boldsymbol{S}_{i} \\rangle$ for its naive mean-field value, leading to\n\\begin{equation} \\mathrm{Attention}(\\boldsymbol{X})_{i} = \\boldsymbol{X}'_{i} + \\mathrm{FeedForward}\\left( \\boldsymbol{X}'_{i} \\right), \\end{equation}\nwith $\\boldsymbol{X}'_{i}$ defined above. Again, the residual connection appears to be crucial for the structure of the mean-field theory equations to match the vanilla transformer module\u0026rsquo;s architecture. As previously discussed in Section 3.4, we hypothesize that feed-forward networks in transformer modules \u0026ldquo;amortize\u0026rdquo; the linear response self-corrections.\n5. Conclusion and outlook We have shown how attention can be understood as the mean-field response of Ising-like spin systems being probed by data. By thinking of incoming data as applied magnetic fields and the output of attention modules as spin expectation values, attention can be interpreted as a fixed-point optimization process solving for a compromise between a system\u0026rsquo;s internal dynamics and the data it\u0026rsquo;s being exposed to. Since the whole system is differentiable, we can optimize the interaction weights in an outer loop to nudge the system\u0026rsquo;s behaviour.\nWe have also seen how transformers fit into the mean-field theory framework. For scalability, transformers introduce two additional constraints/approximations on top of the mean-field approximation: (1) replacing explicit couplings with parametrized couplings that are dynamically computed from the input via linear transformations (softmax query-key-value attention), and (2) replacing the expensive self-consistent computation of Onsager self-correction terms with a neural network (feed-forward layer).\nLooking ahead, the methods introduced in this post could provide ways to implicitly train mean-field approximations of Boltzmann machines and have them serve as distributed attention modules in larger interconnected systems. To go beyond mean-field approaches, it could be interesting to look at tensor network approaches.\n6. Related work A non-exhaustive list of references and inspiration includes:\n On deep equilibrium models: Deep Equilibrium Models (2019) by Shaojie Bai, Zico Kolter, Vladlen Koltun and Chapter 4: Deep Equilibrium Models of the Deep Implicit Layers - Neural ODEs, Deep Equilibrium Models, and Beyond workshop (NeurIPS 2020) by Zico Kolter, David Duvenaud, and Matt Johnson On the adaptive Thouless-Anderson-Palmer (TAP) mean-field approach in disorder physics: Adaptive and self-averaging Thouless-Anderson-Palmer mean-field theory for probabilistic modeling (2001) by Manfred Opper and Ole Winther On variational inference, iterative approximation algorithms, expectation propagation, mean-field methods and belief propagation: Expectation Propagation (2014) by Jack Raymond, Andre Manoel, Manfred Opper On Boltzmann machines and mean-field theory: Efficient Learning in Boltzmann Machines Using Linear Response Theory (1998) by H. J. Kappen and F. B. Rodríguez and Mean-field theory of Boltzmann machine learning (1998) by Toshiyuki Tanaka  References \u0026amp; footnotes   Whatever \u0026ldquo;better\u0026rdquo; means depends on the system\u0026rsquo;s (meta-)loss function, e.g. predicting corrupted tokens BERT-style or aligning representations to a teacher BYOL/DINO-style. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1617805037,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620058217,"objectID":"7ad828f967889fef598d8e9c76ff9232","permalink":"https://mcbal.github.io/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/","publishdate":"2021-04-07T15:17:17+01:00","relpermalink":"/post/deep-implicit-attention-a-mean-field-theory-perspective-on-attention-mechanisms/","section":"post","summary":"Can we model attention as the collective response of a statistical-mechanical system?","tags":["Artificial Intelligence","Associative Memories","Attention","Belief Propagation","Boltzmann Machine","Deep Learning","Emergent Collective Computational Capabilities","Energy-Based Models","Expectation Propagation","Ising Model","Mean-Field Theory","Neural Networks","Statistical Physics","Transformers"],"title":"Deep Implicit Attention: A Mean-Field Theory Perspective on Attention Mechanisms","type":"post"},{"authors":[],"categories":[],"content":"  Introduction Non-equilibrium dynamics: embracing chaos There is no avoiding the C-word Attention, world models, and hallucinations At least two realities The world as an external memory Attention as the dynamical response of non-equilibrium systems \u0026hellip;   1. Introduction In this post, I collect some open-ended thoughts on how a physicist with an inclination towards computationalism and statistical physics might look at neural information-processing systems and the idea of modeling human-like perception and intelligence.\nWe play crackpot bingo and touch on physics, consciousness, awareness, attention, world models, hallucinations, the nature of reality, active inference, and how a physicist might like models of attention to behave.\n2. Non-equilibrium dynamics: embracing chaos Ask physicists what they think about the brain as a physical system and they\u0026rsquo;ll likely tell you it\u0026rsquo;s a wet and messy non-equilibrium statistical-mechanical system. A vanishingly small minority will claim quantum effects are important. A large majority will claim these systems intrude on their honed and delicate sense of beauty as they\u0026rsquo;re too far removed from the world of spherical cows and sterile toy models. If the physicists in your sample happen to like computational metaphors, you might also hear something like \u0026ldquo;emergent collective computational abilities arising from many interacting degrees of freedom\u0026rdquo;.\nBut they might all agree that what survives beyond the small scale of a great many neuron excitations, inhibitions, and modulations are large-scale patterns. Different scales in our effective descriptions of physical reality tend to decouple: you don\u0026rsquo;t need quantum field theory to describe ocean currents. It\u0026rsquo;s what makes physics possible.\nIf the brain is a non-equilibrium statistical-mechanical system, then sensory and internal inputs are time-dependent \u0026ldquo;probes\u0026rdquo; injecting energy into the system. Energy that needs to be dissipated somehow, encouraging dynamical responses across spatiotemporal scales that continuously nudge and alter large-scale behavior. The brain self-organizes by embracing stochasticity and by never being quiet: there are always fluctuations to respond to and there is always energy to dissipate.\n3. There is no avoiding the C-word Most of the brain\u0026rsquo;s computations happen unconsciously, leading to ephemeral signals that wither away. This is the realm of fast perception and the current generation of deep neural networks. According to neuroscientists like Dehaene, only relatively long-lasting flashes of coherent activity appear in consciousness and constitute what we experience as \u0026ldquo;awareness\u0026rdquo;.\nEven though consciousness feels like the space where thoughts, feelings, and subjective experiences appear, it could be interpreted as the mere presence of whatever large-scale, semi-stable pattern happens to be dominating at any one instant, perpetually influenced by the relentless unconscious nudging going on.\nLarge-scale patterns are useful because they can synchronize and remnants of their activity can be manipulated slowly on time scales of seconds, minutes, and days, up until biological death. We need slow, conscious processing of these remnants to come up with responses to probes that are hard to automate within the structure of our brains, which has been shaped by evolutionary baggage and contingincies.\n4. Attention, world models, and hallucinations Consciousness seems like an adaptive control mechanism for attention, affording us to fix on certain aspects of sensory perception and high-level mental representations while varying others, enabling us to reflect on the relation of our attention to these concepts and on attention itself. By processing information to better grasp our environment, we can exert control over our bodily appendages and the likelihood of passing on our genes.\nWhy do we have subjective experiences at all? Maybe it\u0026rsquo;s the result of evolutionary pressure in a social context: empathy, sympathy, perspective taking, and theory of mind all require to feel what other selves feel. That\u0026rsquo;s much easier to model if you are able to feel yourself, not only sharing the biological wetware but also \u0026ldquo;certain flavours of hallucinations\u0026rdquo;. Behavior resulting from large-scale patterns can then be recognized, encoded, and transferred as symbols to the minds of other beings, transcending space and time through language and culture.\nThe \u0026ldquo;feeling of being aware\u0026rdquo; is just another hallucination, your self-story just another part of the grand narrative constructed inside your brain\u0026rsquo;s world model. Even though there is nothing to \u0026ldquo;feel\u0026rdquo; outside our skulls, it is still helpful and beneficial for everyone to pretend that our subjective experiences align most of the time. And while we\u0026rsquo;re at it: free will is a spectrum of illusions and discussing it hardly matters at all; just don\u0026rsquo;t be a dick.\n5. At least two realities Outside of individual subjective experiences, there does seem to exist a physical environment that is shared and which all propagating things exploit to interact with and use as a communication channel for sending and receiving information. Morally and pragmatically, there\u0026rsquo;s also a shared social reality and you would be rightly considered a solipsistic psychopath to deny its existence.\nThe crucial distinction to make here is between that \u0026ldquo;shared physical reality which we can never truly experience but only probe, observe, and approach indirectly\u0026rdquo; on the one hand and the \u0026ldquo;subjective conceptual experience built on our particular human sense data and shared concepts whose meaning has been agreed upon by convention over the course of millennia of cultural evolution\u0026rdquo; on the other hand.\nWe can use language and human concepts to make the statement that the sun has been a flaming ball of nuclear fusion in the middle of our solar system for billions of years. That it feels like warmth, light, and source of all life. These concepts did not arrive until we arrived, made \u0026ldquo;conscious observations\u0026rdquo;, and turned them into symbols that can be shared with the minds of others. Sensual, emotional, and scientific descriptions are all ways to grasp at the impenetrable and to doggedly hold on to order.\n6. The world as an external memory The dominant trend in deep generative modeling nowadays is to reproduce data in a pixel-perfect way, e.g. photo-realistic GANs. These are the incentives and metrics used to measure progress and evaluate state-of-the-art performance. Distilling the space of natural images into a pixel-perfect model is a solid strategy for developing a product or selling cloud computing credits, but it is arguably a lousy one to approach human-like intelligence.\nBiological systems are embedded in a dynamic environment and have experienced early on that it\u0026rsquo;s a complete waste of energy to try to reconstruct accurate, low-level representations of incoming sense data when there is a world out there relentlessly bombarding you with evidence. In a very real sense, the environment is always right there, engulfing the system, and ready to be used as an external, probeable, and malleable memory for low-level details.\nLearning in the brain is induced by perpetual interactions with the environment, nudging the dynamical response behavior of the system and modulating large-scale behavior over time. You cannot overfit if the ground beneath your feet is shifting all the time and you are forced to jump.\n7. Attention as the dynamical response of non-equilibrium systems If the goal is to build a system with self-organized emergent collective computational capabilities arising from many interacting degrees of freedom, we could look at systems where there a lot of metastable non-equilibrium states with rugged and interesting free energy landscapes and dynamic responses on a wide range of time scales.\nExamples of such systems are spin glasses and disordered random systems, which show internal dynamics determined by random couplings between spin degrees of freedom. A random Ising model (or Boltzmann machine) defined for some spins in an external field looks something like\nwith an energy function\n\\begin{equation} E(t) = - \\left( \\sum_{i,j} J_{ij} S_{i} S_{j} + \\sum_{i} \\theta_{i}(t) S_{i} \\right), \\end{equation}\nwhere the couplings $J_{ij}$ are drawn from some probability distribution and the external fields $\\theta_{i}(t)$ specify a \u0026ldquo;preferred direction\u0026rdquo;. Modulating the external fields $\\theta_{i}(t)$ pushes the model to try to align with an incoming \u0026ldquo;data stream\u0026rdquo; through relaxation. If we treat the couplings as free parameters (effectively making them sort of time-dependent as well), we expect structure and organization of the connection graph to emerge through learning. The goal for the system is to learn how to handle being driven by incoming data.\nLearning can happen in at least two ways: (1) for a fixed network structure, we can dump energy in the system and have the system figure out a way to dissipate the energy through relaxation, and (2) the structure of a network itself can be adjusted via the couplings through free energy minimization. The system should never stop learning since stopping learning means death, rigor mortis, and decay.\nA toy neural network agent in this framework would look something like this:\nwhere we again have a graph of spins with two-body interactions but have designated a node as a probe point (e.g. sense inputs) and two other nodes as outputs (e.g. motor commands and muscle nerves). All nodes are part of the computation and can talk to each other but don\u0026rsquo;t have to. The probe point receives local time-varying \u0026ldquo;magnetic fields\u0026rdquo; $\\theta_{i}(t)$ at every timestep which inject energy (and useful information if its content is low-entropy enough) into the system. We picture this system to operate at a sufficiently high level, i.e. the sense inputs could be feature vectors coming from a convolutional neural network.\nWithout any explicit learning by tuning the couplings weights, relaxation of the system can lead to self-organization of the system\u0026rsquo;s response across time scales1. Additionally minimizing free energy on top could (1) adjust the interaction weights to nudge the internal dynamics of the system, and (2) if the output nodes receive gradients, the output nodes could adapt to minimize free energy. This last concept is known as active inference where an agent\u0026rsquo;s actions are adjusted to improve its current world model. You perform saccadic eye movements because doing so will instantaneously improve your hallucinated understanding of the world.\n8. \u0026hellip; \u0026hellip;\nAcknowledgements I would like to thank L.G.P. for inspiring discussions.\n  For an interesting toy example of this kind of behavior, see this talk on Low rattling: a principle for understanding driven many-body self-organization. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1617801437,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617809238,"objectID":"18cfe3650dd4554f5073c3fda7379d15","permalink":"https://mcbal.github.io/post/physics-and-the-brain/","publishdate":"2021-04-07T14:17:17+01:00","relpermalink":"/post/physics-and-the-brain/","section":"post","summary":"Can we walk the fine line between gratuitous armchair philosophizing and crackpot bingo?","tags":["Artificial Intelligence","Attention","Consciousness","Crackpottery","Deep Learning","Emergent Collective Computational Capabilities","Neural Networks","Statistical Physics"],"title":"Physics and the Brain","type":"post"},{"authors":[],"categories":[],"content":"  Prelude: pattern terminology Attention modules  Explicit vanilla softmax attention Implicit energy-based attention   From modern Hopfield networks to multi-head attention  Energy function Verifying the update rule  Cross-attention Self-attention   Adding queries, keys, and values Adding masking and multiple attention heads   Attention in flatland: visualizing energy landscapes Conclusion   1. Introduction  📓 Colab notebook available here. Comments welcome.\n Recent work 1 2 has shown that the softmax-attention update step in transformer models can be intepreted as a one-step gradient update or \u0026ldquo;inference\u0026rdquo; step of a judiciously chosen energy function. An overview of these ideas can be found in previous blog posts:\n An Energy-Based Perspective on Attention Mechanisms in Transformers Transformer Attention as an Implicit Mixture of Effective Energy-Based Models  The goal of this blog post is to explicitly show how vanilla softmax attention is related to energy minimization approaches and how the former can be substituted for the latter. For pedagogical purposes, we will focus purely on the attention operation. However, for transformer models to perform well in practice, it is necessary to wrap attention in residual connections and point-wise feedforward processing layers, see e.g. Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth.\nSummary:\n We provide a pedagogical energy-based attention module that stays as close as possible to vanilla softmax attention for ease of comparison. We walk through the correspondence between modern Hopfield networks and vanilla softmax attention by gradually adding complexity. We present visualizations of energy landscapes and trajectories associated to attention update steps for two-dimensional toy patterns.  Prelude: pattern terminology Transformer literature almost exclusively talks about queries, keys, and values. For self-attention, these are all obtained from different linear transformations acting on the same set of input patterns. For cross-attention, only the queries derive from the input patterns; the keys and values are obtained from a different set of context patterns: think of a decoder architecture attending to encoded translations or the Perceiver model attending to multimodal input.\nHopfield networks literature starts from the idea of trying to implement an associative memory system for storing and retrieving patterns. Patterns stored in memory are called stored patterns. A state pattern is an input prompt for the associative memory system: what patterns stored in memory are closest to this particular prompt?\nDepending on the context (heh), we can refer to input patterns as state patterns or queries and to context patterns as stored patterns or memory or keys.\nAttention modules Explicit vanilla softmax attention To compare the behavior of explicit attention modules to that of energy-based attention modules, we need to first of all define a vanilla softmax attention module. The annotated implementation below features a bare_attn toggle in the forward pass for ease of comparison with the \u0026ldquo;bare\u0026rdquo; modern continuous Hopfield energy function we will discuss later on. The flag essentially disables all linear mappings so input and context patterns are processed \u0026ldquo;raw\u0026rdquo;.\nclass VanillaSoftmaxAttention(nn.Module): \u0026quot;\u0026quot;\u0026quot;Vanilla softmax attention. Adapted from https://github.com/lucidrains/perceiver-pytorch (commit 37e2eb6). \u0026quot;\u0026quot;\u0026quot; def __init__( self, query_dim, context_dim=None, heads=1, dim_head=2, scale=None, ): super().__init__() # Inner dimension is expressed in terms of head count and dimensionality # and thus decoupled from query_dim/context_dim (heads always \u0026quot;fit\u0026quot;). inner_dim = dim_head * heads context_dim = context_dim if context_dim is not None else query_dim # Linear transformations (queries, keys, values, head-mixing). self.to_q = nn.Linear(query_dim, inner_dim, bias=False) self.to_k = nn.Linear(context_dim, inner_dim, bias=False) self.to_v = nn.Linear(context_dim, inner_dim, bias=False) self.to_out = nn.Linear(inner_dim, query_dim) self.heads = heads self.scale = scale if scale is not None else dim_head ** -0.5 def forward(self, x, context=None, mask=None, scale=None, bare_attn=False): # To facilitate comparison with modern Hopfield networks, setting `bare_attn` # to `True` disables all linear mappings, assures there's only a single head and # reduces the module to a barebone attention which takes in \u0026quot;raw\u0026quot; queries or state # patterns and attends to a \u0026quot;raw\u0026quot; context/memory of stored patterns. if bare_attn: assert self.heads == 1, \u0026quot;only a single head when bare attention\u0026quot; if context is not None: assert ( x.shape[-1] == context.shape[-1] ), \u0026quot;query_dim/context_dim must match\u0026quot; # Adaptive scale. scale = scale if scale is not None else self.scale # Take context either from elsewhere of from self (attention vs. self-attention). context = context if context is not None else x # Map x to queries and context to keys and values. q = x if bare_attn else self.to_q(x) k = context if bare_attn else self.to_k(context) v = context if bare_attn else self.to_v(context) # Split up latent dimension into subspaces for heads to act on. # Head dimension becomes part of batch dimension (=\u0026gt; parallel processing of heads). h = self.heads q, k, v = map(lambda t: rearrange(t, \u0026quot;b n (h d) -\u0026gt; (b h) n d\u0026quot;, h=h), (q, k, v)) # Scaled dot product of all queries against all keys (sum over `inner_dim`). sim = einsum(\u0026quot;b i d, b j d -\u0026gt; b i j\u0026quot;, q, k) * scale # Optional masking. if mask is not None: max_neg_value = -torch.finfo(sim.dtype).max mask = repeat(mask, \u0026quot;b j -\u0026gt; (b h) () j\u0026quot;, h=h) sim.masked_fill_(~mask, max_neg_value) # Softmax operation across \u0026quot;keys\u0026quot; sequence dimension. attn = sim.softmax(dim=-1) # Contract attention matrix with values. out = einsum(\u0026quot;b i j, b j d -\u0026gt; b i d\u0026quot;, attn, v) # Move head dimension out of batch again. out = rearrange(out, \u0026quot;(b h) n d -\u0026gt; b n (h d)\u0026quot;, h=h) # Mix all the heads' outputs; stir well and serve immediately. return out if bare_attn or h == 1 else self.to_out(out)  Implicit energy-based attention Next, we define our energy-based attention module. Its forward pass will make use of the simple gradient descent function defined below to do energy minimization and update queries accordingly.\ndef minimize_energy( energy_func, queries, keys, mask=None, step_size=1.0, num_steps=1, return_trajs=False, ): \u0026quot;\u0026quot;\u0026quot;Minimize energy function with respect to queries. Keeps track of energies and trajectories for logging and plotting. \u0026quot;\u0026quot;\u0026quot; out = defaultdict(list) out[\u0026quot;queries\u0026quot;].append(queries) for _ in range(num_steps): energies = energy_func(queries, keys, mask=mask) grad_queries = torch.autograd.grad( energies, queries, grad_outputs=torch.ones_like(energies), create_graph=True, # enables double backprop for optimization )[0] queries = queries - step_size * grad_queries out[\u0026quot;queries\u0026quot;].append(queries) out[\u0026quot;energies\u0026quot;].append(energies) out[\u0026quot;energies\u0026quot;].append(energy_func(queries, keys, mask=mask)) if return_trajs: return out return out[\u0026quot;queries\u0026quot;][-1]  The EnergyBasedAttention module below has been structured to look as similar as possible to the the VanillaSoftmaxAttention module defined above. The main difference is the appearance of an energy function and the energy minimization call in the forward pass where the softmax attention used to be. Other differences include the absence of a linear map to \u0026ldquo;values\u0026rdquo; and masking being pushed into the energy function.\nclass EnergyBasedAttention(nn.Module): def __init__( self, query_dim, context_dim=None, heads=1, dim_head=2, scale=None, energy_func=None, ): super().__init__() inner_dim = dim_head * heads context_dim = context_dim if context_dim is not None else query_dim # Linear transformations (queries, keys, output). self.to_q = nn.Linear(query_dim, inner_dim, bias=False) self.to_k = nn.Linear(context_dim, inner_dim, bias=False) self.to_out = nn.Linear(inner_dim, query_dim) self.energy_func = energy_func if energy_func else hopfield_energy self.heads = heads self.scale = scale if scale is not None else dim_head ** -0.5 def forward( self, x, context=None, mask=None, scale=None, bare_attn=False, step_size=1.0, num_steps=1, return_trajs=False, ): # Bare checks. if bare_attn: assert self.heads == 1, \u0026quot;only a single head when bare attention\u0026quot; if context is not None: assert ( x.shape[-1] == context.shape[-1] ), \u0026quot;query_dim/context_dim must match\u0026quot; scale = scale if scale is not None else self.scale context = context if context is not None else x q = x if bare_attn else self.to_q(x) k = context if bare_attn else self.to_k(context) h = self.heads q, k = map(lambda t: rearrange(t, \u0026quot;b n (h d) -\u0026gt; (b h) n d\u0026quot;, h=h), (q, k)) if mask is not None: mask = repeat(mask, \u0026quot;b j -\u0026gt; (b h) () j\u0026quot;, h=h) # Minimize energy with respect to queries. outputs = minimize_energy( partial(self.energy_func, scale=scale), q, k, mask=mask, step_size=step_size, num_steps=num_steps, return_trajs=return_trajs, ) if return_trajs: return outputs out = rearrange(outputs, \u0026quot;(b h) n d -\u0026gt; b n (h d)\u0026quot;, h=h) return out if bare_attn or h == 1 else self.to_out(out)  From modern Hopfield networks to multi-head attention Let\u0026rsquo;s start with the simplest possible case: bare attention. We disable all linear mappings to queries/keys/values/output to make sure input and context patterns are processed \u0026ldquo;raw\u0026rdquo; and restrict ourselves to a single attention head. We numerically verify that a \u0026ldquo;bare\u0026rdquo; explicit attention module indeed returns the same result as doing a single, big step of energy minimization with respect to input state patterns. Put differently and more to the point, we merely show that automatic differentiation works.\nEnergy function Consider the energy function of a modern continuous Hopfield network for a set of state patterns $\\boldsymbol{\\Xi}$ and stored patterns $\\boldsymbol{X}$:\n\\begin{equation} E(\\boldsymbol{\\Xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\Xi}^T \\boldsymbol{\\Xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi} \\right),\\label{eq:energy} \\end{equation}\nThink of this model as the scoring function of an associative memory system. For now, we\u0026rsquo;d like to keep the stored patterns fixed as memory slots and wiggle around the state patterns. We can translate this energy function into the following (batched) function:\ndef hopfield_energy(state_patterns, stored_patterns, scale, mask=None): kinetic = 0.5 * einsum(\u0026quot;b i d, b i d -\u0026gt; b i\u0026quot;, state_patterns, state_patterns) scaled_dot_product = scale * einsum( \u0026quot;b i d, b j d -\u0026gt; b i j\u0026quot;, state_patterns, stored_patterns ) if mask is not None: max_neg_value = -torch.finfo(scaled_dot_product.dtype).max scaled_dot_product.masked_fill_(~mask, max_neg_value) potential = -(1.0 / scale) * torch.logsumexp(scaled_dot_product, dim=2) return kinetic + potential  Verifying the update rule Let\u0026rsquo;s sample some state patterns and stored patterns and enable gradient tracking for the state patterns since we want to take derivatives with respect to these parameters later on.\nlatent_dim = 512 state_patterns = torch.randn(1, 8, latent_dim).requires_grad_(True) stored_patterns = torch.randn(1, 32, latent_dim)  Cross-attention First up is cross-attention. We feed state patterns as input and stored patterns as context into a vanilla softmax attention module.\nsoftmax_attn = VanillaSoftmaxAttention( latent_dim, context_dim=latent_dim, heads=1, dim_head=latent_dim, scale=latent_dim ** -0.5, ) output_bare_softmax_attn = softmax_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), bare_attn=True, )  Now we do the same for an energy-based attention module and tell it to take a single, big gradient update step.\nenergy_attn = EnergyBasedAttention( latent_dim, context_dim=latent_dim, heads=1, dim_head=latent_dim, scale=latent_dim ** -0.5, energy_func=hopfield_energy, ) output_bare_energy_attn = energy_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), step_size=1.0, num_steps=1, bare_attn=True, )  Now let\u0026rsquo;s compare the outputs of the two methods:\ntorch.allclose(output_bare_softmax_attn, output_bare_energy_attn, atol=1e-6)  True  Both tensors are approximately equal: bare softmax attention corresponds to taking a single gradient step of step_size=1.0 with respect to the state patterns using the energy function of modern Hopfield networks as a loss. For more details on this correspondence, we refer to a previous blog post.\nSelf-attention Let\u0026rsquo;s do the same check for self-attention, which boils down to only inputting state patterns. Internally, the modules will consider the state patterns as stored patterns and effectively make the patterns pay attention to themselves.\noutput_bare_softmax_self_attn = softmax_attn( copy_tensor(state_patterns), bare_attn=True ) output_bare_energy_self_attn = energy_attn( copy_tensor(state_patterns), step_size=1.0, num_steps=1, bare_attn=True, ) print( torch.allclose( output_bare_softmax_self_attn, output_bare_energy_self_attn, atol=1e-6 ) ) print( f\u0026quot;Norm between input state patterns and energy-minimized patterns: \u0026quot; f\u0026quot;{torch.norm(state_patterns - output_bare_energy_self_attn)}\u0026quot; )  True Norm between input state patterns and energy-minimized patterns: 5.553587470785715e-06  The pattern update step looks almost like an an identity operation, which is to be expected for \u0026ldquo;bare\u0026rdquo; self-attention3. Without any linear transformations to map state patterns to queries and keys, every state pattern starts off already close to a local minimum since it coincides with itself as a stored pattern. The query starts off close to the key since the query-key mappings are identities. We will visualize this behavior in Section 4 for two-dimensional patterns.\nAdding queries, keys, and values Let\u0026rsquo;s now move closer to proper vanilla softmax attention by enabling linear transformations which map state patterns to queries and stored patterns to keys (and values). These parameters are able to move patterns around on the energy landscape before (queries, keys) and after (values) paying attention.\nWe recycle the previously instantiated patterns and modules and compare outputs again, making sure the parameters are equal and omitting the bare_attn flag:\noutput_softmax_attn = softmax_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns) ) energy_attn.load_state_dict(softmax_attn.state_dict(), strict=False) output_energy_attn = energy_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), step_size=1.0, num_steps=1, ) torch.allclose(output_softmax_attn, output_energy_attn, atol=1e-6)  False  Why don\u0026rsquo;t the outputs match? We have to make sure we compare apples to apples and be mindful of the fact that the energy minimization step only knows about keys. Indeed, as shown previously in Hopfield Networks is All You Need, the one-step energy minimization, expressed in terms of queries and keys, effectively implements\n\\begin{equation} \\boldsymbol{Q}^{\\text{new}} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\boldsymbol{Q} \\boldsymbol{K}^T \\right) \\boldsymbol{K} \\end{equation}\ninstead of the vanilla softmax attention step\n\\begin{equation} \\boldsymbol{Q}^{\\text{new}} = \\text{softmax}\\left( \\frac{1}{\\sqrt{d_k}} \\boldsymbol{Q} \\boldsymbol{K}^T \\right) \\boldsymbol{V} \\end{equation}\nWe can approximately undo this mapping to make a forced comparison for fixed parameters:\noutput_energy_attn_transformed = softmax_attn.to_v( output_energy_attn @ torch.pinverse(energy_attn.to_k.weight.t()) ) torch.norm(output_softmax_attn - output_energy_attn_transformed)  tensor(0.0005, grad_fn=\u0026lt;CopyBackwards\u0026gt;)  Yet since all these parameters would be optimized in a real-world scenario, we should only care about whether the representational power of the modules is similar. To make the single-head energy-based attention module more expressive, we can always add an output layer, parametrized by weights $W_{O}$, to the module. As long as the composition of linear transformations $W_{K}W_{O}$ doesn\u0026rsquo;t collapse and its rank does not fall below that of the softmax attention\u0026rsquo;s $W_{V}$, things should be okay.\nAdding masking and multiple attention heads Finally, let us tie up some loose ends and complete the correspondence between vanilla softmax attention and energy-based minimization.\nMasking Since masking boils down to putting restrictions on what patterns in the inputs are allowed to talk to each other, it can just as well be done at the level of the energy function. By filling the tensor inside the logsumexp operator in hopfield_energy with $-\\infty$ values at to-be-masked-out positions, we get the same effect as the masking operation in the forward pass of VanillaSoftmaxAttention. Boolean masks can be passed to the EnergyBasedAttention\u0026rsquo;s forward function and propagate to the energy function.\nMulti-head attention Up to now, we have only considered a single attention head. Essentially, multiple attention heads subdivide the latent space into equal parts and process these subproblems in parallel. The head dimension becomes part of the batch dimension. This translates to having parallel energy minimizations going on for different heads, each acting on their own subspace. Since our hopfield_energy function is already batched, we can use the same machinery of the previous sections, as shown below.\nheads = 8 dim_head = latent_dim // heads scale = dim_head ** -0.5 mha_energy_attn = EnergyBasedAttention( latent_dim, context_dim=latent_dim, heads=heads, dim_head=dim_head, scale=scale, energy_func=hopfield_energy, ) mha_energy_attn( copy_tensor(state_patterns), context=copy_tensor(stored_patterns), step_size=1.0, num_steps=1, )  tensor([[[-0.0514, -0.0353, 0.0243, ..., -0.0335, -0.0060, 0.0243], [-0.1004, -0.0136, -0.0297, ..., 0.0079, 0.0083, 0.0336], [-0.0507, -0.0369, -0.0219, ..., -0.0022, -0.0246, -0.0223], ..., [-0.0388, -0.0217, -0.0470, ..., -0.0067, 0.0020, -0.0139], [-0.0283, -0.0699, -0.0205, ..., -0.0261, -0.0667, 0.0052], [-0.0262, -0.0360, -0.0139, ..., -0.0011, -0.0199, -0.0004]]], grad_fn=\u0026lt;AddBackward0\u0026gt;)  It is hard to compare with the exact output of the equivalent VanillaSoftmaxAttention module for fixed module parameters. For multi-head attention, the updated queries coming out of the separate energy minimization steps will have summed over each heads' keys instead of its values. For a single attention head we could undo the keys' transformation by acting with the inverse of the keys' weights. For multiple attention heads, that is no longer possible.\nAgain, since all these parameters would be optimized in a real-world scenario, we should only care about whether the representational power of the modules is similar. One approach would be to add parameters inside the energy function that take care of mapping to \u0026ldquo;values\u0026rdquo; on the level of the heads.\nAttention in flatland: visualizing energy landscapes We now leave the world of high-dimensional latent spaces behind us and focus on the toy model scenario of just two latent space dimensions. We only consider a single attention head because having just two heads, each with dimension one, is just silly. For every two-dimensional token pattern vector, a third dimension will be provided by the value of the scalar energy function at that point.\nLet\u0026rsquo;s sample some tiny toy patterns to play around with.\ntoy_state_patterns = torch.randn(1, 16, 2).requires_grad_(True) toy_stored_patterns = torch.randn(1, 32, 2)  Bare cross-attention Let\u0026rsquo;s plot our tiny toy patterns taking a big gradient step!\nfig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=2 ** -0.5, step_size=1.0, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  In the figure above, the blue open circles correspond to the stored patterns (memory, context, keys, \u0026hellip;), the red circles denote the initial state patterns (inputs, queries, probes, \u0026hellip;) and the red crosses the updated queries obtained after n_steps of energy minimization. The red arrows denote the trajectory in the energy landscape.\nWe will now illustrate some example scenarios.\nSmall steps go nowhere fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=2 ** -0.5, step_size=0.1, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Lots of (big) steps converge near (global) minimum or repeated softmax iterations make all token representations identical fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=2 ** -0.5, step_size=1.0, num_steps=10, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Decreasing the scale (increasing the temperature) makes the landscape smoother and encourages convergence to same (global) minimum fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=0.1 * 2 ** -0.5, step_size=1.0, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Increasing the scale (lowering the temperature) creates \u0026ldquo;disconnected\u0026rdquo; valleys in the energy landscape inhabited by stored patterns which act as attractors for any query that happens to be in its basin of attraction fig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), context=copy_tensor(toy_stored_patterns), scale=10 * 2 ** -0.5, step_size=1.0, num_steps=5, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Adding linear query-key-value transformations # As commented on before, the value transformation is applied # after the update step so that effectively the product # W_K x W_V is applied to the updated state patterns. to_q = nn.Linear(2, 2, bias=False) to_k = nn.Linear(2, 2, bias=False) to_v = nn.Linear(2, 2, bias=False) fig, ax = simulate_and_plot_patterns( hopfield_energy, to_q(copy_tensor(toy_state_patterns)), context=to_k(copy_tensor(toy_stored_patterns)), scale=2 * 2 ** -0.5, step_size=1.0, num_steps=1, values_post_processing_func=to_v, plot_grid_size=2, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  The yellow arrows point from the final, energy-minimized, query updates to the \u0026ldquo;value-transformed\u0026rdquo; output queries, which are denoted with yellow crosses. Running this cell again in the colab notebook will give different landscapes and trajectories every time since the queries and keys depend on the random linear layers. The differences are more pronounced when increasing the scale (lowering the temperature).\nSince the value transformation is done after the energy minimization, it can and does undo some of the influence of the keys' attractors, e.g. sending updated queries to \u0026ldquo;uphill\u0026rdquo; regions in the energy landscape defined at that that layer. This suggests that the value transformation should not be seen as part of the core attention mechanism but that its role is rather to learn during training how to best hop to different regions in preparation for whatever the next layer needs.\nBare self-attention: on the importance of scale and why multiple heads Since all of the flatland examples so far have been for cross-attention, let\u0026rsquo;s also visualize a self-attention update below:\nfig, ax = simulate_and_plot_patterns( hopfield_energy, copy_tensor(toy_state_patterns), scale=2 ** -0.5, step_size=1.0, num_steps=1, plot_title=f\u0026quot;Energy landscape for two-dimensional toy patterns\u0026quot;, )  Wait, what? Why did the updated state patterns move from their initialization? Didn\u0026rsquo;t we see before that the norm between inputs and outputs hardly changed at all for bare self-attention?\nTo look into this, let\u0026rsquo;s plot the norm between inputs and outputs in function of the latent dimension, while scaling the scale or inverse temperature relative to the transformer default $\\beta = 1/\\sqrt{\\mathrm{d_k}}$. We sample toy patterns repeatedly for every dimension/scale combination to get an idea of the statistical behavior.\ndims = np.linspace(2.0, 1024, num=100, dtype=np.int32) beta_scales = np.linspace(0.2, 2.0, num=50, dtype=np.float32) norms = np.zeros((len(beta_scales), len(dims))) for i, dim in enumerate(dims): bare_attention = VanillaSoftmaxAttention(dim, heads=1, dim_head=dim) for j, beta_scale in enumerate(beta_scales): inputs = torch.randn(1, 32, dim).requires_grad_(True) outputs = bare_attention(inputs, bare_attn=True, scale=beta_scale * dim ** -0.5) norms[j][i] = torch.norm(inputs - outputs) # Suppresses a warning. norms = np.ma.masked_where(norms \u0026lt;= 0, norms) # Plot data. fig = plt.figure(figsize=(10, 8)) ax = fig.gca() X, Y = np.meshgrid(beta_scales, dims) contourplot = ax.contourf( dims, beta_scales, norms, norm=colors.LogNorm(vmin=1e-5, vmax=1e2), levels=np.logspace(-8, 2, 10), ) ax.set_xlabel(\u0026quot;d_k\u0026quot;) ax.set_ylabel(\u0026quot;scale / sqrt(d_k)\u0026quot;) plt.colorbar(contourplot, format=\u0026quot;%.e\u0026quot;, ticks=ticker.LogLocator(base=10)) ax.axvline(x=2, color=\u0026quot;r\u0026quot;) ax.axvline(x=512, color=\u0026quot;r\u0026quot;) transformer_default_scale = ax.axhline(y=1.0, color=\u0026quot;r\u0026quot;)  In this contour plot, we plot the norm differences between inputs and outputs of a bare self-attention step for a sweep across latent dimensions and inverse temperature scale factors. The horizontal red line corresponds to the scale factor used by default in most transformer implementations. Some comments:\n For a fixed latent dimension, we see that increasing the scale factor corresponds to smaller norm differences, i.e. more pronounced valleys where it\u0026rsquo;s much harder to get out of, especially if you start at the bottom and there is no query-key-value mapping taking you elsewhere. The vertical red line corresponds to the earlier bare self-attention result using a latent dimension of 512. The intersection point indeed corresponds a norm difference of the order we saw previously. The value for a latent dimension of 2 (left border of plot) suggests that patterns do move around quite a bit, confirming our visualization above. Setting the scale for bare multi-head attention proportionally to the (smaller) head dimension instead of the full latent dimension corresponds to moving leftwards along the horizontal red line. The norm difference increases so that, for bare multi-head self-attention, patterns in multiple small heads tend to bounce around more than they would in a single big head. This might be one of the reasons why multiple heads help with training transformers: since the effective temperature is lower in the smaller latent spaces, the topography of the lower-dimensional energy landscapes is more pronounced and individual heads can go explore a bit to find their niche valley.  Conclusion Using the tools presented in this blog post, we have shown that it is possible to swap the explicit attention module in a transformer for an implicit energy minimization method. What happens when we start playing around with different energy functions? Can we make patterns interact? Can we make the energy minimization step more efficient by treating it as a fixed-point problem? It remains to be seen whether all of this is a useful thing to do.\nReferences \u0026amp; footnotes   Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, Hopfield Networks is All You Need (2020) \u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020) \u0026#x21a9;\u0026#xfe0e;\n Caveat: For the special case of bare energy-based self-attention, state patterns actually appear quadratically in the argument of the logsumexp part of the energy function. Taking the derivative using minimize_energy(..) however assumes the context is a different node in the computational graph, which, in this case, where we should be taking the derivative of energy(x, x) instead of energy(x, context), yields a gradient that misses a factor of 2. But ensuring the gradient is \u0026ldquo;correct\u0026rdquo; for this special case would of course screw up the cancellation of the state pattern with itself for step_size=1.0 and num_steps=1 so that the updated query would no longer match the output of bare vanilla softmax attention. Proper treatment of doing multiple steps of bare energy-based self-attention should also include manually setting the context to the updated queries (since the queries themselves change every update step). Luckily no one would seriously consider using bare energy-based self-attention. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1616016977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616103377,"objectID":"e0bbd7f261e7e88f59a52332d9bb7ac9","permalink":"https://mcbal.github.io/post/attention-as-energy-minimization-visualizing-energy-landscapes/","publishdate":"2021-03-17T22:36:17+01:00","relpermalink":"/post/attention-as-energy-minimization-visualizing-energy-landscapes/","section":"post","summary":"Can we swap softmax attention for energy-based attention?","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Dynamical Systems","Energy-Based Models","Neural Networks","Transformers"],"title":"Attention as Energy Minimization: Visualizing Energy Landscapes","type":"post"},{"authors":[],"categories":[],"content":"  Introduction Attention from effective energy-based models  Restricted Boltzmann Machines Integrating out hidden units Effective energies and correlations Modern Hopfield networks as mixtures of effective RBMs   Attention as implicit energy minimization  Bending the explicit architecture From explicit architectures to implicit energy minimization Deep implicit layers for attention dynamics   Conclusion References \u0026amp; footnotes   _In your [previous post](https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/), you introduced the energy function of modern Hopfield networks without explanation. Where does it come from? What's up with the logarithm? Is there actually any other interpretation then it being reverse-engineered from the Transformers' attention step? Is this all a desperate attempt to make Hopfield networks cool again? Also, I cannot see the value of looking at attention from an energy-based perspective if it doesn't help me achieve SOTA. Weak reject._ -- 1. Introduction In a previous post, I provided an overview of attention in Transformer models and summarized its connections to modern Hopfield networks. We saw that the energy-based model \\begin{equation} E(\\boldsymbol{\\Xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\Xi}^T \\boldsymbol{\\Xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi} \\right). \\label{eq:mhnenergy} \\end{equation} enables fast pattern storage and retrieval through its simple and robust dynamics, leading to rapid convergence \\begin{align} \\boldsymbol{\\Xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi}_{n}\\right) \\label{eq:mhnupdate} \\end{align} of input queries $\\boldsymbol{\\Xi}_{n}$ to updated queries $\\boldsymbol{\\Xi}_{n+1}$ lying in the convex hull of stored patterns $\\boldsymbol{X}$. I also argued by means of handwaving that optimizing a Transformer looks like meta-learning from the point of view of its attention modules, sculpting energy landscapes to accommodate statistical patterns found in data.\nThe main goal of this post is to build on these insights and highlight how an energy-based perspective can be a useful, complementary approach towards improving attention-based neural network modules. Parallel to scaling compute and making (self-)attention more efficient, it might be worthwhile to try to scale learning itself by experimenting with radically different attention mechanisms.\nTo this end, we will first revisit ancient ideas at the boundary of statistical physics and machine learning and show how vanilla attention looks like a mixture of simple energy-based models. We will then argue how going beyond these simple models could benefit from thinking in terms of implicit instead of explicit attention modules, suggesting opportunities to put ideas from Deep Implicit Layers to work.\n2. Attention from effective energy-based models In this section, we will introduce Restricted Boltzmann Machines as a particular class of energy-based models, focusing on their capacity to capture effective correlations. After identifying classical discrete Hopfield networks and modern discrete Hopfield networks, we will demonstrate a naive way to fit modern continuous Hopfield networks into this framework. Throughout this section, we will rely heavily on the wonderful review A high-bias, low-variance introduction to machine learning for physicists by Mehda et al.1.\nRestricted Boltzmann Machines A Restricted Boltzmann Machine (RBM) is an energy-based model with a bipartite structure imposed on visible and hidden degrees of freedom: visible and hidden degrees of freedom interact with each other but do not interact among themselves (this is the \u0026ldquo;restriction\u0026rdquo;). The energy function looks like\n\\begin{equation} E \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right) = - \\sum_{i} a_{i} (v_{i}) - \\sum_{\\mu} b_{\\mu} (h_{\\mu}) - \\sum_{i \\mu} W_{i \\mu} v_{i} h_{\\mu}, \\end{equation}\nwhere the matrix $W_{i \\mu}$ encodes the coupling between hidden and visible units and where $a_{i} (\\cdot)$ and $b_{\\mu} (\\cdot)$ are functions that can be chosen at will. Popular options are:\n\\begin{align} a_{i} (\\cdot) = \\begin{cases} a_{i} v_{i} \u0026amp; \\text{if $v_{i} \\in {0,1}$ is binary (Bernouilli)}\\\\\n\\frac{v_{i}^2}{2\\sigma_{i}^{2}} \u0026amp; \\text{if $v_{i} \\in \\mathbb{R}$ is continuous (Gaussian)}\\\n\\end{cases} \\end{align}\nand similar for $b_{\\mu} (\\cdot)$.\n\nWhy hidden units? Introducing hidden or latent variables is a powerful technique to encode interactions between visible units. Complex correlations between visible units can be captured at the cost of introducing new degrees of freedom and letting them interact with visible units in a simpler way. Since this trick often relies on exploiting Gaussian integral identities and physicists like their Gaussians, it shows up in several places across physics, e.g. in the Hubbard-Stratonovich transformation.\n Renormalization group: Rather than trying to fix the interactions in the \u0026ldquo;microscopic theory\u0026rdquo; like is done in the modeling scenario above, physicists are more familiar with the \u0026ldquo;reverse\u0026rdquo; procedure of deducing what effective theory emerges at large scales from a given microscopic theory. Indeed, integrating out degrees of freedom in physical theories can lead to complex, effective interactions between remaining degrees of freedom. This insight crystallized in the development of renormalization group theory in the early 1970s. By focusing on theories defined at different length scales, Kenneth G. Wilson and his contemporaries introduced and unified the notions of flows, fixed points, and universality in theory space to understand the behavior of physical systems under a change of scale.\n As we will see in the next sections, the bipartite structure of RBMs enables pairwise and higher-order correlations to emerge between visible units after integrating out hidden units. Additionally, the conditional independence of visible and hidden units enables tractable training methods like (block) Gibbs sampling and contrastive divergence1. We will not consider explicitly training RBMs in this post but will instead reflect on the idea of implicitly training these models, which is what seems to be happening inside Transformers.\nEffective energies and correlations Let us now consider what kind of correlations between visible degrees of freedom are supported by RBMs. The distribution of the visible degrees of freedom can be obtained by marginalizing over the hidden degrees of freedom:\n\\begin{equation} p \\left( \\boldsymbol{v} \\right) = \\int \\mathrm{d} \\boldsymbol{h} \\ p \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right) = \\int \\mathrm{d} \\boldsymbol{h} \\ \\frac{\\mathrm{e}^{- E \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right)}}{Z} \\end{equation}\nWe try to find an expression for the marginalized energy $E (\\boldsymbol{v})$ by defining\n\\begin{equation} p \\left( \\boldsymbol{v} \\right) = \\frac{\\mathrm{e}^{- E (\\boldsymbol{v})}}{Z} \\end{equation}\nso that we can identify\n\\begin{align} E \\left( \\boldsymbol{v} \\right) \u0026amp;= - \\mathrm{log} \\int \\mathrm{d} \\boldsymbol{h} \\ \\mathrm{e}^{- E \\left( \\boldsymbol{v}, \\boldsymbol{h} \\right)} \\\\\n\u0026amp;= - \\sum_{i} a_{i} (v_{i}) - \\sum_{\\mu} \\log \\int \\mathrm{d} h_{\\mu}\\ \\mathrm{e}^{b_{\\mu}(h_{\\mu}) + \\sum_{i} W_{i\\mu} v_{i} h_{\\mu}} \\label{eq:effvisenergy} \\end{align}\nFollowing Mehda et al., we can try to better understand the correlations in $p(\\boldsymbol{v})$ by introducing the (prior) distribution\n\\begin{equation} q_{\\mu} \\left( h_{\\mu} \\right) = \\frac{\\mathrm{e}^{b_{\\mu} (h_{\\mu})}}{Z} \\end{equation}\nfor the hidden units $h_{\\mu}$, ignoring the interactions between $\\boldsymbol{v}$ and $\\boldsymbol{h}$. Additionally, we can introduce the hidden unit\u0026rsquo;s distribution\u0026rsquo;s cumulant generating function\n\\begin{align} K_{\\mu} (t) \u0026amp;= \\mathrm{log}\\ \\mathbb{E} \\left[ \\mathrm{e}^{t h_{\\mu}} \\right] \\\\\n\u0026amp;= \\mathrm{log} \\int \\mathrm{d} h_{\\mu} \\ q_{\\mu} \\left( h_{\\mu} \\right) \\mathrm{e}^{t h_{\\mu}}\\\\\n\u0026amp;= \\sum_{n=1}^{\\infty} \\kappa_{\\mu}^{(n)} \\frac{t^{n}}{n!}, \\end{align}\nwhich is defined such that the $n^{\\mathrm{th}}$ cumulant $\\kappa_{\\mu}^{(n)}$ of $q_{\\mu} \\left( h_{\\mu} \\right)$ can be obtained by taking derivatives $\\kappa_{\\mu}^{(n)} = \\partial_{t}^{n} K_{\\mu} \\rvert_{t=0}$.\nLooking back at the effective energy function \\eqref{eq:effvisenergy} for the visible units, we find that the effective energy can be expressed in terms of cumulants:\n\\begin{align} E \\left( \\boldsymbol{v} \\right) \u0026amp;= - \\sum_{i} a_{i} \\left(v_{i}\\right) - \\sum_{\\mu} K_{\\mu} \\left( \\sum_{i} W_{i\\mu} v_{i} \\right) \\\\\n\u0026amp;= - \\sum_{i} a_{i} \\left(v_{i}\\right) - \\sum_{\\mu} \\sum_{n=1}^{\\infty} \\kappa_{\\mu}^{(n)} \\frac{\\left( \\sum_{i} W_{i\\mu} v_{i} \\right)^{n}}{n!} \\\\\n\u0026amp;= - \\sum_{i} a_{i} \\left(v_{i}\\right) - \\sum_{i} \\left( \\sum_{\\mu} \\kappa_{\\mu}^{(1)} W_{i\\mu} \\right) v_{i} \\\\\n\u0026amp;\\ \\ \\ \\ \\ - \\frac{1}{2} \\sum_{ij} \\left( \\sum_{\\mu} \\kappa_{\\mu}^{(2)} W_{i\\mu} W_{j\\mu} \\right) v_{i} v_{j} + \\ldots \\label{eq:effectivenergy} \\end{align}\nWe see that the auxiliary, hidden degrees of freedom induce effective pairwise and higher-order correlations among visible degrees of freedom. Each hidden unit $h_{\\mu}$ can encode interactions of arbitrarily high order, with the $n$-th order cumulants of $q_{\\mu} \\left( h_{\\mu} \\right)$ weighting the $n$-th order interactions. By combining many hidden units and/or stacking layers, RBMs can in principle encode complex interactions at all orders and learn them from data.\nLet us now recover some known models by picking a suitable prior distribution for the hidden units:\n  Classical discrete Hopfield networks: Consider a Bernouilli distribution for the visible units and a standard Gaussian distribution for the hidden units. For a standard Gaussian, the mean $\\kappa_{\\mu}^{(1)} = 0$, the variance $\\kappa_{\\mu}^{(2)} = 1$, and $\\kappa_{\\mu}^{(n)} = 0$, $\\forall n\\geq 3$, leading to the quadratic energy function of Hopfield networks: \\begin{align} E \\left( \\boldsymbol{v} \\right) = - \\sum_{i} a_{i} v_{i} - \\frac{1}{2} \\sum_{ij} \\left( \\sum_{\\mu} W_{i\\mu} W_{j\\mu} \\right) v_{i} v_{j} \\end{align}\n  Modern discrete Hopfield networks: Consider a Bernouilli distribution for the visible units. Since it can be shown that the normal distribution is the only distribution whose cumulant generating function is a polynomial, i.e. the only distribution having a finite number of non-zero cumulants2, it looks like we cannot model a finite amount of polynomial interactions in this framework. But we can model an exponential interaction by considering a Poisson distribution $\\mathrm{Pois}(\\lambda)$ with rate $\\lambda=1$ for the hidden units, whose cumulants are all equal to the rate, i.e. $\\kappa_{\\mu}^{(n)} = 1$, $\\forall n\\geq 1$. Up to a constant, we then obtain an exponential interaction \\begin{align} E \\left( \\boldsymbol{v} \\right) = - \\sum_{i} a_{i} v_{i} - \\sum_{\\mu} \\exp \\left( \\sum_{i} W_{i\\mu} v_{i} \\right) \\end{align}\n  Other kinds of effective interactions can be obtained by substituting the cumulants of your favorite probability distribution. The cumulants of hidden Bernouilli units induce interactions of all orders. Considering exponential or Laplacian distributions where $\\kappa^{(n)} \\sim (n-1)!$ seems to lead to funky logarithmic interactions.\nModern Hopfield networks as mixtures of effective RBMs Let us now turn to the energy function of modern Hopfield networks for a single query $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ and $N$ stored patterns encoded by $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$, \\begin{equation} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right), \\end{equation} which we can transform into the RBM notation of the previous section by changing the names of variables and transposing the stored pattern matrix, \\begin{equation} E(\\boldsymbol{v}; W) = \\frac{1}{2} \\sum_{i} v_{i}^{2} -\\log \\left( \\sum_{\\mu} \\exp \\left( \\sum_{i} W_{\\mu i} v_{i} \\right) \\right). \\end{equation}\nIs there a simple way to interpret this energy function in terms of (effective) RBMs? Let\u0026rsquo;s imagine this energy to be an effective energy $E(\\boldsymbol{v})$ for the visible units with probability distribution \\begin{equation} p(\\boldsymbol{v}) = \\frac{\\mathrm{e}^{-E(\\boldsymbol{v})}}{Z} = \\frac{1}{Z} \\sum_{\\mu} \\mathrm{e}^{-\\frac{1}{2} \\sum_{i} v_{i}^{2} + \\sum_{i} W_{\\mu i} v_{i}}, \\end{equation} where the partition function $Z$ follows from doing a Gaussian integral \\begin{equation} Z = (2\\pi)^{n/2} \\sum_{\\mu} Z_{\\mu} = (2\\pi)^{n/2} \\sum_{\\mu} \\mathrm{e}^{\\frac{1}{2} \\sum_{i} W_{\\mu i} W_{i\\mu}} \\end{equation}\nWe can then identify the probability distribution $p(\\boldsymbol{v})$ with a mixture of effective energy-based models3 \\begin{equation} p(\\boldsymbol{v}) = \\sum_{\\mu} w_{\\mu} \\frac{\\mathrm{e}^{-\\frac{1}{2} \\sum_{i} v_{i}^{2} + \\sum_{i} \\mathbf{W}_{\\mu i} v_{i}}}{Z_{\\mu}} = \\sum_{\\mu} w_{\\mu} \\frac{ \\mathrm{e}^{ -E_{\\mu}(\\boldsymbol{v}) }}{Z_{\\mu}} \\end{equation} where $w_{\\mu} = Z_{\\mu} / Z$ so that $\\sum_{\\mu} w_{\\mu} = 1$. During training, the model can control prior weights $w_{\\mu}$ by adjusting relative norms of patterns. If the difference in norms between the stored patterns is not too wild, $w_{\\mu} \\approx 1/N$.\nA single model in the mixture has an effective energy function derived from a joint energy function with just a single hidden unit,\n\\begin{equation} E_{\\mu} \\left( \\boldsymbol{v}, h_{\\mu} \\right) = - \\sum_{i} a_{i} (v_{i}) - b_{\\mu} (h_{\\mu}) - \\sum_{i} W_{i \\mu} v_{i} h_{\\mu} \\end{equation}\nLooking back at \\eqref{eq:effectivenergy}, we see that we can recover $E_{\\mu}(\\boldsymbol{v})$ by picking a hidden prior distribution that is a constant random variable so that $\\kappa_{\\mu}^{(1)}=1$ is the only non-zero cumulant. This frozen property of hidden units seems to agree with the fast dynamics of memory neurons in the dynamical systems model proposed in Krotov and Hopfield (2020)4.\nIn conclusion, the energy-based model underlying vanilla Transformer attention is not terribly exciting.\n3. Attention as implicit energy minimization Let\u0026rsquo;s finish this post with some comments on how one could leverage the idea of implicit energy minimization to develop novel attention mechanisms.\nBending the explicit architecture A lot of work on post-vanilla Transformer architectures tries to improve softmax-attention by making it more efficient through approximations and/or modifications at the level of the architecture. Kernel-based approaches like Rethinking Attention with Performers have shown not only that softmax attention can be efficiently approximated by a generalized attention mechanism but also that generalized ReLU-based attention performed better in practice. Papers like Normalized Attention Without Probability Cage show how we can replace the softmax non-linearity in \\eqref{eq:mhnupdate} with pure normalization and still end up with a competitive algorithm, noting that the updated query being restricted to lie in the convex hull of the stored patterns is a bias we might want to question.\nFrom the above examples, it seems like at least a part of current research on attention is trying to break away from the confines of existing, explicit attention architectures but doesn\u0026rsquo;t quite know how to do so in a principled way. Does an energy-based perspective help to understand these developments?\nFrom explicit architectures to implicit energy minimization We have seen in this post that the energy function behind the softmax attention mechanism can be understood as a mixture of simple energy-based models. But what can we actually do with this information? Especially since we know from language modeling experiments that \u0026ldquo;just scaling\u0026rdquo; these simple models to billions of parameters enables them to store enough patterns to be useful. Despite huge progress, there however remain important challenges in terms of efficiency and generalizability. Considering slightly less trivial energy-based models might address both by adding interactions in such a way that attention modules are able to return a collective response rather than a sum of decoupled contributions.\nTo some extent, the additional linear transformations on the input patterns in the query-key-value formulation of Transformer self-attention already try to address this: \\begin{equation} \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V} \\label{eq:vanilla-attention} \\end{equation} These linear transformations slightly generalize the \u0026ldquo;naked\u0026rdquo; explicit gradient step of \\eqref{eq:mhnupdate} and can in principle learn to cluster and direct patterns to neighborhoods in the energy landscape, parametrizing the energy function. But why stop there?\nDeep implicit layers for attention dynamics An interesting way forward might be to integrate attention with deep implicit layers. Funnily enough, the authors of the NeurIPS 2020 tutorial on Deep Implicit Layers list self-attention as a prime example of an explicit layer in their introductory notebook. Approaches like Deep Equilibrium Models implicitly train DEQ-Transformers but still consider the attention module itself an explicit function.\nYet we have seen in a previous post that self-attention can \u0026mdash; and perhaps should \u0026mdash; actually be considered an implicit layer solving for a fixed point query. Because of the lack of dynamics of the current generation of attention mechanisms, this can be done in a single big gradient step, removing the need to iterate. Attention models with more complicated dynamics might benefit from a differentiable solver to find a fixed point and return the most appropriate result in a given context.\nCompared to modifying explicit architectures, the implicit-layer perspective seems to act on a different \u0026ldquo;conceptual level\u0026rdquo; of neural network architecture design. This raises a lot of questions. Which families of attention architectures can be expressed in terms of implicit energy functions like softmax-attention? How many of these have efficient minimization properties with closed-form gradients? Beyond closed-form gradients, how far can we go in parametrizing more general energy-based attention models and still end up with an efficient algorithm? What does the trade-off look like between an attention model\u0026rsquo;s complexity and it still being implicitly trainable?\n4. Conclusion Looking back and reversing causation, one could argue that the now-famous dot-product attention module introduced in Attention Is All You Need5 could only have been arrived at because of the properties of its implicit energy function \\eqref{eq:mhnenergy}. Indeed, it is only because of the associative memory\u0026rsquo;s decoupled and rather crude way of storing patterns in isolated, high-dimensional valleys that expensive, implicit energy minimization steps can be traded for a cheap, explicit one-step gradient update like \\eqref{eq:mhnupdate}.\nThe obvious pitfall of continuing to hold on to the conceptual framework introduced by this shortcut is that a potentially far richer picture of (sparse) attention dynamics remains obscured. Rather than perpetually rethinking what is all you really need within the confines of existing, explicit attention modules, why not opt for implicit modules built on top of an energy-based perspective to try to push things forward?\nReferences \u0026amp; footnotes   Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, David J. Schwab, A high-bias, low-variance introduction to Machine Learning for physicists (2019) \u0026#x21a9;\u0026#xfe0e;\n Proof by Marcinkiewicz (1935) according to http://www.stat.uchicago.edu/~pmcc/courses/stat306/2013/cumulants.pdf. \u0026#x21a9;\u0026#xfe0e;\n We are aware that this identification might be tremendously trivial when considering prior work on Implicit Mixtures of Restricted Boltzmann Machines or, more generally, mixture models in the context of expectation-minimization optimization. \u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020) \u0026#x21a9;\u0026#xfe0e;\n Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention Is All You Need (2017) \u0026#x21a9;\u0026#xfe0e;\n   ","date":1608627797,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609320857,"objectID":"6710ba4e2ffbeaf0ef110d190c98e7ed","permalink":"https://mcbal.github.io/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/","publishdate":"2020-12-22T10:03:17+01:00","relpermalink":"/post/transformer-attention-as-an-implicit-mixture-of-effective-energy-based-models/","section":"post","summary":"Where does the energy function behind Transformers' attention mechanism come from?","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Energy-Based Models","Neural Networks","Renormalization Group","Restricted Boltzmann Machine","Statistical Physics","Transformers"],"title":"Transformer Attention as an Implicit Mixture of Effective Energy-Based Models","type":"post"},{"authors":[],"categories":[],"content":"XKCD 793: A physicist encountering machine learning for the first time   Introduction A growing zoo of Transformers  Vanilla Transformers Beyond vanilla: confronting quadratic scaling   From Hopfield networks to Transformers  Classical discrete Hopfield networks Modern discrete Hopfield networks Modern continuous Hopfield networks Modern continuous Hopfield networks as energy-based models  Energy-based models: a gentle introduction Exactly optimizing modern continuous Hopfield networks   Transformers store and retrieve context-dependent patterns Where are patterns stored in a Transformer?   Training Transformers  Pretraining loss functions Stepping through the Transformer: implicit energy minimization Meta-learning and few-shot inference   Beyond dot-product attention  Attention dynamics: embracing collective phenomena Why very long sequences should not be needed   Conclusion References \u0026amp; footnotes   1. Introduction In 2017, Attention Is All You Need 1 demonstrated state-of-the-art performance in neural machine translation by stacking only (self-)attention layers. Compared to recurrent neural networks, Transformer models exhibit efficient parallel processing of tokens, leading to better modeling of long-range correlations and, most importantly, favorable scaling in terms of data and compute. Since then, Transformers seem to have taken over natural language processing. Widespread adoption of attention-based architectures seems likely given recent work like An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale and the flurry of developments addressing the architecture\u0026rsquo;s quadratic scaling bottlenecks.\nRecently, the papers Hopfield Networks is All You Need 2 3 4 and Large Associative Memory Problem in Neurobiology and Machine Learning 5 provided complementary post-facto explanations of some of the success of Transformers from the perspective of energy-based models. In this post, I provide a biased overview of (self-)attention in Transformers and summarize its connections to modern Hopfield networks. Along the way, I look for intuition from physics and indulge in hand-wavy arguments on how an energy-based perspective can shed light on training and improving Transformer models.\n2. A growing zoo of Transformers Let\u0026rsquo;s start off with an overview of the components in a vanilla Transformer model. Since our focus is on (self-)attention, I am going to assume some prior knowledge6 and skip comprehensive architecture descriptions and experimental results. In Section 3, we will start from scratch and use Hopfield networks to build back up to the attention module described below.\nVanilla Transformers The proto-Transformer was introduced in an encoder-decoder context for machine translation in Attention Is All You Need. The original motivation seems to have been mostly driven by engineering efforts to model long-range correlations in sequence data and the recent successes of attention mechanisms stacked on top of recurrent neural networks. The main contribution and selling point of the paper was making an attention-only approach to sequence modeling work.\nLet\u0026rsquo;s focus on the encoder on the left and ignore the decoder on the right. Transformer models accept (batches of) sets of vectors, which covers most inputs people care about in machine learning. Text can be modelled as a sequence of embedded tokens. Images can be viewed as a snaky sequence of embedded pixels or embedded patches of pixels. Since sets have no notion of ordering, learned or fixed positional information needs to be explicitly added to the input vectors.\nThe main module in the Transformer encoder block is the multi-head self-attention, which is based on a (scaled) dot-product attention mechanism acting on a set of $d$-dimensional vectors:\n\\begin{equation} \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V} \\label{eq:vanilla-attention} \\end{equation}\nHere, queries $\\mathbf{Q}$, keys $\\mathbf{K}$, and values $\\mathbf{V}$ are matrices obtained from acting with different linear transformations \u0026mdash; parametrized respectively by weights $\\mathbf{W}_{\\mathbf{Q}}$, $\\mathbf{W}_{\\mathbf{K}}$, and $\\mathbf{W}_{\\mathbf{V}}$ \u0026mdash; on the same set of $d$-dimensional inputs. Cross-attention takes the inputs for its queries from a different source than for its keys and values, as can be glimpsed from the decoder part of the architecture on the right.\nFor every input query, the updated output query of \\eqref{eq:vanilla-attention} is a linear combination of values weighted by an attention vector quantifying the overlap of the input query with the keys corresponding to these values. Stacking input query attention vectors leads to an attention matrix. Since all objects are vectors and the attention mechanism is just a dot product between vectors, we can think of the attention module as matching query vectors to their \u0026ldquo;closest\u0026rdquo; key vectors in latent space and summing up contributions from value vectors, weighted by the \u0026ldquo;closeness\u0026rdquo; of their keys to the queries.\nThe remaining components of the Transformer encoder block are needed to make the module work properly in practice:\n The multi-headedness of the attention module refers to chunking up the dimension of the vector space and having multiple attention operations running in parallel in the same module, yet with each acting on a lower-dimensional segment of the full space. This is a trick to (1) get around the fact that every input vector only couples to one query at a time to calculate its attention coefficient, and (2) provide multiple starting points in the subspaces for the queries, which might help to avoid bad local minima in parameter space during optimization. A positional feed-forward network, made up of two linear layers with a non-linearity in between, is inserted at the end of the module. Folklore wisdom tells us that the feed-forward layer needs to blow up the dimension of the latent space by a factor of four for it to be able to \u0026ldquo;disentangle\u0026rdquo; the represention. More likely though, it\u0026rsquo;s a way to increase model capacity and warp latent spaces since the attention modules on their own are pretty much linear apart from the $\\mathrm{softmax}$-operator used to obtain the normalized attention coefficients. Residual connections are added to control the flow of gradients. Layer normalisation is used to control learning dynamics and keep vector norms from exploding.  Beyond vanilla: confronting quadratic scaling Most architectural variations of the vanilla Transformer are targeted at the attention module, which scales poorly with respect to the input sequence length $N$. Since the overlap of all queries with all keys is required, calculating a dense attention matrix scales like $\\mathcal{O}(N^2)$ in time and space. Limits on the context window of the attention mechanism during training prevent the model from learning how to deal with long sequences and long-range correlations. The majority of post-vanilla Transformer species can be classified into one of the following buckets6:\n Low-rank approximations: truncate the matrix product $\\mathbf{Q} \\mathbf{K}^T$ since it\u0026rsquo;s likely not full rank for structured data Sparsification: reduce the attention calculation from all query-key pairs to a subset because not all of them feel the need to talk to each other Recurrence: keep track of a (compressed) history of context Kernels: approximate the attention operation with kernel methods  For the remainder of our discussion, we will focus on vanilla Transformers. One of the goals of this blog post is to explore how a different perspective on the function of attention-based algorithms might lead to qualitatively different improvements beyond what is possible by relying on scaling and reducing computational complexity alone.\n3. From Hopfield networks to Transformers In this section, we provide a short history of Hopfield networks and gradually build up intuition until we can recognize the Transformer self-attention mechanism for what it really is. We refer to the blog post accompanying Hopfield Networks is All You Need for more details and insightful visualizations of pattern storage and retrieval.\nClassical discrete Hopfield networks A Hopfield network is a simple model for associative memory popularized by John Hopfield in his 1982 paper Neural Networks and Physical Systems with Emergent Collective Computational Abilities7. The task of an associative memory is to store and retrieve patterns, preferably in a way that allows one to recover stored patterns quickly with a low error rate.\nThe basic idea of the Hopfield network \u0026mdash; and other energy-based models like Boltzmann machines \u0026mdash; is to construct an energy function which defines an energy landscape containing basins of attraction around patterns we want to store. Starting at any pattern, we want to have an update rule pointing towards the closest stored pattern, guided by a scalar \u0026ldquo;closeness\u0026rdquo; score provided by the energy function.\n\nLet\u0026rsquo;s make this a bit more formal but not too formal. Consider trying to store a set of $N$ binary patterns $\\{\\boldsymbol{x}_{i}\\}_{i=1}^{N}$ where each pattern $\\boldsymbol{x}_{i}$ is a $d$-dimensional vector whose entries are either $-1$ or $1$. For example, in the case of storing black-and-white images, every image would correspond to a string of pixel values, a binary pattern $\\boldsymbol{x}_{i}$.\nFor any query $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$, or state pattern, we want to find a way to retrieve the closest stored pattern. In his paper, Hopfield considered the energy function\n\\begin{equation} E = - \\frac{1}{2} \\boldsymbol{\\xi}^{T} \\boldsymbol{W} \\boldsymbol{\\xi} + \\boldsymbol{\\xi}^{T} \\boldsymbol{b} = - \\frac{1}{2} \\sum_{i=1}^{d} \\sum_{j=1}^{d} w_{ij} \\xi_{i} \\xi_{j} + \\sum_{i=1}^{d} b_{i} \\xi_{i} , \\label{eq:ising} \\end{equation}\nwhere $\\boldsymbol{b} \\in \\mathbb{R}^{d}$ denotes a bias vector and the weights $\\boldsymbol{W} \\in \\mathbb{R}^{d \\times d}$ are set to the sum of the outer products of the patterns we want to store\n\\begin{equation} \\boldsymbol{W} = \\sum_{i=1}^{N} \\boldsymbol{x}_{i} \\otimes \\boldsymbol{x}_{i}^{T}. \\end{equation}\nThe state pattern update rule is given by the sign of the gradient of \\eqref{eq:ising} with respect to $\\boldsymbol{\\xi}$ and can be done in one step (synchronously) or separately for every component of the vector (asynchronously):\n\\begin{equation} \\boldsymbol{\\xi}_{n+1} = \\mathrm{sgn} \\left( \\boldsymbol{W} \\boldsymbol{\\xi}_{n} - \\boldsymbol{b} \\right). \\end{equation}\nThe storage capacity of this system for retrieval of patterns with a small amount of errors can be shown to be $C \\cong 0.14 d$, scaling linearly with the dimension of the pattern vector.\nPhysical intuition Physicists immediately recognize the energy function \\eqref{eq:ising} as an incarnation of the Ising model. Spin degree of freedoms $\\xi_{i}$ are grouped into patterns $\\boldsymbol{\\xi}$ that are equivalent to spin configurations of $d$ spins. The weight matrix is a sum of stored-pattern spin configurations, serving as attractors for the state-pattern spin configuration. The couplings $w_{ij}$ can be regarded a sum of samples of an underlying pattern data distribution. They are not restricted to (nearest-)neighbors and their values are neither uniform like in exactly solvable models nor totally random like in spin glass models.\n Neural networks and spin glasses: There is some literature on connections between spin glasses and neural networks. Spin glasses are phases of matter describing disordered magnetic systems exhibiting both quenched disorder and frustratation. Spin glasses were a major inspiration for Hopfield networks, as beautifully explained by the condensed matter physicist Philip W. Anderson in a column series for Physics Today (1988-1990). However, apart from Efficient training of energy-based models via spin-glass control 8, I could not find any recent papers that point to a productive research direction beyond qualitative statements like \u0026ldquo;here\u0026rsquo;s two hard problems where symmetry and order will not help you solve them\u0026rdquo;.\n Modern discrete Hopfield networks Modern discrete Hopfield networks (or dense associative memories) introduced the following family of energy functions to improve pattern storage capacity and pattern separation capabilities 9 10\n\\begin{equation} E = - \\sum_{i=1}^{N} F \\left( \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{\\xi} \\right) \\end{equation}\nCompared to the classical discrete Hopfield network energy function \\eqref{eq:ising}, the explicit weight matrix is gone and the energy has been reduced to a sum of a function of dot products between the state pattern $\\boldsymbol{\\xi}$ and every stored pattern $\\boldsymbol{x}_i$. For a polynomial interaction function $F(x) = x^{a}$, low-error storage capacity is $C \\cong d^{a-1}$. The quadratic, classical discrete Hopfield network is recovered by setting $a=2$.\nEssentially, the role of $F(x)$ is to separate close patterns by blowing up differences in dot product values. Few things blow up better than exponentials, so we can generalize the energy to\n\\begin{equation} E = - \\sum_{i=1}^{N} \\exp \\left( \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{\\xi} \\right) \\end{equation}\nwith storage capacity $C \\cong 2^{d/2}$. The corresponding update rules for modern discrete Hopfield networks can be shown to converge quickly with high probability10.\nModern continuous Hopfield networks Most machine learning applications are tailored to work with continuous embeddings (vector representations) rather than discrete patterns. Is there a way to generalize modern Hopfield networks to continuous data? Recently, Hopfield Networks is All You Need proposed the following energy function to deal with continuous $d$-dimensional patterns11:\n\\begin{equation} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right), \\label{eq:energyfunc} \\end{equation}\nwhich we consider to be a function of the state pattern $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ and parametrized by $N$ stored patterns $\\boldsymbol{X} = (\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{N}) \\in \\mathbb{R}^{d \\times N}$. From the point of view of restricted Boltzmann machines, the stored patterns $\\boldsymbol{X}^T$ can also be interpreted as weights mapping $\\boldsymbol{\\xi}$ to hidden units5.\n Smoothly taking a maximum: The $\\mathrm{logsumexp}$ operator is defined for vectors $\\mathbf{x}$ as \\begin{equation} \\mathrm{logsumexp} \\left( \\mathbf{x} \\right) = \\log \\left( \\sum_{i=1}^{N} \\mathrm{e}^{x_i} \\right) \\end{equation} while for matrix arguments (like a batch of vectors), the $\\mathrm{sumexp}$ is understood to apply to just one dimension after which the $\\log$ acts element-wise on the resulting vector.\n Physical intuition We assume that the stored patterns equilibrate much quicker than those of the state pattern so that the former can effectively be considered \u0026ldquo;frozen\u0026rdquo;. The energy function \\eqref{eq:energyfunc} looks deceptively simple: there is a single state pattern and there are no interactions among stored patterns. The first term takes care of making sure the norm of the input state pattern is finite, while the second term scores the query\u0026rsquo;s overlap based on its individual alignment with every stored pattern. The exponential function in the term\n\\begin{equation} \\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) = \\log \\left( \\sum_{i=1}^{N} \\mathrm{e}^{\\mathbf{x}_i \\cdot \\boldsymbol{\\xi}} \\right) \\end{equation}\nis used to pull apart close patterns by blowing up differences in the dot product between state pattern and stored patterns. From the perspective of the query, it is not so much an interaction term but rather a measure of the alignment of the query to external \u0026ldquo;magnetic fields\u0026rdquo; generated by the stored patterns.\nDeriving the update rule In the spirit of hand-waving, let us refuse to resort to of the dynamical systems machinery used in the original references 2 5 and rather derive the update rule for the state pattern $\\boldsymbol{\\xi}$ by taking the derivative of the energy function \\eqref{eq:energyfunc} with respect to $\\boldsymbol{\\xi}$\n\\begin{equation} \\nabla_{\\boldsymbol{\\xi}} E(\\boldsymbol{\\xi}; \\boldsymbol{X}) = \\boldsymbol{\\xi} - \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right). \\end{equation}\nA gradient descent update with step size $\\gamma$ looks like\n\\begin{equation} \\boldsymbol{\\xi}_{n+1} = \\boldsymbol{\\xi}_{n} - \\gamma \\left( \\boldsymbol{\\xi}_{n} - \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi}_{n}\\right) \\right). \\label{eq:conthopfupdate} \\end{equation}\nWe are very confident that the topography of the energy landscape allows us to take big steps and boldly set $\\gamma = 1$ to recover the familiar update rule\n\\begin{align} \\boldsymbol{\\xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi}_{n}\\right) . \\end{align}\nThe updated vector is a linear combination of all stored patterns, weighted by an attention vector quantifying the overlap with the input pattern.\nModern continuous Hopfield Networks as energy-based models Let\u0026rsquo;s now try to connect the system defined by the energy function \\eqref{eq:energyfunc} to the statistical mechanics framework of energy-based models 12 13.\nEnergy-based models: a gentle introduction Energy-based models learn a parametrized energy function $E_{\\theta}$ which maps data points $\\boldsymbol{x}$ to real, scalar energy values $E_{\\theta}(\\boldsymbol{x})$. The data distribution is modeled by the Boltzmann distribution, \\begin{equation} p_{\\theta}(\\boldsymbol{x}) = \\frac{\\mathrm{e}^{ - E_{\\theta}(\\boldsymbol{x}) }}{Z(\\theta)}, \\label{eq:boltzmann} \\end{equation} where $Z(\\theta) = \\int \\mathrm{d} \\boldsymbol{x} \\ \\mathrm{e}^{-E(\\boldsymbol{x})}$ denotes the system\u0026rsquo;s partition function. Configurations $\\boldsymbol{x}$ with low energies $E_{\\theta}(\\boldsymbol{x})$ are considered more likely and their weight contributes more strongly to the partition function.\nTo steer the model distribution $p_{\\theta}$ towards a target data distribution $p_{\\mathrm{data}}$, we can try to minimize the likelihood loss function\n\\begin{equation} \\mathcal{L}_{\\mathrm{ML}} (\\theta) = \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\mathrm{data}}} \\left[ -\\log p_{\\theta} (\\boldsymbol{x}) \\right], \\label{eq:nll} \\end{equation}\nwhere the negative log-likelihood equals\n\\begin{equation} -\\log p_{\\theta} (\\boldsymbol{x}) = E_{\\theta} (\\boldsymbol{x}) + \\log Z (\\theta). \\end{equation}\nThis is a hard optimization problem because calculating $\\log Z (\\theta)$ is hard for the vast majority of high-dimensional data distributions we care about. In practice, people resort to approximations like contrastive divergence to push the energy down on \u0026ldquo;positive examples\u0026rdquo; drawn from the data distribution while pushing up on \u0026ldquo;negative examples\u0026rdquo; obtained from sampling the model distribution. Even though sampling from \\eqref{eq:boltzmann} can be done with methods like Markov Chain Monte Carlo, it is computationally expensive to do so, especially as part of an inner-loop optimization step14.\nExactly optimizing modern continuous Hopfield networks So what about the system defined by the energy function \\eqref{eq:energyfunc}? Let\u0026rsquo;s consider the stored patterns $\\mathbf{X} \\in \\mathbb{R}^{d \\times N}$ as the model parameters we want to optimise. The task for the model is then to try to memorise incoming state patterns $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ drawn from some data distribution $p_{\\mathrm{data}}$ by deciding what kind of patterns to store. The partition function looks like\n\\begin{equation} Z = \\int \\mathrm{d} \\boldsymbol{\\xi} \\ \\mathrm{e}^{-E(\\boldsymbol{\\xi})} = \\int \\mathrm{d} \\boldsymbol{\\xi} \\ \\mathrm{e}^{-\\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi}} \\left( \\sum_{i=1}^{N} \\mathrm{e}^{ \\boldsymbol{x}^{T}_{i} \\cdot \\boldsymbol{\\xi} } \\right) \\label{eq:zforcontinuoushopfield} \\end{equation}\nwhich, because of the $\\log$ in the \u0026ldquo;interaction term\u0026rdquo;, boils down to a sum of $n$-dimensional Gaussian integrals\n\\begin{aligned} Z = (2\\pi)^{n/2} \\sum_{i=1}^{N} \\mathrm{e}^{ \\frac{1}{2} \\boldsymbol{x}_{i}^{T} \\cdot \\boldsymbol{x}_{i} } \\end{aligned}\nAfter taking the logarithm, we end up with the $\\mathrm{logsumexp}$ operator:\n\\begin{equation} \\log Z = \\frac{n}{2} \\log \\left( 2\\pi \\right) + \\mathrm{logsumexp} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) \\end{equation}\nwhere the $\\mathrm{diag}$ operator is understood to turn the diagonal of its matrix argument into a vector. Plugging this expression into \\eqref{eq:nll} leads to the following loss function for the matrix of stored patterns\n\\begin{align} \\mathcal{L}_{\\mathrm{ML}} (\\mathbf{X}) = \u0026amp; \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\frac{1}{2} \\boldsymbol{\\xi}^T \\boldsymbol{\\xi} -\\mathrm{logsumexp} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\\n\u0026amp; + \\mathrm{logsumexp} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) + \\frac{n}{2} \\log \\left( 2\\pi \\right) \\end{align}\nand a gradient\n\\begin{align} \\nabla_{\\mathbf{X}} \\mathcal{L}_{\\mathrm{ML}} (\\mathbf{X}) = \u0026amp; - \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\boldsymbol{\\xi} \\otimes \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\\n\u0026amp; + \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T} \\boldsymbol{X} \\right) \\right) \\end{align}\nand an update with step size $\\gamma$\n\\begin{align} \\mathbf{X}_{n+1} = \\ \\mathbf{X}_{n} \u0026amp;+ \\gamma \\ \\mathbb{E}_{\\boldsymbol{\\xi} \\sim p_{\\mathrm{data}}} \\left[ \\boldsymbol{\\xi} \\otimes \\mathrm{softmax} \\left( \\boldsymbol{X}^T_{n} \\boldsymbol{\\xi} \\right) \\right] \\nonumber \\\\\n\u0026amp; - \\gamma \\ \\mathbf{X}_{n} \\ \\mathrm{softmax} \\left( \\frac{1}{2} \\mathrm{diag} \\left( \\boldsymbol{X}^{T}_{n} \\boldsymbol{X}_{n} \\right) \\right) \\end{align}\nLet\u0026rsquo;s try to guess what this means for a single input state pattern. The first gradient term pushes all stored patterns towards the sample but weighted by a dot-product attention vector quantifying their overlap with the input pattern, similar to \\eqref{eq:conthopfupdate} but in the other direction. The second gradient term comes from the partition function and acts as a regularizer by keeping the norms of the stored patterns in check. Regularization keeps pattern values within a reasonable range and pushes the system towards regions in parameter space with non-trivial small dot-product values.\nTransformers store and retrieve context-dependent patterns Making the leap from modern continous Hopfield networks to the vanilla Transformer (self-)attention mechanism we encountered in Section 2 requires a few additional steps, as explained in detail in the blog post accompanying Hopfield Networks is All You Need.\n We want to act on multipe $d$-dimensional state patterns at the same time in order to retrieve multiple updated patterns in parallel: \\begin{align} \\boldsymbol{\\xi} \\in \\mathbb{R}^{d} \\to \\boldsymbol{\\Xi} = (\\boldsymbol{\\xi}_{1}, \\ldots, \\boldsymbol{\\xi}_{S}) \\in \\mathbb{R}^{d \\times S} \\end{align} so that \\begin{align} \\boldsymbol{\\Xi}_{n+1} = \\boldsymbol{X} \\ \\mathrm{softmax} \\left( \\boldsymbol{X}^T \\boldsymbol{\\Xi}_{n}\\right) . \\end{align} In practice, the number of state patterns $S$ is often taken to be equal to the number of stored patterns $N$. We want to map stored patterns $\\mathbf{X}$ and state patterns $\\boldsymbol{\\Xi}$ respectively to keys $\\mathbf{K} \\in \\mathbb{R}^{N \\times d}$ and queries $\\mathbf{Q} \\in \\mathbb{R}^{S \\times d}$ in a common feature space using linear transformations $\\mathbf{W_{K}}$ and $\\mathbf{W_{Q}}$. We want introduce another linear transformation $\\mathbf{W_{V}}$ on stored patterns to transform them into values $\\mathbf{V} \\in \\mathbb{R}^{N \\times d}$ appropriate for the keys' content. We want to modify the learning dynamics by decreasing the inverse temperature to $\\beta = 1 / \\sqrt{d}$, effectively making the $\\mathrm{softmax}$ softer by increasing the temperature of the system15. Physically, this might correspond to warming up the system just enough to get out of the spin-glass phase while not introducing too much thermal noise8.  The result is the update rule we stated without explanation in Section 2: \\begin{equation} \\mathbf{Q}^{\\mathrm{updated}} = \\mathrm{Attention}\\left( \\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\mathrm{softmax} \\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d}} \\right) \\mathbf{V}, \\label{eq:transformerattnupdate} \\end{equation} where the $\\mathrm{softmax}$ acts row-wise. In practice, the vanilla Transformer module additionally wraps the above attention module in (1) residual connections to control the flow of gradients, (2) layer norms to control pattern normalisations and learning dynamics, and (3) a positional feed-forward network for additional model capacity.\nWhere are patterns stored in a Transformer? Let\u0026rsquo;s try to digest the implications of these quite substantial changes. It\u0026rsquo;s useful to think of Transformer (self-)attention modules as dynamic pattern storage and retrieval systems. In modern continuous Hopfield networks, stored patterns are considered a given. However, in the Transformer (self-)attenton module, patterns to be matched and retrieved are dependent on inputs and implicitly stored in the weights $\\mathbf{W_{Q}}$, $\\mathbf{W_{K}}$, and $\\mathbf{W_{V}}$ of the linear transformations. In every layer, the module needs to learn how to map a set of inputs to patterns it wants to store (keys and values) as well as how to best retrieve them (queries). Within the same layer, dynamically generated queries are matched to keys within the same latent space. Between attention modules of neighboring layers, the non-linear activation function in the positional feed-forward network warps latent spaces.\n4. Training Transformers Now that we are aware of an energy-based interpretation of dot-product (self-)attention, we can start hand-waving about what could be going on during the supervised training procedure of Transformer models and how energy-based models suggest a qualitatively different approach to improving attention mechanisms.\nPretraining loss functions The goal of pretraining loss functions is to induce useful data-dependent pattern storage and retrieval behavior. Pretraining strategies for Transformer-based language models rely on loss functions derived from auxiliary tasks to learn statistical patterns in natural language. Starting from almost identical model architectures, autoregressive models like GPT-3 leverage all their parameters to predict the next token in a sequence given previous tokens while autoencoding models like BERT try to reconstruct corrupted tokens. In both cases, the loss function is a cross-entropy loss involving predictions in the space of the model\u0026rsquo;s token vocabulary.\nStepping through the Transformer: implicit energy minimization Although no energy function is explicitly optimized during training16, let\u0026rsquo;s see how far we can push hand-wavy energy-based arguments by stepping through the forward and backward pass of a Transformer model. We have learned that the attention update \\eqref{eq:transformerattnupdate} in every Transformer layer is actually a hidden gradient step. This trivial insight leads to a trio of trivial observations.\nTrivial Observation #1: During training, the update step \\eqref{eq:transformerattnupdate} of the attention mechanism in a Transformer layer acts as an inner-loop optimization step, minimizing an implicit energy function determined by the queries, keys, and values constructed from the output of the previous layer.\nTrivial Observation #2: During the forward pass of a deep Transformer model, a nested hierarchy of energy functions is minimized.\nTrivial Observation #3: During the backward pass of a deep Transformer model, the parameters of its attention modules get updated such that the inner-loop optimization steps conspire to pattern match queries to keys in such a way that the sequentially-updated final latent representations are useful for improving the loss.\nMeta-learning and few-shot inference Squinting our eyes, we can see traces of a meta-learning problem: how to tune model parameters \u0026mdash; in particular the attention mechanisms' linear transformation matrices \u0026mdash; such that applying a sequence of one-step attention updates to sets of input patterns converges to representations useful for minimizing the (meta-)loss function. Learnable modules of a differentiable program can of course often be considered part of a larger meta-learning setup. But what this point of view suggests is that confining the one-step inner-loop update to a simple associative memory pattern lookup might be quite restrictive.\nYet even with with a simple dense associative memory, OpenAI\u0026rsquo;s paper Language Models are Few-Shot Learners showed that large-capacity models like GPT-3 already exhibit quite impressive meta-learning capabilities. The energy-based perspective provides a naive yet attractive explanation for this phenomenon. At inference time, the few-shot demonstrations, which make up the initial part of a few-shot learning query, condition the sequential generation process by providing basins of attraction in the energy landscape for other energy minimization steps to be pulled towards. The GPT-3 model is memorizing to the extent the demonstrations match patterns seen during training and generalizing within the possibilities of the rudimentary attention dynamics of the simple underlying energy functions.\n5. Beyond dot-product attention Let\u0026rsquo;s conclude this post with two related thoughts inspired by an energy-based perspective on current attention architectures: attention dynamics and modeling very long sequences.\nAttention dynamics: embracing collective phenomena We have seen that the energy function of a modern continuous Hopfield network \\eqref{eq:energyfunc} is rather uninspiring from a physics perspective. Theoretically, the exponential storage and efficient retrieval of patterns is obtained by burning deep valleys into the energy landscape around stored patterns (keys) for neighbouring state patterns (queries) to quickly roll into. In practice, the authors of Hopfield Networks is All You Need observed three kinds of fixed-point behavior in a pretrained BERT model: (1) global fixed points averaging over all stored patterns, (2) metastable states averaging over a subset of stored patterns, and (3) fixed points returning a single, well-separated stored pattern.\nWhat does this tell us? Assuming the attention updates converge faithfully during training, the linear maps turning input vectors into queries, keys, and values can become bottlenecks in terms of being able to separate patterns and organise the energy landscape. Additionally, the lack of interactions among patterns and the decoupled dot-product overlap between queries and keys puts considerable limits on how the network can process information. In practice, this is being partially addressed by using multiple attention heads (see Section 2), but this solution does not feel satisfactory.\nWhy very long sequences should not be needed Recurrent neural networks try to compress patterns in a single hidden state via sequential propagation but often fail to do so and forget stuff along the way. Transformers bake patterns into a hierarchical energy landscape but focus on a fixed-length context window to store and retrieve patterns. As we\u0026rsquo;ve seen in Section 2, a lot of research on improving Transformers focuses on alleviating the $\\mathcal{O}(N^2)$ bottleneck of the attention computation with the implicit goal of scaling to longer sequences and enabling larger context windows.\nBut very long sequences should not be needed if patterns are allowed to talk to each other. A model should not need all of the world as context if patterns and emergent concepts can be connected. It\u0026rsquo;s definitely worthwhile to try to reduce the computational complexity of current attention architectures, but it might be far more valuable to swap the simple energy-based model \\eqref{eq:energyfunc} for more interesting energy-based models. Why not dust off the old unrestricted Boltzmann machine once again? Or experiment with any one of a century\u0026rsquo;s worth of physics models? Not to train them explicitly, but have them serve as implicit models underlying more intricate attention mechanisms, mediated by (local) interactions among patterns. Naturally, after so much hand-waving, our journey has to end here.\n6. Conclusion Even if attention turns out to not be all we need, (self-)attention modules have established themselves as highly parallelizable neural network building blocks capable of dynamically routing information based on context. We have seen that dot-product attention modules in Transformer models work by encoding high-dimensional patterns into the landscapes of simple energy functions, enabling fast pattern storage and retrieval. During training, these landscapes are sculpted to accommodate statistical patterns found in data by hierarchically matching and combining latent pattern representations through a sequence of implicit energy function minimizations.\nWe argued that an energy-based perspective on attention provides an intuitive explanation of meta-learning capabilities of large-capacity language models and encourages the exploration of qualitatively different attention mechanisms for pattern storage and retrievel. Rather than naively scaling the current generation of Transformers, it might be more rewarding to scale learning itself by exploring more powerful, expressive, and computationally efficient attention mechanisms, guided by energy-based models. Perhaps we should consider looking at neural networks again like John Hopfield already did in 1982: physical systems with emergent collective computational abilities.\nReferences \u0026amp; footnotes   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin, Attention Is All You Need (2017) \u0026#x21a9;\u0026#xfe0e;\n Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Milena Pavlović, Geir Kjetil Sandve, Victor Greiff, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter, Hopfield Networks is All You Need (2020) \u0026#x21a9;\u0026#xfe0e;\n Johannes Brandstetter, https://ml-jku.github.io/hopfield-layers/ (2020) \u0026#x21a9;\u0026#xfe0e;\n Johannes Brandstetter and Hubert Ramsauer, https://ml-jku.github.io/blog-post-performer/ (2020) \u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Large Associative Memory Problem in Neurobiology and Machine Learning (2020) \u0026#x21a9;\u0026#xfe0e;\n If you have only just joined the attention revolution, there are a lot of great resources out there to get you started. Yannic Kilcher provides a great introduction in his video on Attention is All You Need. The High Performance NLP tutorial slides presented at EMNLP 2020 contain a thorough and visually appealing introduction to attention-based models. Because code is usually more to the point than papers that need to sell themselves, I highly recommend Phil Wang\u0026rsquo;s excellent collection of self-contained repositories showcasing some of the latest models and techniques. \u0026#x21a9;\u0026#xfe0e;\n John Hopfield, Neural Networks and Physical Systems with Emergent Collective Computational Abilities (1982) \u0026#x21a9;\u0026#xfe0e;\n Alejandro Pozas-Kerstjens, Gorka Muñoz-Gil, Miguel Ángel García-March, Antonio Acín, Maciej Lewenstein, Przemysław R. Grzybowski, Efficient training of energy-based models via spin-glass control (2019) \u0026#x21a9;\u0026#xfe0e;\n Dmitry Krotov and John Hopfield, Dense Associative Memory for Pattern Recognition (2016) \u0026#x21a9;\u0026#xfe0e;\n Mete Demircigil, Judith Heusel, Matthias Löwe, Sven Upgang, and Franck Vermet, On a Model of Associative Memory with Huge Storage Capacity (2017) \u0026#x21a9;\u0026#xfe0e;\n A physicist might consider these continuous patterns spin configurations of the degrees of freedom in a vector spin model where the internal dimension $D \\sim 10^2-10^4$ is much bigger than familiar small-$D$ cases like the XY model or the Heisenberg model but much smaller than infinity. \u0026#x21a9;\u0026#xfe0e;\n Yann LeCun, Sumit Chopra, Raia Hadsell, Marc\u0026rsquo;Aurelio Ranzato, and Fu Jie Huang, A Tutorial on Energy-Based Learning (2006) and Yann LeCun and Alfredo Canziani, Deep Learning DS-GA 1008 course (2020) \u0026#x21a9;\u0026#xfe0e;\n Pankaj Mehta, Marin Bukov, Ching-Hao Wang, Alexandre G.R. Day, Clint Richardson, Charles K. Fisher, and David J. Schwab, A high-bias, low-variance introduction to Machine Learning for physicists (2019) \u0026#x21a9;\u0026#xfe0e;\n The generator in a Generative Adverserial Network (GAN) setup can be considered a clever way to generate negative samples for the implicit energy function optimization taking place in the discriminator. \u0026#x21a9;\u0026#xfe0e;\n As we have seen in Section 2, the naive interpretation of $\\beta$ as the effective inverse temperature is tenuous in practice given the influence of the surrounding layer normalisation modules. \u0026#x21a9;\u0026#xfe0e;\n The implicitly defined energy functions in Tranformer layers are not optimized directly because they arguably do not provide a meaningful training signal on their own. Verifying whether this is true or not could make for an interesting experiment. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1606557261,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606995021,"objectID":"8e7322e243b34c70767e389210fb4d59","permalink":"https://mcbal.github.io/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/","publishdate":"2020-11-28T10:54:21+01:00","relpermalink":"/post/an-energy-based-perspective-on-attention-mechanisms-in-transformers/","section":"post","summary":"Can an energy-based perspective shed light on training and improving Transformer models?","tags":["Artificial Intelligence","Associative Memories","Attention","Deep Learning","Energy-Based Models","Neural Networks","Statistical Physics","Transformers"],"title":"An Energy-Based Perspective on Attention Mechanisms in Transformers","type":"post"}]